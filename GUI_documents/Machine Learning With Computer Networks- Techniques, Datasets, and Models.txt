In recent years, the ever-growing interconnection of businesses and people and their increased reliance on networked services have prompted computer network architectures to continually grow in size and complexity. Moreover, with the increased efficiency and convenience of network-based services and businesses, the expectations of enterprises and people with respect to network performance indicators such as latency, throughput, reliability, and resilience are steadily growing. Consequently, conventional algorithmic and heuristic-based approaches for network management tasks are starting to fall behind the expected levels of performance, as they fail to deliver timely and nuanced decisions in the face of the complex environment they are operating in. Meanwhile, Machine Learning (ML) has shown remarkable results in various problem domains such as discovering new antibiotic drugs, generating high-fidelity images from arbitrary text prompts, and even finding new mathematical conjectures. Such successes usually become very visible even beyond the research community, and thus ML has soared in popularity in the past few years. Generally, Artificial Intelligence (AI) refers to machines or systems that can perform tasks typically requiring human intelligence, such as learning, reasoning, problem-solving, perception, language understanding, and decision-making. ML is a subfield of AI that concentrates on developing algorithms and statistical models. These models enable computers to perform tasks without explicit programming. In other words, it involves using statistical techniques to enable machines to learn from data and improve their performance over time. ML models repeatedly show their potential for delivering high-quality output (e.g., classifications/decisions, regression values, and generated artifacts) in highly complex environments with non-trivial decision boundaries. Generally, for that sort of environment, the proposed ML model greatly reduces the compute resources needed to generate an adequate response and/or generates outputs that are much "better" than what existing models could deliver. That being said, for more complex problem domains, most ML approaches require substantial amounts of compute resources and training data. Since most ML models aim at generalizing from specific records of data, the quality of these data samples is essential to the overall model performance. This often means that large amounts of data records are required to depict a sufficiently representative portion of the problem’s data domain. Also, more sophisticated models can quickly explode in terms of parameter/compute operation count and thus often require specialized training hardware (i.e., memory and compute). Nevertheless, the continuous improvement of used hardware as well as the increased attention towards training data acquisition, preparation, and generation has paved the way for ML to enter into more and more application domains. Computer networking is a highly complex problem domain with a plethora of tasks and problems that, to this day, are solved predominantly through hand-crafted, algorithmic, or heuristic methods. These methods have to respect a wide range of topologies, network types and scopes, configurations, hardware and protocol stacks, traffic patterns, and other sources of variation. Furthermore, there are many different ways to assess network performance, and in many cases, minimum performance guarantees and security policies add special constraints to the optimization problem. Additionally, contemporary networks use specialized hardware to deliver optimized performance, e.g., for forwarding packets at line speed. Oftentimes, this hardware does not easily allow ML models to replace existing functionality, e.g., because certain types of computations are not supported or because the storage is not available for more complex ML models. Finally, while network administrators and networking researchers do monitor their networks in action, the amount of useful ML training data in networking – data that is not noisy nor incomplete, publicly available, and diverse enough to cover large parts of the problem’s underlying data domain – is only a fraction of what other problem domains have at their disposal. As a consequence, optimizing network performance has so far been largely beyond the reach of ML research. However, given the increased visibility of ML, researchers are beginning to take on the aforementioned challenges of the networking community on ML, and combining ML and networking in research seems more attractive than ever. Furthermore, computer network infrastructures have been used recently to improve the performance of existing ML approaches, e.g., by distributing the training process or the data collection to improve resource utilization or training speed. ML is a very active and rapidly expanding research field that includes an abundance of learning techniques, model types, tools and frameworks, practices, and application possibilities. Although we focus here on ML models, some applications require considering the whole running system, i.e., AI system, to properly evaluate and understand the output, instead of focusing solely on the ML models. This paper is intended as a primer/practical guide for researchers who are keen on quickly applying ML to problems in computer networking and/or leveraging networking techniques to improve the performance of their ML systems but feel overwhelmed by the possibilities the intersection of ML and computer networking provides. The key points of the paper are the following: it first introduces the most relevant concepts and model architectures of ML and then puts them into the context of the different networking problem domains and the latest advancements therein; it exposes the currently open problems within computer networking and introduces a selection of different tools, datasets, and approaches that have been popular among the research community and might serve as a starting point for future work; it covers several techniques for utilizing networks to improve ML efficiency, such as reducing resource requirements via Split Learning (SL) and distributed training via Federated Learning (FL) or incorporating the right inductive biases into ML models to improve their ability to generalize from limited data; it discusses challenges related to networks for ML, such as resource constraints, security concerns, and the lack of understanding of how ML models make decisions (and how techniques such as Explainable Artificial Intelligence (XAI) may help in gaining understanding); it comprehensively provides pointers for further study on related surveys and research. The organization of the paper is visualized in Figure 1, and the remainder is organized as follows: Section II explains the basic concepts and categories of ML and relates common networking problems to them. Section III introduces the ML subfield of deep learning, which has been responsible for most of the recent ML breakthroughs, elaborating on the most common model architectures and how and why they are suited to specific tasks within computer networking. Thereafter, Section IV sheds light on the variety of accessible datasets, tools, and frameworks that ease the development and training of ML-powered networking systems. Section V discusses explainability in Artificial Intelligence (XAI), which is rightfully gaining traction because many recently tapped application domains (including computer networks) come with amounts of complexity and risk that disqualify fully black-box ML models for widespread adoption. Section VI broadens the scope presented up until now and introduces ML techniques and paradigms such as distributed and parallel learning. These techniques leverage existing networking concepts and technology and seem useful, if not mandatory, for many problems in the networking domain. Section VII and Section VIII give an overview of related survey papers and open challenges in the concerned areas, and finally, Section IX concludes this paper by summarizing the content presented in this paper and providing perspectives on the open challenges and questions of ML in networking and vice versa. The aim of this paper is to provide interested but inexperienced readers an inspiring and practical jumpstart for research in the intersection of ML and computer networking. This encompasses not only the creation of novel ML-powered solutions for covered networking scenarios but also leveraging established networking technology to enhance existing ML approaches. Compared to the aforementioned surveys and tutorials, we are the first to provide a comprehensive bidirectional overview of ML and XAI techniques across different networking fields, and vice versa. Furthermore, in addition to an overview of the current state of the art, our work provides practical guidance for aspiring researchers to shortcut their way into meaningful research: many of the mentioned related papers do not consider datasets and/or starting points to reproduce the results or even to just start experimenting. In contrast, we refer to publicly available datasets as well as methods and tools to generate synthetic datasets and design ML models suitable for the respective task. We categorize existing approaches as ML serving networks (ML4N) and networks serving ML (N4ML) based on the used metrics, which helps to identify research gaps and possible future directions of research. We introduced the most popular ML techniques, model types, and tools as well as several practical aspects to consider when practicing ML, such as obtaining high-quality data for the learning algorithm or the incorporation of inductive biases (more specifically for networking data and network topologies) into ML models in order to reduce resource requirements. Secondly, we introduced the most common computer networking problem domains and pointed to existing tools and datasets to accelerate and facilitate ML research on networking problems. Thirdly, we introduced how XAI methods can improve the transparency of ML models’ decisions and thus push their acceptance in the computer networks research domain and their suitability in productive environments. We also elaborated on how networking techniques can boost the performance of existing ML setups and workflows, e.g., through several approaches for distributed learning. Lastly, we provided a large number of pointers for further reading, such as surveys on more specific ML/networking domains, example research works for some of the problems introduced in this paper, or links to many of the mentioned datasets or tools. Despite our comprehensive coverage of established tools, approaches, and recent breakthroughs, it’s important to acknowledge the dynamic nature of ML research. The field is characterized by the emergence of new algorithms, the potential availability of additional tools and features in the future, and the hopeful prospect of more open-sourced datasets. While this evolution is happening at an unprecedented pace, this paper still serves as a valuable starting point for researchers and newcomers alike and provides a timely and relevant contribution to the intersection of the fields of ML and computer networking.

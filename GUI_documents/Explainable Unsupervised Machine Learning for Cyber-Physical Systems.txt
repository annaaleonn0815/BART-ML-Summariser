Cyber-Physical Systems (CPSs) are capable of seamlessly integrating computing and physical resources. Integration of physical components with cyber components allows resizing and reconfiguration of CPS, resulting in better scalability and flexibility than traditional standalone systems. Computing resources allow better information flow within CPSs, resulting in production efficiency (decreasing production downtimes, increasing product quality, adjusting production planning) while reducing building and operations system costs. Due to the various advantages of CPS, modern critical infrastructure has become heavily reliant on them. Therefore, it is important to research on building more efficient, reliable, and safe CPSs with innovative capabilities to address the needs of humans. Many independent agencies and national institutes such as the National Science Foundation (NSF), U.S. Department of Homeland Security (DHS), U.S. Department of Transportation (DOT), National Institute of Health (NIH), National Institute of Biomedical Imaging and Bio-engineering (NIBIB), National Cancer Institute (NCI), and European Commission (E.C.) have recently put their attention towards the advancements of CPS. Their interests include Internet of Things, Industrial Internet, Smart Cities, Smart Grids, and "smart" anything (Manufacturing, Cars, Buildings). Due to the widespread usage and economic benefits of CPSs, ensuring the secure, reliable, resilient, and consistent performance of CPSs is crucial. One solution is to apply data-driven machine learning methods to the massive amount of data generated through these CPSs to improve their operation reliability, improve their performance (in terms of production capacity and cost), performance optimization, preventive maintenance, and threat detection. Despite the tremendous benefits of machine learning (AI), many people hesitate to trust AI-based systems due to their black-box nature, which makes it difficult to get insight into the internal decision-making process of AI models. Especially for human-in-the-loop systems, humans need to understand these algorithms such that they can trust these models. By addressing this question, the explainable machine learning (XAI) research area has received a lot of attention. The goal of XAI is to provide reasoning for ML model outputs, allowing humans to understand and trust ML models' decision-making process. Currently, many entities have put their attention to XAI. DARPA is one of the first organizations that initiated XAI programs focusing on developing explainable models. Their program is interested in developing a toolkit library consisting of machine learning and human-computer interface software modules that could be used to develop future explainable AI systems. Currently, many entities have put their attention to XAI, such as the European Commission, NSF, NIST, and IBM. While XAI has become a trendy research topic, the majority of the work has been focused on supervised machine learning methods. However, real-world settings such as CPSs bring the challenge of dealing with high volumes of unlabeled data at a rapid pace. The manual labeling process is expensive, time-consuming, and requires the expertise of the data. It has been found that 25% of the time allocated to machine learning projects is for data labeling. Further, supervised feature learning is not only unable to take advantage of the abundance of real-world unlabelled data, but it also can result in biases by relying on labeled data. These limitations have gained the focus towards unsupervised ML algorithms and are predicted to be far more important in the long term. Given the abundance of real-world unlabelled data, it is important to focus on developing explainable unsupervised ML methods. However, in the current literature, very little work has been performed focusing on explainable unsupervised ML. Therefore, this paper focuses on unsupervised explainable ML. In this paper, we explore Explainable Unsupervised Machine Learning on different aspects. Further, we propose a novel Explainable Unsupervised Machine Learning (XUnML) approach using the Self Organizing Map (SOM) algorithm, a widely used unsupervised algorithm with various Visual Data Mining (VDM) capabilities. The motivation for this paper is two-pronged: 1) Unsupervised Machine Learning (UnML) has gained significant attention due to large amounts of unlabelled data generation at rapid speed, and 2) the majority of the work on explainable/interpretable AI is focused on supervised machine learning, but real-world settings bring the challenge of dealing with unlabelled data, making supervised machine learning alone not sufficient for data-driven decision making. Therefore, in this paper, we investigated the need for Explainable Unsupervised Machine Learning (XUnML). We explored and revealed that the current literature has limited work on XUnML. We refined the terminologies in explainable machine learning in the unsupervised domain, exploring current terms in XAI towards achieving XUnML. We specifically focused on the Cyber-Physical Domain (CPS) domain as these systems generate a large amount of unlabelled data at rapid speeds. Therefore, unsupervised ML is a viable option to extract knowledge from the data coming from CPSs. We observed from the recent literature that three unsupervised approaches are being widely used within CPSs: Clustering, Unsupervised Feature Learning, and Model pre-training. Under each approach, we discussed the need for XUnML and explored the advantages. We proposed a novel model-specific explainable method for the Self-Organizing Map (SOM) algorithm, generating local and global explanations. Through feature value perturbation, we evaluated the model fidelity and showed that the proposed approach identifies the most important feature used by the decision-making process of SOMs. We showed that changing the values of important features affects the model outcomes of SOMs. We presented the proposed approach as a strong candidate as an XUnML method by comparing it with current XUnML methods in terms of model-specific features, limitations, and usability. Further, we explored how to apply the proposed method for specific requirements of CPSs such as safety and security, process optimization, sales strategies, the generalizability of models, and real-time operations. We discussed the usability and advantages of generated local and global explanations for each identified requirement, showing that explainable SOMs are highly beneficial for distinct needs in CPSs. In future work, the proposed approach will be further evaluated through a human study.

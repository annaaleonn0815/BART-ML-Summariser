
Pre-trained language models based on the transformer architecture have become an absolute standard for state-of-the-art performance on a wide variety of natural language processing applications. BERT, a renowned transformer-based technique, brought a great revolution that had huge impacts on the evolution of NLP. Since its release as an academic research paper, this technologically pioneering NLP model has amazed the AI world. It's the first-ever deeply bidirectional and fully unsupervised technique for language representation that was pre-trained just using a plain text corpus. Numerous advancements are happening in BERT nowadays. One notable modification over BERT by Facebook is named ROBERTA, which uses a more robust architecture with massive computational power and an enormous dataset. Another method invented called XLNet was inspired by BERT's autoregressive formation. Both these models require substantial computational power, which becomes a problem for a particular aspect. For this power with less computation, another comprehensive model comes from BERT, compromising only 5% performance degradation named DistilBERT. Despite its small size, it gives a faster performance, and DistilBERT results in almost identical performance on similar tasks. Google announces ALBERT, a lite version of BERT. Even though it has fewer parameters than BERT, it produces significant outcomes. BERT establishes its supremacy over all other language processing units. The authors included a "multilingual" version of BERT (mBERT) pre-trained on the Wikipedia articles of 104 distinct languages, including the Bangla language, to serve as a resource for languages other than English. This renowned variation of BERT emphasizes the contextual representation for several multilingual tasks. This model showed promising results and obtained state-of-the-art performance on cross-lingual benchmarks by optimizing for language-specific tasks. This consequence yields a wave of implementing BERT on monolingual data. The monolingual implementation of the BERT model for a resource-constrained language like Bangla can create a new era of language modeling for Bangla. The majority of the latest BERT models are only available in English and other resource-rich languages such as Chinese, Arabic, and Spanish. When it comes to low resources like Bangla, it is still at the bottom of the heap, leading to the lack of availability of many downstream task datasets and pre-trained language models. Additionally, mBERT, trained in 104 languages, has two significant gaps. It was prepared using only relatively structured and limited language data from Wikipedia, and another being the aggregate weights of all 104 languages. This article has addressed those deficiencies and proposed a monolingual BERT for Bangla language based on a developed large Bangla language dataset (BanglaLM). In addition, this paper also discusses the process of the pretraining architecture of the BERT transformer model for Bangla, which we refer to as Bangla-BERT. This model has been trained from scratch, and its performance is compared to mBERT and other Bangla pre-trained word embedding models on some published datasets for sentiment analysis, binary and multilabel text classification, and NER. We have developed the largest Bangla language modeling dataset to train the proposed model. The dataset is 40 GB, with three variants containing around 20 million samples for each variant. The proposed model has been trained on a substantial quantity of unsupervised developed data (BanglaLM) before fine-tuning. However, it is initiated using the parameters that have already been trained before utilizing labeled data from downstream tasks. Most of the research in Bangla didn't examine the power of the transformer. Furthermore, none of them used an extensive dataset for any pre-trained model as the resource is constrained. This work examines the potential by fine-tuning a range of Bangla downstream tasks. The proposed pre-trained model has been compared to a range of non-contextual neural models, including Bangla fasttext (skip-gram and CBOW models) and word2vec. We used some NLP datasets for the proposed BERT model's performance analysis. We compared them with classical machine learning and hybrid deep learning models, including LSTM, CNN, CRF, and proved that the Bangla-BERT model outperformed them. When we compared the outcomes to the current state of the art in performance, Bangla-BERT came out on top. As a summary, our contributions are as follows: this work proposes a massive Bangla unsupervised language dataset (BanglaLM) for language modeling; this paper presents the whole mechanism for pre-training the context-aware BERT model using BanglaLM; this work includes training a language model with the largest dataset ever created for Bangla and exploring the possibility of fine-tuning a transformer model for a low-resource language like Bangla; this work resolves the mBERT's limitation for Bangla (trained on limited and more structured data only) and mixed weights issues among 104 languages; we examine Bangla-BERT and show its effectiveness on four NLP downstream tasks: Sentiment Analysis, Named Entity Recognition, Binary, and Multilevel Text Classifications. Apart from that, compared to mBERT and other non-contextual models such as Bangla fasttext (including skip-gram and CBOW models), word2vec, in these downstream tasks, we showed that the proposed model outperformed them all by a wide margin; we make Bangla-BERT available on popular site Huggingface so that it can be adopted as the new baseline and to advance Bangla NLP research. The rest of this paper is organized as follows. Section 2 contains a brief overview of the previous research on language representation. The architecture of BERT is described in Section 3, and the method used to construct Bangla-BERT is described in Section 4. Section 5 illustrates the technique for developing vocabulary. The following section describes the downstream NLP tasks and benchmark datasets in-depth and the acquired results. Section 7 details the comparison with the previous work. Section 8 discusses future work, and section 9 concludes this paper. The emergence of Transformer-based pre-trained language models rapidly expanded the accessibility of high-performing models to the typical user. However, several established multilingual BERT models include Bangla. The only Bangla-specific BERT model known to date trains on minimal website data. We used the most extensive Bangla text corpus to pre-train the language model. This paper efficiently pre-trains the Bangla-BERT model following state-of-the-art BERT architecture. We make it available to the community with the training corpus and evaluation benchmarks. Practitioners from fields other than computer science can fine-tune them for domain-specific downstream tasks. Because of the ease of use of a pre-trained NLP model, its use cases are much broader. By publishing our Bangla-BERT model, we intend to promote deep learning research and applications in Bangla-speaking nations. Additionally, the work will optimize Bangla NLP models in complexity, storage, and processing requirements.

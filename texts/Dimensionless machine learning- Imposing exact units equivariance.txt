In recent years, there has been significant progress in developing machine learning methods that incorporate exact symmetries, with convolutional neural networks benefiting from the imposition of local translational symmetry at the bottom layers of the networks. When a learning problem adheres to an exact symmetry, enforcing symmetry often enhances learning and generalization. Much of the research on exact symmetries has been rooted in physics and natural sciences, where physical laws are governed by various symmetries. One such symmetry is the symmetry underlying dimensional analysis, where quantities added or subtracted must have identical units. This symmetry, referred to as “units equivariance,” implies that if the unit system of the inputs to a function changes, the units of the output must adjust accordingly. This symmetry reveals that outputs' scalings and dependencies on inputs can be inferred from units alone. For example, if a problem involves length, acceleration, and mass, and the goal is to predict time, dimensional analysis shows that time is proportional to the square root of length divided by acceleration. Additionally, inhomogeneous functions can only be applied to dimensionless quantities. These dimensional symmetries are strict and apply across various scientific problems, including chemistry, ecology, and economics. Machine learning methods in physical sciences often adhere to units-equivariance, even if not explicitly stated. This work focuses on a specific case of group-equivariant machine learning related to changes in units. By using dimensionless quantities, which are invariants of this group, the method builds on previous work involving group invariants to create group-equivariant functions. The approach involves transforming inputs into invariant features before training the machine learning model and then scaling back outputs to their correct dimensions or units. This symmetry-respecting machine learning philosophy is easy to implement and performs well. The assumption is that all regression inputs' dimensions and units are known, but another research direction explores how to discover dimensional relationships or other symmetries. Dimensional analysis has classical applications in engineering and science and has previously been connected to machine learning. The Buckingham Pi Theorem states that a function is units equivariant if it depends only on dimensionless quantities, which can often be derived from integer powers of input features. Incorporating group invariances and equivariances into neural network design has led to advances in fields such as molecular dynamics, turbulence, climate prediction, and traffic forecasting. Symmetries are often implemented approximately through data augmentation or exactly, as seen in graph neural networks where functions are equivariant to permutations of nodes in the adjacency matrix. In equivariant machine learning, neural networks represent functions invariant or equivariant with respect to group actions, utilizing methods such as group convolutions, irreducible representations, or constraints on optimization. In regression problems, theoretical work shows that imposing symmetries and group equivariance can reduce generalization error and sample complexity, while the absence of certain symmetries can hinder learning. Our contributions include defining units equivariance and incorporating it into machine learning models using classical dimensional analysis, demonstrating that exact units equivariance can be imposed by constructing a dimensionless version of the task, performing the dimensionless task, and then scaling back to correct dimensions or units. This approach, related to group invariants for imposing exact group equivariances, extends theoretical results on generalization bounds for regression problems under compact group symmetries to the group of scalings, which is reductive. We show through numerical regression problems that reducing model capacity due to units equivariance improves generalization. Our exploration includes symbolic regression and emulator-related tasks, and we discuss the limitations related to unknown dimensional constants. We defined units equivariance for machine learning, focusing on regression and complex functions. A function obeying this equivariance obeys the exact scalings required by dimensional analysis rules. We developed a simple framework for implementing units equivariance into regression problems. This framework puts burdens on the investigator—consistent units for all inputs and a comprehensive list of dimensional constants—but is otherwise lightweight in terms of modifying existing regression methods. We did not consider the important problems of learning dimensions or discovering missing dimensionless inputs, but these are worthy extensions. We argued that imposing units equivariance improves the bias and variance of regression methods by incorporating correct information and reducing model capacity, often by an enormous factor. The equivariance also enables out-of-sample generalization because a test set that doesn’t overlap a training set in dimensional inputs will often significantly overlap in dimensionless combinations. We illustrated these effects empirically with a few simple experiments. Units equivariance applies to all functions in the natural sciences and is most useful when many independent units are involved and the full range of physical constants is known. It is not applicable to standard image-recognition tasks, where inputs have the same units and unknown physical quantities are involved. It is also not useful in natural-science problems with unknown physical constants or laws. The discovery of physical laws often involves discovering dimensional constants, as illustrated by our black-body radiation law example. Nonetheless, we are optimistic about the usefulness of units equivariance in emulation and symbolic regression problems, where symmetries are exact and inputs, including fundamental constants, are known. In particular, problems related to the growth of structure in the Universe, dominated by gravity and with few dimensioned quantities, show promise for machine learning applications.

Text summarization is a key method used in data mining and natural language processing to extract significant information from massive text files. For text summarizing, a number of models are used, including transformers, KL-summarizer, Luhn, LEX, Word Rank, GPT-1, GPT-2, and BERT Model. The two main categories of text summarizing methods are extractive text summarization and abstractive text summarization. Extractive text summarization involves selecting key phrases, lines, or sentences from a paragraph and combining them to create a summary; for example, if the original text mentions that Helen and Jim attended a party in Delhi and Jim adopted a cat named Perl in the city, the extractive summary would be: "Helen and Jim attended a party in Delhi. Jim adopted Perl." Abstractive text summarization entails creating new words and phrases that successfully communicate the important details, even if they aren't stated clearly in the original text. Compared to extractive summary, it is a more difficult task since it calls for a deeper comprehension of the text and the capacity to construct grammatically sound and logical phrases. For instance, an abstractive summary of the same example could be: "Helen and Jim came to Delhi, where Perl was born." These techniques play a vital role in condensing and presenting the key information contained within lengthy texts. Due to the training framework and corpus, the model is big. Because it is large and there are several weights to update, training takes a while. It is high in price as it requires high computational speed and power. It must be adjusted for downstream activities, which can be fussy, because it is designed to be fed into other systems rather than as an independent application. From the working mechanism, we could implement the Rouge score between human text summarized and BERT summarized text. Based on the different dataset and effective dataset cleaning, the BERT model gives efficient answers. I have shown results in the “implementation Output Section.” We have even shown limitations and drawbacks. Users must have a high computing PC to implement this algorithm. One more important BERT Model is that it is a classifier algorithm, not an extractive summarization algorithm. One has to precisely select the dataset for handling the task process. BERT is a cutting-edge NLP model with tremendous power. The pre-trained model was developed using a vast corpus of data, and you can fine-tune it using a smaller dataset based on the task and your needs.

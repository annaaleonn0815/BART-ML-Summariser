The proposed framework is used to detect fake news from e-newspapers and social media platforms and to determine whether the content could be potentially fake or harmful. More and more news is being shared on social media platforms and websites, as it is cheaper and easier than traditional newspapers and reaches a broader demographic, and some of these sites like Facebook and Twitter have become active news-sharing mediums. Sadly, this has led to an increase in fake news being generated, possibly to spread propaganda, create tensions, or disseminate maliciousness. The framework will use a transfer learning model, BERT, which stands for Bi-directional Encoder Representation for Transformers. Bi-directional means that each word will derive its context from the words coming before and after it. For example, "I will run a marathon" and "I will run for the elections" have two different meanings for the word "run." BERT will be able to understand these two varying meanings and assign two different tokens for them. Encoder Representation means that each word in the corpus will be assigned a different token according to the BERT tokenizer. A transformer model takes the learnings of a pre-trained model, in this case BERT, and trains its own data according to the pre-trained model. The BERT model has been trained with an extensive vocabulary, including 1500 million words from Wikipedia and 800 million from the Bokus corpus. The paper will cover a literature survey conducted on the topic, a brief description of the dataset considered, a description of the models considered after performing basic NLP techniques on the dataset, followed by the architecture of the BERT model with the description of the processing techniques, and finally the results as a comparative study between the two methodologies and conclusions. The BERT model is one of the best pre-trained models for Natural Language Processing, as it is trained with a large corpus of data. The vocabulary of the BERT model is exhaustive for the English language. There is also context-based tokenization, which makes it easier for the model to differentiate between two different meanings of the same word. For the current fake news use case, the BERT model facilitates the Neural Network layer in predicting whether news articles could potentially be fake.

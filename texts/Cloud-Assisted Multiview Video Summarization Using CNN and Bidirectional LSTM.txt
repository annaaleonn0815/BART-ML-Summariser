The tremendous amount of video data generated by camera networks installed at industries, offices, and public places meets the requirements of Big Data. For instance, a simple multiview network with two cameras, acquiring video from two different views with 25 frames per second (fps), generates 180,000 frames (90,000 for each camera) for an hour. Surveillance networks acquire video data for 24 hours from multiview cameras, making it challenging to extract useful information from this Big Data, which requires significant effort when searching for salient information in such a huge-sized 60×60 video dataset. Therefore, automatic techniques are needed to extract prominent information from videos without involving human efforts. In the video analytics literature, several key information extraction techniques exist, such as video abstraction, video skimming, and video summarization (VS). VS techniques investigate input video for salient information and create a summary in the form of keyframes or short video clips that represent lengthy videos. The extracted keyframes assist in many applications such as action and activity recognition, anomaly detection, and video retrieval. The domain of VS is broadly divided into singleview video summarization (SVS) and multiview video summarization (MVS). SVS summarizes a singleview video and generates a short and representative output. SVS is a hot area of research with several traditional and learned feature-based techniques for smart surveillance in industries and various other scenarios. For instance, Mahmoud et al. proposed an SVS pipeline using a density-assisted unsupervised machine learning technique known as spatial clustering to generate a video summary. Similarly, an SVS approach is presented using clustering along with semantical, emotional, and shoot-quality clues for user-generated summaries. Fei et al. used the fused score of memorability and entropy to generate a final summary. Most of the research in the literature focuses on SVS due to its simplicity compared to MVS. MVS acquires the representative frames of multiview videos (MVV) by considering both interview and intraview correlations. The fundamental workflow of MVS includes preprocessing, feature extraction, and postprocessing for summary generation. The preprocessing of MVV involves redundancy removal techniques for video segmentation. Feature extraction aims at object detection or tracking, and postprocessing involves different learning or matching-based techniques for computing interview correlations and final summary generation. The final generated summary is useful in many applications including indoor and outdoor CCTV automatic monitoring for activities and events detection. Furthermore, it can be utilized for postaccident scenario investigation, retrieval applications, and salient information can assist in diverse domains such as law enforcement, sports, and entertainment. Similarly, MVS can be used in the field of virtual reality for creating a single 360° image view captured through various cameras. MVS literature employs various machine learning, deep learning, saliency, and motion-based techniques. For instance, Fu et al. used random walks clustering applied to spatiotemporal shot graphs, while a hypergraph-based representation is used for computing correlations among different views, converted into a spatio-temporal graph. Text summarization concepts, such as maximal marginal relevance (MMR), are used for MVS by Li and Merialdo. The idea of video-MMR is to reward relevant keyframes and penalize redundant ones. Inspired by this work, Ou et al. proposed an online distributed MVS by integrating MMR with a bandwidth-efficient distributed algorithm for finding K-nearest and farthest neighbors, using K-means clustering for final keyframes selection. Mahapatra et al. proposed a scheme for producing a video synopsis for MVV based on human actions in both indoor and outdoor scenarios, using a support vector machine to classify human actions. Sparse coding has been used by Panda et al. for SVS and MVS, where they computed interview and intraview correlations in a joint embedding space, using a pretrained CNN model to extract features. They used latent subspace clustering to utilize these features for summary generation. Panda et al. also computed two proximity matrices to accumulate interview and intraview correlations, using sparse representative selection for summary generation. Another MVS research computed interview and intraview correlations via sparse coefficients, with shots of highest importance considered for the final summary using clustering technologies. It can be observed from the MVS literature that most methods are based on low-level features and apply traditional machine learning techniques for summary generation. These methods generate summaries locally without involving cloud computing for faster MVS and instant detailed analysis. To address these challenges, this article introduces an efficient framework for MVS that performs lightweight computation locally and costly processing on the cloud. Our key contributions are as follows: (1) We present a target-appearance-based shots segmentation mechanism for industrial settings, performing target-appearance-based shots segmentation in a cost-effective manner. (2) We introduce a novel lookup table concept for computing interview correlations, storing segmented shots with targets in a timely manner and synchronized for all videos, computing interview correlations without extra processing unlike traditional MVS methods. (3) We use sequential learning for summary generation with deep features and deep bidirectional long short-term memory (DB-LSTM), which outputs probabilities of informativeness for a sequence of frames, making our system efficient and accurate for MVS in industrial environments. (4) We transmit segmented shots of video to the cloud for MVS to efficiently analyze only important data, saving bandwidth and computational resources, and allowing cloud computing to process data for advanced analysis such as abnormal activities and violence recognition without transmission delay. The article is structured as follows: Section II describes the major components of our framework in detail, Section III discusses experimental results and analysis, and Section IV concludes the article. Our framework segments MVV into shots based on human and vehicle appearances, stores segmented shots in a lookup table with timestamps, and transmits useful data to the cloud for processing. Using CNN for feature extraction and DB-LSTM for learning informative frames, our system efficiently generates summaries and has been validated through extensive experiments compared to other state-of-the-art techniques. Future work includes replacing the heavyweight CNN model with an optimized deep learning model and extending the framework for activity recognition and instant reporting of abnormal activities.

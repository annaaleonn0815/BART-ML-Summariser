Abstractive text summarization deals with creating a smaller, concise synopsis of larger textual data, which still contains all the important points present in the original text. Unlike extractive text summarization, the final summary does not contain the same sentences that are present in the original text. Here, we take the tasks of summarizing opinions and reviews of products, movies, and businesses from Amazon, Yelp, and Rotten Tomatoes to get a smaller, more convenient review that has all that a new user needs to look through to get a gist. Autoencoders are now becoming a common model used for summarization tasks. An autoencoder model consists of an encoder that converts a high-dimensional input to a low-dimensional latent vector and a decoder that converts the latent vector back to a high-dimensional output of a different size from the input size. For unsupervised learning, a categorical vector is also produced as an intermediate result, which is used to find similar latent vectors. Variational autoencoders and adversarial autoencoders are improvements to the traditional autoencoder model since they regularize each input as a distribution in the given latent space. They generate a probabilistic latent vector which helps in generative tasks. The K-Means clustering algorithm is then used to cluster the normally distributed latent vector, which is verified by the one-hot encoded vector produced by the encoder. Finally, a pre-trained T5 model is used to generate nearly semantically correct sentences. The key contributions of this work are using adversarial autoencoders for abstractive text summarization, using language modeling techniques to get final semantically correct summaries, and using a Hindi dataset for the same task. The structure of the paper is organized as follows: We first discuss the existing research studies as part of our literature survey, including their methodology and results obtained. Section 3 deals with the methodology of our study, which includes the dataset, preprocessing technique, and model architecture. Finally, the evaluation, results, and analysis are explained in Section 4, with the concluding remarks in Section 5. The experiment was conducted on four datasets in English and one in Hindi. The performance metric used for this study is Recall-Oriented Understudy for Gisting Evaluation or ROGUE scores. ROGUE-N matches N-Grams of the given text with respect to a reference text when an N-Gram refers to a group of N consecutive words. Here, N used is one and two. The recall is measured as the ratio of the number of common N-Grams found in both summary and reference to the number of N-Grams present in the reference. This is different from the BLEU score, which finds the precision or the ratio of common N-Grams to the number of N-Grams present in the generated summary. ROGUE-L is used to measure the longest common subsequence present in the given text and the reference. While ROGUE-N provides us a measure of how informative the generated text is, the ROGUE-L score tells us how fluent it is. The ROGUE-1 score obtained is 22.19, ROGUE-2 is 4.56, and ROGUE-L is 19.88. The BLEU scores obtained for the datasets are shown for 1-gram, 2-gram, and overall sentence BLEU score. The sentence BLEU score is the value obtained by giving equal weights to 1-gram, 2-gram, 3-gram, and 4-gram scores. Figures show the input text and the generated summary for all four datasets. This paper deals with the training of an adversarial autoencoder to generate an abstractive summary of an input text. Amazon, Yelp, and Rotten Tomatoes reviews for English and the Hindi Text Short Summarization Corpus for the Hindi language were used for training the model. The model is trained using two discriminators, which help ensure the latent vector follows a normal distribution and categorical vectors are predicted correctly. The ROGUE-1, ROGUE-2, and ROGUE-L scores are comparable to other studies for the Amazon, Yelp, Rotten Tomatoes, and CNNMail datasets. The ROGUE scores for the Hindi dataset were lower, but this is primarily because the headlines for the articles were used for reference instead of human-generated summaries, which is not a good reference to calculate the ROGUE scores. For future work, we would like to use a better dataset for summarization tasks in Indian languages with human-generated summaries to evaluate the model better.

Abstractive text summarization is a crucial application of Natural Language Processing (NLP) in condensing lengthy texts into coherent and informative summaries. However, the challenge of efficient legal text summarization lies in the unique characteristics of legal documents, such as their length and specialized terminology. There are currently two types of summarizations: extractive summarization, which extracts significant phrases or sentences from a lengthy text, and abstractive summarization, which paraphrases a lengthy sentence while maintaining its meaning. This research uses advanced AI and Machine Learning models, specifically XLNet and BART, to tackle the intricacies of abstractive summarization within the legal domain. The traditional, labor-intensive process of manual case summarization can be revolutionized with the aid of state-of-the-art machine learning models, saving substantial time and resources. With over 4.70 crore pending legal cases in India alone, automatic summarization holds the potential to streamline and expedite legal proceedings significantly. The study proposes a novel methodology for Indian legal texts, focusing on extractive and abstractive text summarization. By normalizing Indian legal texts and utilizing domain-independent models, the work offers a fresh perspective on the effectiveness of XLNet and BART in transforming lengthy legal documents into succinct, informative summaries. The major contribution of this work is the use of simple processing that is well suited to the XLNet and BART models. The compressed documents are then fed to BART to generate concise and coherent summaries, demonstrating how this method works well for enhancing the summarization of legal documents. The development of advanced transformer-based models, such as XLNet and BART, has significantly improved the field of abstractive text summarization. XLNet and BART have demonstrated superior capabilities in generating coherent and concise abstractive summaries, capturing complex language structures and preserving the original content’s context and coherence. BART’s denoising sequence-to-sequence pre-training approach has also produced accurate and meaningful abstractive summaries. Despite their proficiency in natural language processing tasks, these models often struggle to capture long-range dependencies and generate coherent and contextually accurate summaries. XLNet and BART are more adept at understanding the nuances of input text, resulting in more precise and contextually rich summaries. Given the dynamic nature of text summarization and the constant evolution of transformer-based models, it is crucial for researchers and practitioners to continue exploring the potential of XLNet and BART to further enhance abstractive text summarization capabilities and meet the growing demands for precise and contextually relevant summarization in various domains.

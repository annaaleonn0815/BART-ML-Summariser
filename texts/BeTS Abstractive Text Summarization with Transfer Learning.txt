In today’s world, time is a valuable investment. In our busy lives, most of us hardly have time to read a complete book, let alone spending time on finding one that interests us. Books are packed with insights into knowledge and life, blurring the boundaries of age restrictions and encouraging us in the hardest of times. The first rule of choosing books is to read what you like! With so many options, any reader can find a book that suits their preferences. Usually, a reader picks a book of their preferred genre by reading the inside flap. If the plot description seems interesting, then that’s the book. The situation is not the same with people who are visually impaired or have lost their ability to see. They have to read braille with their fingertips or special equipment, which complicates the process a bit more. Braille is a unique system of reading and writing developed for the visually impaired, with a set of symbols composed of small rectangular braille cells that contain tiny three-dimensional tactile bumps called raised dots traditionally written with embossed paper as thick as a file folder or index card. Technological advancements let all of us walk together regardless of the inabilities people face; for example, one being braille e-readers which highly stress upon the improvement of cognitive skills, and boosting literacy and numeracy of the visually impaired community. Speech synthesizers are also commonly used for the task, but the method of reading through braille remains popular in the daily lives of visually impaired people, especially the deaf-blind. Despite witnessing a steadily increasing number of books becoming available electronically as soft copies, the number of books in braille, available for specially-abled people is relatively less, and it is necessary to first translate the available English books into braille. Only then can the visually challenged community read the books with their fingertips or electronic equipment. However, manual translation and summarization are time-consuming and prone to errors. Hence, the requirement for different language processing techniques that are able to handle larger documents such as books is becoming increasingly more significant. As we know, document summarization is a very useful means for people to quickly read and browse for books of their interest. Single-document summarization is the process of automatically creating a shorter version of a document automatically. This task has received a lot of interest in the Natural Language Processing (NLP) community due to its potential for various applications for information access. Existing summarization systems are primarily concerned with the content quality and fluency of summaries, and they typically extract informative and diverse sentences in the input text to form a summary of a specific length or construct an abstractive summary by rewording and rebuilding sentences. Despite abstractive summarization being highly desirable for multiple reasons, as well as being the focus of numerous research papers, this method is difficult to automatically generate, either demanding many Graphics Processing Units (GPUs) that need to be trained over time via deep learning approaches or complex algorithms and rules with a narrow range of applicability for conventional NLP approaches. Further, it necessitates a more in-depth examination of the text specific to the domain. With this obvious hurdle in mind, this paper employs extractive summarization. Although many neural models have been proposed for extractive summarization recently, a staggering amount of implemented solutions utilize outdated natural language processing algorithms requiring regular maintenance due to poor generalization. As a result, many of the summary outputs obtained from the mentioned tools may appear irregular in their creation of content. Currently, a few attempts have been made for end-to-end training of documents for summarization tasks but are yet to achieve a significant improvement in performance. Moreover, the summaries are usually produced for sighted people, but not for visually impaired people. A text summary can be translated into a braille summary for the reading of the visually impaired, and the length of a braille summary is defined as the number of the braille cells in the summary. Here, automatic translation plays a major role, as manual translation is a tedious and time-consuming task. In this work, we address the difficulty of long document summarization in braille. While there exists a considerable amount of research in the field of text summarization, most of it has focused on the summarization of short documents, with a particular focus on news articles. Research pertaining to long documents, such as books is very minimal. Further, books vary in length as well as in genre, necessitating the use of various summarization methods. In this paper, we elucidate the details of a new benchmark by compiling a dataset consisting of books with human-generated summaries obtained online used as a baseline, designed specifically for the generation of braille summaries. Due to the need for better encoding and various levels of attention on both words and sentences along with potential external memory units for storing farther but rather more significant information, we have used the bert-extractive-summarizer, a simple variant of BERT as an encoder for extractive summarization. Further, we have developed a novel, English & Braille interconversion library to generate braille summaries. Successful data collection was obtained for a total of 20 books. The dataset of books was collected based on variable length and genres for testing on a model which worked accurately well for different genres and long documents. The books in the given dataset have a mean length of 475258 words, including the summaries from Cliffnotes with a mean length of 7,476 words. Most books are of length between 1,08,049 to 8,93,246 words, and reference summaries are 2,837 to 15,236 words. For very long books, with more than 5,00,000 words, the summaries tend to become correspondingly longer. The result of EDA can be used as a basis for a better understanding of the dataset used for braille summarization. The Flesch Score measures the ease of readability of a given text. The Flesch reading score of all the books in the dataset generated is considered appropriate for all age groups above 11. As a result, children could share the pleasure of reading these books as well. The comparison with existing benchmark datasets is of less relevance due to the large disparity between the type of data in the text summarized in this project as compared to that of the benchmark datasets. While there has been a significant amount of work done in the domain of text summarization, the majority of research is concerned with short document datasets such as news articles. In this paper, we have attempted to overcome the existing gaps by focusing on long text document summarization, irrespective of domain and genres, independent of handcrafted features and thereby avoiding secondary or redundant information. We believe this paper has made major contributions in this relatively unexplored arena, by making an initial progress in collecting large document book data for future research in this field, in addition to setting a new benchmark for long document summarization using our novel English & Braille interconversion library. In our summarizer, the ROUGE Measure is quite accurate for our dataset with relatively lower error rates against the existing benchmark implementations. Provided with a good number of books available in English texts and now in braille as well, we hope that the concept of automation in braille book summarization will play a vital role in enabling informed choices of reading amongst the visually challenged. This work could be extended to design an end-to-end model by directly generating braille summarization from braille books. We believe that our work will ease up and enhance research concerned with English as well as braille book summarization. Automatic text summarization is a natural language processing (NLP) task of generating concise and precise version of a text document while retaining the essential information. Machine learning algorithms have been applied to this task to generate summaries from news, reviews, dialogs, or even scientific articles. This task has been typically classified into two categories, extractive summarization and abstractive summarization. Extractive summarization focuses on selecting important texts (usually sentences) from the original document and then concatenating them together in a summary form, whereas abstractive summarization involves generating novel sentences to form a summary from information extracted from the text. Abstractive summarization involves both; a knowledge modeling process and a language generation process, making it significantly more complex than extractive summarization. The improvement on automatic ROUGE metrics on the abstractive summarization task has reached a bottleneck due to its complexity. Recently, pre-trained language models have become popular for NLP tasks. Bidirectional Encoder Representations from Transformers (BERT) is one of the most popular pre-trained language models, which have been widely used NLP tasks. has modified the BERT model to make it usable for extractive summarization and included a decoder to produce abstractive summaries with the help of a multi-optimizer training mechanism. However, the performance of this model is highly dependent on the dataset as in case of news text summarization where the gold summaries are highly extractive in nature. In this paper, we put forward a simple abstractive summarization model named BeTS (Bert Transformer Summarizer), which consists of BERT as an encoder and a Transformer as a decoder. Unlike the existing models that use a modified version of BERT for summarization, our model adopts vanilla BERT without any modifications to the original architecture. We argue that even in its original form, with its pre-training on a huge dataset and its ability to extract rich and complex features, BERT can further boost the performance of abstractive summarization. In our work, we focus on transfer learning using our pre-trained model to abstractively summarize multiple kinds of input texts. We first train our model using the CNN/Daily Mail summarization corpus and then perform transfer learning on the AMI dialog summarization corpus. We also demonstrate that our model is significantly better in generalizing on unseen data and generates better abstractive summaries as shown in Table I. Even though BeTS, BertSumAbs, and BertSumExtAbs use BERT, BeTS is significantly different both in terms of network architecture as well as the training process. Both BertSumAbs and BertSumExtAbs have additional encoder layers over the vanilla BERT encoder layers but with a smaller decoder.

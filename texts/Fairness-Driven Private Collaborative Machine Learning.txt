Machine Learning (ML) is one of the most prominent technological developments in recent years, rapidly becoming a central part of our life. Applications of ML are all around us, ranging from traffic prediction and virtual personal assistants to automated radiology and autonomous vehicles. The performance of ML models inherently depends on the availability of a large quantity of useful training data. In neural network learning, for example, recent studies have shown that accuracy can be improved considerably by having access to large datasets. In many applications, however, data is scattered and held by multiple different parties that may be reluctant to share their information for multiple reasons such as commercial competition, privacy concerns, and in some domains even legal constraints. For example, the Health Insurance Portability and Accountability Act (HIPAA) in the United States places strict constraints on the ability of health care providers to share patient data. As a result, a variety of methods have been recently proposed to allow multiple parties to collaboratively train ML models while preventing the disclosure of private information. Hereinafter, we refer to this setting as Private Collaborative Machine Learning (PCML). While these methods address a very similar problem, they are often associated with different (although overlapping) research domains including privacy-preserving data mining, privacy-preserving ML, collaborative ML, and federated ML. These methods take two main approaches for preserving privacy. The first approach is based on perturbation, which incorporates noise to the training data to obscure sensitive information. Differential privacy is perhaps the most prominent perturbation technique. The main limitation of this approach is that incorporating noise to the data may yield an inferior model. The second approach is based on secure Multi-Party Computation (MPC). MPC achieves privacy protection by applying cryptographic techniques that enable several parties to perform a joint computation on private data that is distributed among them, where only the outcome of the computation is revealed to the parties, but no other information is exposed. In contrast to perturbation, MPC does not change the data, and therefore the output issued by such algorithms is identical to the output of their non-privacy-preserving counterparts. However, since in many cases a generic and perfectly secure MPC solution is infeasible, lower security requirements are typically accepted for the sake of higher efficiency. Despite the growing number of studies proposing algorithms for PCML, the fairness of such algorithms was overlooked. Since many automated decisions (including which individuals will receive jobs, loans, medication, bail or parole) can significantly impact peoples’ lives, there is great importance in assessing and improving the fairness of the decisions made by such algorithms. Indeed, in recent years, the concern for algorithmic fairness has made headlines. One of the most prominent examples was in the field of criminal justice, where recent revelations have shown that an algorithm used by the U.S. criminal justice system had falsely predicted future criminality among African-Americans at twice the rate as its parallel predictions for White people. These lines of evidence and concerns about algorithmic fairness have led to growing interest in the literature on defining, evaluating, and improving fairness in ML algorithms (see a recent review in other works). All of these studies, however, focused on centralized settings. In this article, we consider a learning setting similar to the PCML setting described previously, where data is scattered among several parties, who wish to engage in a joint ML procedure, without disclosing their private information. Our setting, however, also adds a fairness requirement, mandating that the learned model satisfies a certain level of fairness. To address the new fairness requirement, we suggest a privacy-preserving pre-process mechanism for enhancing fairness of collaborative ML algorithms. Similarly to the pre-process fairness mechanism suggested in the work of Feldman et al., our method improves fairness through decreasing distances between the distributions of attributes of the privileged and unprivileged groups. Our approach is not tailored to a specific algorithm and therefore can be used with any PCML algorithm. In contrast to Feldman et al., our method was designed to allow privacy-preserving enhancements, which are obtained through MPC techniques. An extensive evaluation that we conducted over real-world datasets shows that the proposed method is able to improve fairness considerably, with almost no compromise in accuracy. Furthermore, we show that the runtime of the proposed method is feasible, especially considering that it is executed once as a pre-process procedure. The article is organized as follows. We begin with a review of related work in Section 2. In Section 3, we define the problem that we study and present relevant notations that we shall be using. In Section 4, we describe our solution to this problem—a privacy-preserving fairness-enhancing mechanism. We present our experimental evaluation in Section 5 and conclude in Section 6. In this article, we proposed a privacy-preserving pre-process mechanism for enhancing fairness of collaborative ML algorithms. In particular, our method improves fairness by decreasing distances between the distributions of attributes of the privileged and unprivileged groups. We use a binning approach that enables the implementation of privacy-preserving enhancements, by means of MPC. Being a pre-process mechanism, our method is not limited to a specific algorithm, and therefore it can be used with any collaborative ML algorithm. An extensive evaluation conducted using three real-world datasets revealed that the proposed method is able to improve fairness considerably, with only a minor compromise in accuracy. We also showed that using a small number of bins (e.g., B = 3), it is possible to achieve that considerable improvement of fairness, with very minor and benign leakage of information. Finally, we demonstrated that the runtime of the proposed method is practical, especially considering that it is executed once as a pre-process procedure. Possible future research directions are as follows: Data distribution: We assumed a horizontal data distribution setting, where each party holds full records of a subset of the population. Forthcoming work may investigate a vertical distribution scenario, where each party holds a subset of the information about all individuals, or a mixed scenario that combines both horizontal and vertical distributions. Sensitive attribute: We assumed a single sensitive attribute that attains two possible values. While most studies in the algorithmic fairness literature make the same assumptions, future research should consider the case of multiple sensitive attributes that may attain more than just two values. Fairness mechanism: We proposed a private version of a pre-process fairness enhancing mechanism, inspired by Feldman et al. While it is clear qualitatively how our fairness mechanism’s parameters, B and λ, affect fairness, we do not offer theoretical bounds on their effect. Establishing such theoretical bounds could be a promising direction for future research. Moreover, future efforts should be invested in developing private versions of additional fairness enhancing mechanisms, including in-process and post-process mechanisms.

Text summarization is the process of effectively summarizing long sentences. Text summarization algorithms in machine learning are mainly divided into two types: extractive and abstractive summaries. In the former type, an input sentence is split into smaller sentences, and a summary sentence is generated by combining important sentences. On the other hand, the latter abstract type apprehends the input sentence and yields a corresponding summary sentence by itself. Both are similar in that they summarize the key points of the input sentence; however, extractive summaries can only process sentences gathered from the input sentence, while abstractive summaries generate summary sentences by themselves and are hence more flexible. In this study, we focus on abstract summaries. In recent years, various models have been proposed for abstract text summarization. Zhang et al. proposed an abstractive text summarization model using Bidirectional Encoder Representations from Transformers (BERT). Experimentation results as reported in the paper revealed that their model achieved new state-of-the-art performance on both CNN/Daily Mail and New York Times datasets. Viswani et al. proposed an abstractive text summarization model named Pointer-Generator Network based CopyNet. The Pointer-Generator Network model has advantages in both abstractive and extractive text summarizations. The model developed in this study was constructed with reference to the text summarization model using BERT and has two stages. In the first stage, input text was encoded into context representations using BERT, and the output was a draft summary text, generated from the input text processed with BERT using a Transformer-based decoder. In the second stage, the draft summary text was reverified using BERT for crisper summarizations. In our experiment, we employ only the first stage in our text summarization model. In this study, we build a text summarization model using BERT and evaluate the model. We also highlight future issues that could arise from the text generated as a result of training in the Japanese corpus. We conducted an experiment to demonstrate Japanese abstractive text summarization with a neural network model using BERT. Our model comprised a BERT encoder and Transformer-based decoder. The dataset used in this paper was the livedoor news corpus consisting of 130,000 data points, of which 100,000 were used for training. The results of the experiment revealed that the model was able to learn correctly as the summary sentence captured the key points of the text to some extent. However, the contents of the summary sentence were repeated, and the model was unable to handle unknown words. Additionally, there was a problem of simple word mistakes. We believe that the above problems could be solved by utilizing the coverage and copy mechanisms, and by improving the models. In the future, we will explore these recommendations with new experiments and compare the results.

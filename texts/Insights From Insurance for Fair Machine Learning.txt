Insurance is “interestingly uninteresting.” In this work, we argue that in fact insurance is far from uninteresting and indeed a rich source of inspiration and insight to scholarship interested in social issues surrounding machine learning, specifically the field now known as fair machine learning. Our proposal is that insurance can be viewed as an analogon to machine learning with respect to these issues arising from the social situatedness. While machine learning is a relatively recent technology, debates regarding social issues in the context of insurance have been ongoing for a long time. Thus, we argue that taking inspiration from studies of insurance can contribute to a more integrative view of machine learning systems as socio-technical systems. Both machine learning and insurance are firmly based on a statistical, probabilistic mode of reasoning — an actuarial mode. Indeed, insurance can be viewed as the first commercial test of probability theory. Insurance, a technology for doing risk, transforms uncertainty into calculable risk. The key idea is to share the risk of a loss in a collective, organized through an abstract mutuality; due to the ‘law’ of large numbers, uncertainty thus becomes manageable and the effect of chance can be offset. In this way, insurance creates a “community of fate” in the face of uncertainty. To enter into this community (the insurance pool), the insurer demands a certain fee, called premium, from the policyholder. In insurance, questions of fairness inevitably arise, and have been the subject of much debate. The central point of debate is the tension between risk assessment and distribution. In other words, who is to be mutualized in the pool. Some form of segmentation is found in many insurantial arrangements: the pool of policyholders can be stratified by separating high and low risk individuals. But the specific nature that such segmentation takes typically depends not only on risk assessment, but on further considerations such as assignment of responsibility, modulated by social context; in this way, insurance is not a neutral technology. Our non-comprehensive outline of the history of insurance illustrates how uncertainty, fairness and responsibility interact, and can be entangled and disentangled. From this background, we can extract conceptual insights which also apply to machine learning. The tension between risk assessment and distribution is mirrored in formal fairness principles: solidarity, which can be linked to independence in fair machine learning, contrasts with actuarial fairness, linked to calibration. Briefly, actuarial fairness demands that each policyholder should pay only for their own risk, that is, mutualization should occur only between individuals with the same ‘true’ risk. In contrast, solidarity calls for equal contribution to the pool. On one level of this text, we problematize actuarial fairness (by extension, calibration) as a notion of fairness in the normative sense by taking inspiration from insurance. This perspective is aligned with recent proposals that stress the discrepancy of formal algorithmic fairness and “substantive” fairness, which some prefer to call justice. Parallel to this runs a distinct textual level, where we emphasize two intricately interacting themes: responsibility and tensions between aggregate and individual. Both entail criticism of actuarial fairness, but we suggest that they additionally provide much broader, fruitful lessons for machine learning from insurance. At the highest level of abstraction, our goal is to establish a general conceptual bridge between insurance and machine learning. In this way, our approach shares similarities with the work of Hutchinson and Mitchell, who conceptually linked fair machine learning and testing in education. Traversing our conceptual bridge, machine learning scholars can obtain new perspectives on the social situatedness of a probabilistic, statistical technology — we attempt to offer a new ‘cognitive toolkit’ for thinking about the social situatedness of machine learning. Our point of view is that fairness cannot be reduced to a formal, mathematical issue, but that it requires taking broader social context into account, reasoning for instance about responsibility. And for this, we suggest, insurance is an insightful analogon. Therefore, our objective is to furnish the reader with a guide that charts the landscape of insurance with respect to social issues and to establish links to machine learning. We summarize our insights from insurance for machine learning in Section 7. On a formal level, we use the following analogy. In a machine learning task, we are given some features X and associated outcomes Y, which we attempt to approximate by predictions Ŷ. The structural relation to insurance is established by conceiving of X as the features of policyholders (e.g. age, gender) with outcomes Y (e.g. having an accident or not), and the task is to set a corresponding premium Ŷ. The main claim of our work is that insurance is an insightful analogon for the social situatedness and impact of machine learning systems. By traversing this conceptual bridge, machine learning scholars can make use of the rich and interdisciplinary literature on insurance. In particular, we suggest that the multifaceted concept of responsibility, tightly linked to causality and control, deserves more attention. We have illustrated problems with actuarial fairness as a notion of fairness in the normative sense. In this way, our suggestions are in line with others who demand moving beyond formal fairness to substantive fairness and argue that accurate predictive models need not be ‘fair’. While in this text we have focused on social issues, there are also technical lessons that machine learning could take from insurance, a technology for handling uncertainty. For instance, the problems of dataset shift and model ambiguity have been recognized in insurance as well as machine learning. On the other hand, the use of machine learning in insurance is increasing. We thus believe that a research agenda linking machine and learning and insurance may lead to a fruitful, two-way interaction of these fields. In summary, we offer the following insights. Recent impossibility theorems in the fair machine learning literature are not as surprising when considering them in the light of the old, fundamental tension in insurance between solidarity and actuarial fairness. In essence, this tension is grounded in how individuals are related to the aggregates they form. This relation rests on responsibilization. Responsibility and responsibilization should be conceptually distinguished, even if in the recent mode of personalized insurance (tightly linked to machine learning) the two are increasingly intertwined. For insurance and machine learning purposes, responsibility is a key ingredient in fairness, and reasoning about responsibility leads to causality and control. Unlike the prevailing trend in machine learning, our analysis underscores the conceptual priority of control over causality. We have emphasized that the relevant control question has a normative flavour, and thus cannot be left to engineers alone. What is under individuals’ control is often hotly contested. Finally, individual probability (risk) is not a viable concept and thus should not be invoked. As a general research heuristic, many case studies by social scholars of insurance can inspire analogous studies in the context of machine learning, covering a diverse set of topics; mining the literature is thus a rich source of inspiration.

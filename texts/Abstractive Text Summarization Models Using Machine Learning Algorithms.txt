The amount of text data is tremendously increasing daily on cyberspace and other repositories. Text summarization involves extracting important details contained in large volumes of text and presenting them in a brief, representative, and consistent summary. As the availability of literal data increases, the manual generation of summaries is no longer sufficient to meet demand. Text summarization is crucial in areas such as Natural Language Processing (NLP), Data Mining, Information Retrieval (IR), Multimedia, and Computer Science. Abstractive and Extractive summarization are two approaches to generating summaries from text documents. Extractive summarization selects key sentences or identifies major texts from the original content, while abstractive summarization generates new sentences that are semantically correct. With advancements in machine learning, particularly deep learning, the effectiveness of abstractive text summarization models has improved significantly, especially when using encoder-decoder frameworks. This paper presents an overview of abstractive text summarization by reviewing related scientific literature. It discusses the algorithms, methods, datasets, and evaluation metrics used in current abstractive text summarization models. The review covers various encoder-decoder mechanisms, the languages for which these models are designed, and the issues and challenges associated with these models. While some of these challenges have been addressed, others remain open for further research. The research gaps and future directions highlighted in this paper suggest that there is still significant room for improvement in abstractive text summarization models. Future research should focus on enhancing the efficiency and robustness of these models to meet the growing demands for accurate and contextually relevant summaries in various domains.

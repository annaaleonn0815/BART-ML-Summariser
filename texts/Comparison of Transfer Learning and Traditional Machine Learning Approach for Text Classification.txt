
Text classification is a sub-field of Natural Language Processing (NLP). It is used in many applications like news categorization, sentiment analysis, spam detection, recommendation systems, etc. In the review of literature, it is observed that different approaches like rule-based approach, statistical approach, and neural approach have been used to solve this task. Rule-based approach relies on manual writing of rules for building NLP systems and statistical approach also known as machine learning approach is based on feature engineering process in which we extract various features from text by using statistical methods. The limitation of both approaches is that they need domain expert for each NLP task to build task-specific model only. However, this problem is addressed by neural approach which is based on neural networks and recently became more popular to solve NLP tasks using deep learning. Automatic feature engineering is the key advantage of deep neural networks over non-neural methods used earlier for solving NLP tasks. Various deep learning-based neural networks like convolution neural networks, recurrent neural networks, graph neural networks, and attention mechanism are applied in the field of text classification. Deep learning-based pre-trained language models (PLMs) like ELMo, GPT, BERT, etc. have given rise to a technique called transfer learning (TL) in NLP. It is a process in which knowledge acquired while training a model on a particular task or domain can be reused to solve downstream task or domain. There are many NLP tasks and domains for which it is very difficult to find large scaled labelled data. PLMs are pre-trained on one or more tasks and can be reused to solve several other NLP tasks using the process of fine-tuning. Unlike machine learning, TL eliminates the need to build task-specific language models. LMs are pre-trained with a huge amount of unlabelled text and can be fine-tuned with a small amount of labelled data to perform various NLP tasks. Dai and Le first proposed the idea to pre-train language models. Among various PLMs, BERT is considered as state-of-the-art (SOTA) for TL due to its ability to predict context of the word from both directions, left and right. However, BERTâ€™s model size is very large and many research studies have been conducted in the literature to find optimized variants of BERT like DistilBERT, ALBERT, RoBERTa, TinyBERT, etc. Among these, DistilBERT is smaller, cheaper, and faster version of BERT. Therefore, we have selected BERT and its optimized version DistilBERT to compare transfer learning against classical machine learning approach based on Term Frequency - Inverse Document Frequency (TF-IDF) algorithm which is used to extract features from text. Transfer learning has shown remarkable results in computer vision and nowadays it is gaining momentum in the field of NLP as well. The motivation behind this research is to explore the potential of TL in NLP against traditional machine learning. The comparisons are made on the basis of experimental study on four different datasets used for text classification. In each experiment, different text classifiers are built using BERT, DistilBERT, and TF-IDF based machine learning approach. The rest of this paper is organized as follows. In section 2, related work is discussed. In section 3, methodology used to perform comparative study is described. In section 4, datasets, tools & techniques used, and experimental details are given. In section 5, results obtained in the experiments are presented. In section 6, conclusions are made along with future scope of work in section 7. The results of the comparative study based on experiments conducted on different datasets are summarized in Table II and Table III. It can be observed from Table II that BERT and DistilBERT are giving outstanding performance when fine-tuned with very less amount of data as compared to traditional machine learning models. Table III shows the comparative analysis of BERT and DistilBERT on the basis of training parameters and model accuracy. We have kept the same learning rate for both pre-trained models during the process of fine-tuning models for each dataset. In each experiment, it can be observed that DistilBERT can be trained with less number of training parameters and gives similar performance like BERT. This observation reveals that DistilBERT can be preferred over BERT and it can be used as default TL model for different applications of text classification. In this paper, we introduced the classical NLP approach based on TF-IDF and briefly described the pre-trained models like BERT and DistilBERT. A comparison was made between transfer learning and traditional machine learning approach based on four different experiments adding empirical evidence to state that transfer learning method is likely to give better results than traditional method. We found that the implementation of PLMs is far less complicated than classical NLP methods. This research study revealed that pre-trained models can be fine-tuned to give SOTA performance using very less amount of labelled data as compared to traditional machine learning models. Our work aimed to study the efficiency of TL in NLP. BERT has its own limitations in terms of model size, so we used DistilBERT (optimized version of BERT) to compare the results with BERT. From the experiments, it can also be concluded that DistilBERT can be used as default TL model for text classification.

Data preprocessing is a tedious task that often consumes a significant portion of the time spent on data science projects. This is partly because it is not a streamlined process but rather an iterative approach where the optimal processing steps and parameters are determined through trial and error. The goal of data preprocessing for machine learning is to find a data representation that yields the best results, meaning the format in which the machine learning model can most effectively map the input to the output. Data scientists measure the effectiveness of their preprocessing by analyzing machine learning metrics obtained when evaluating the model. However, this approach has drawbacks. Since multiple preprocessing steps are applied to the data, it is not always clear which steps improve or degrade model performance. Additionally, data preprocessing is just one factor that affects machine learning metrics, alongside model and hyperparameter selection. Non-experts often find it challenging to accurately assess the effects of their preprocessing pipeline due to the experimental nature of model training. Decisions regarding which preprocessing algorithms to use, which machine learning model is suitable, and what parameters are appropriate can be overwhelming, even for experienced data scientists. The AutoML community has invested considerable effort in addressing these issues, automating tasks such as hyperparameter optimization, neural architecture search, data preparation, and feature engineering. Although these methods can help broaden the adoption of machine learning, they often suffer from a lack of user trust, as users may not know if they can rely on these systems. A study highlighted the importance of transparency—displaying information that may have led to automated decisions—as a critical factor for building trust. Another issue necessitating the evaluation of data preparation is erroneous preprocessing, which can introduce technical bias, such as imputing missing values with the most frequent value, leading to data imbalance. To address these challenges, the paper proposes a transparency system for data preprocessing that provides insights into the preprocessing pipeline, helping to detect potential problems and errors and build trust in automatically or manually built pipelines. A key component of this system is a tracking mechanism that extracts metadata from preprocessing pipelines, with an emphasis on comprehensive summaries of the processed data. This metadata is then used to create visual presentations that facilitate the observation and assessment of the resulting pipeline. The system also supports formal comparisons between intermediate datasets produced by preprocessing tasks, offering a better understanding of the implications of each step. The paper presents an overview of the general approach and initial developments, highlights open challenges, and suggests possible solutions. It emphasizes the importance of evaluating the effects of preprocessing on both data and model performance, noting that this task is not always straightforward. The proposed transparency system collects comprehensive metadata on input, intermediate, and output data, providing data scientists with the information needed to evaluate data transformations and make informed decisions about pipeline design. A prototype and example use case were developed as a starting point for further research, which could include automatic metadata collection about preprocessing pipelines and approaches for visualizing changes in data between transformations. Another research direction could involve using transparency information to recommend preprocessing mechanisms by comparing different schemes based on data and change profiles generated by the proposed transparency system.

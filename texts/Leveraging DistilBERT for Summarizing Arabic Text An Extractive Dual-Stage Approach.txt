In this era of digitalization, tremendous amounts of textual data and electronic documents are exponentially pumping and advance to diffuse over the Internet rapidly. Habitually, online users find it time-consuming and effortful to find the desired information within a large body of text, particularly when filtering out the retrieved information using current search engines. This consideration has motivated many researchers to contribute to Automatic Text Summarization (ATS) within the Natural Language Processing (NLP) for saving user's effort and time. Here, ATS technologies can be of magnificent help in addressing the information overload problem by, e.g., suggesting solutions that work around shortening long texts with summaries without losing the intended meaning. Although ATS has been researched for approximately 60 years now, it still remains a very active research area as many problems related to text analysis and semantic complexities have not been solved entirely yet. Glancing over the ATS literature, one can observe that most intelligent studies and summarizing methodologies have been conducted to support the Indo-European languages, such as English, while limited works have been dedicated to supporting the Arabic language. While the Arabic language is a fast-growing language on the Internet with an annual growth rate of 9.348.0% (measured between 2000 - 2021), making it the fourth most spoken language in the world, it is less explored with respect to ATS due to its complex syntax, structure, and verb conjugation. Additionally, most Arabic-ATS approaches are experiencing poor to moderate performance due to the high linguistic complexity of the Arabic language itself. To clarify more, considering the low-resource Arabic NLP, the available supporting tools are suffering from many difficulties. Few of these many difficulties include (1) ambiguity in recognizing the POS-tagging correctly in the absence of diacritics, which is often the case; (2) ambiguity in identifying word-lemmatization (e.g., removing inflectional endings correctly), which is because the Arabic language has a rich derivational morphology; and (3) lack of having necessary parsing and tokenizing resources, including comprehensive lexicons. Furthermore, to the extent of our knowledge, few extractive-based techniques exist in Arabic summarization, and no work is suggested based on abstractive technique. The extractive technique means to shorten a given textual document by only focusing on essential sentences presented in the original documents and ignoring less important ones. The abstractive technique, however, can generate new texts that may not exist in the original documents. Broadly, there are different extractive-based techniques through which Arabic ATS is achieved by many researchers. These techniques can be categorized into different approaches: the statistical approach, the semantic approach, the machine learning approach, and the meta-heuristic approach. In a little more detail, Al-Abdallah and Al-Taani discuss the meta-heuristic approach in Arabic summarization and used Particle Swarm Optimization (PSO) algorithm to obtain extractive Arabic text summarization. Azmi and Al-Thanyyan used Rhetorical Structure Theory (RST) that is a statistical approach to form a candidate summary based on RST between the terms. In order to group the similar sentences statistically, a clustering method was used to extract the candidate summary by selecting the most important sentences in the clusters. The study presents the machine learning approach by using the Adaboost algorithm for Arabic text summarization. We will give more discussion on the closely related approaches in the next section. Seeking to investigate the capability of the deep transfer learning model to tackle the Arabic ATS problem, this paper presents a text summarization system based on a Distilled Bidirectional Encoder Representations from Transformers model (so-called DistilBERT). This model is one of the emerging state-of-the-art pre-trained language understanding models, distinguished by its lightness and efficiency compared to the large (base) BERT. In a nutshell, this paper introduces the following contributions: We introduce an Arabic summarizer approach (ArDBertSum) that consolidates two extractive summarization stages, built upon DistilBERT. The second supporter stage strives to enhance the typical summarization method used (i.e., depends on sentence-based selection) by further shortening long sentences. We propose a domain-specific sentence-clauses segmentation for Arabic texts (SCSAR). We make a new Arabic corpus available online for benchmarking human-based summarization approaches. Furthermore, we report on an implementation and experimental evaluation (including a user-based experiment), highlighting the efficient performance and practicality of our proposal. The implementation and coding details are publicly accessible to the interested researchers for replicating our experiments. In the following sections, we first review gaps in the literature concerning the Arabic ATS approaches and present noteworthy limitations of the existing techniques. Next, we introduce our proposal in detail that is Arabic-domain-specific, and then present the subsequent experimental analysis and findings. Summing up the paper and outlining the potential future avenues of research are given afterward. Automatic Text Summarization is deemed as one of the most complex NLP applications, particularly for the Arabic language that has not been intelligently developed like the other Indo-European languages. Towards producing a summarizer for text written in Arabic, relying on a pre-trained Language Understanding model (LUM), this paper has examined the ability of a fine-tuned version of DistilBERT in addressing the Arabic ATS concluded with offering a summarizer (ArDBertSum). To shorten long or complex sentences using our ArDBertSum, we have attempted to segment them into possibly independent clauses using the proposed SCSAR. Furthermore, we have carried out several experiments to assess the performance of ArDBertSum (with and without using SCSAR) against the related competitor Arabic summarizers. The experiments were conducted on two datasets (one of which is a well-known EASC corpus) utilizing ROUGE metrics. While ensuring the quality of candidate summaries produced by most of the ATS tools available today is a state of uncertainty, we have also conducted a specific human-based evaluation to verify the obtained performance measures. The human judges rated eight automatically-generated summaries, covering various domains, on five main criteria. Although a 75% text reduction rate was applied to the original articles, the judges deemed the produced summaries as acceptable. The main observations, however, focused on the coherence of the sentences and use of punctuations. The results of our experiments illustrated that ArDBertSum yields the best performance, compared with non-heuristic Arabic summarizers, in producing an acceptable quality of candidate summaries. A notable limitation to mention (i.e., besides the uncertainty problem in evaluating the ATS systems) is the dataset size (EASC) may not be comprehensively enough to evaluate the suggested ATS solutions. Therefore, towards the generality, we imagine a more refinement and extension of this ongoing work in three directions. The first direction is to evaluate our ArDBertSum on other datasets. Since there is a noticeable lack of available datasets for the Arabic ATS, we plan to create a specific benchmarking dataset, focusing on the formal Arabic texts. The second direction would be to study the evaluation metric for ATS by probably combining statistical metrics (e.g., ROUGE) along with human estimation. The third natural extension to this work would be to combine the abstractive summarization technique as a feature in our ATS solution. Rather than confining our approach to DistilBERT only where other significant competitors' models have emerged (such as Longformer, GPT2-3, RoBERTa, or XLMs), this third extension should incorporate a practical comparison among the state-of-the-art pre-trained NLU models in order to base our approach on the most appropriate version.

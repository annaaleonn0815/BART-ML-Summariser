The amount of textual material on the web and other libraries is growing tremendously daily. Information utilization has become an expensive and time-consuming activity since data expands in large quantities at a time and includes irrelevant content or noise. Text summarization is a method used to summarize the data. A manual text summarization process is undoubtedly an effective way to preserve the meaning of the text; however, this is a time-consuming activity. Another approach is to utilize automatic text summarization (ATS). In ATS, different practical algorithms can be programmed into computers to produce summaries of information. Thus, text summarization creates a brief and accurate overview of a lengthy text document by concentrating on the essential parts that provide valuable details while maintaining the overall context. In natural language processing (NLP), automatic text summarization is a method of evaluating, comprehending, and extracting information from human language. Nowadays, students to researchers, business leaders to business analysts, people from every domain work with numerous documents. Sometimes, people get confused about finding the relevant part within a document or documents where ATS can be very helpful and useful. The fundamental goal of ATS is to create a compact and persuasive summary and maintain critical information from the document. ATS also aims to generate a review that condenses the significant ideas from the input content into a small amount of space. Furthermore, the ATS systems assist users in obtaining the essential points of the original content without reading the complete document. Users will profit from the automatically generated summaries, saving them a great deal of time and work. The objectives of this study focus on providing a thorough review of various ATS research projects. To acquire deep knowledge, researchers require a sense of what has already been done and further possibilities in this broad topic. Therefore, this study aims to assist academics and professionals in developing an idea of the evolution of ATS, research progress, and future research directions in this topic. In addition, the apparent obstacles or limitations in future research in this field are also discussed in this paper. Text summarization was invented by H.P. Luhn in the 1950s, which was used in the first commercial computer IBM 701. Using the bag-of-words approach, he counted the frequencies of the most frequent words based on their occurrence. Then, the most frequent words were selected and assigned a number to each sentence depending on a regular event. Gradually, linguistics was being considered and began to use various word types and formation using natural language processing (NLP). The extraction, categorization, and classification of texts are the main targets. After that, the evolution of NLP between 1990 and 2000 introduced the conversion of sentences into vectors and words into their base forms. The introduction of advanced NLP techniques such as neural word embedding, Bag of Words (BoW), and word2vec, and modern deep learning approaches such as recurrent neural networks (RNN) and long short-term memory (LSTM) have observed significant progress in the ATS domain. The evolution of ATS from the 1950s to the present is reviewed in this study. Text summarization processes from the 1970s to the early 2000s are considered traditional methods. Traditional text summarization processes require a better knowledge of the document to find the essential keywords. ATS has become an appealing domain for its influential assistance in the study and expansion of automation, too. The improvement behind this new ATS is achieved by following a standard structure. Text summarization becomes more accurate and fluent by getting trimmed and interpreted with the proper design of processes. As we have investigated ATS in-depth in this comprehensive survey, the study required a collection of scholarly research between 1998 and 2021. We followed a systematic literature review (SLR) approach to complete the review. Kitchenham proposed this systematic literature review (SLR) approach which consists of three phases: planning, conducting, and reporting the review. The SLR approaches tried to answer all possible questions that could arise while progressing in this research field. The goal of this research was to examine the findings of several essential research disciplines. The necessary materials for this research are assembled using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses work flow diagram. The PRISMA work flow for this survey is shown in Figure 1. The overall contributions of this paper are given as follows: This article performs a systematic review of the automatic text summarization, including the fundamental theories and evolutions. The survey includes the investigation of the existing dataset, feature extraction, text summarization approaches, text summarization algorithms, performance measurement, evaluation matrices, and challenges. The article compiles ATS architectures based on current methods, datasets, feature extraction, and summarization approaches. Moreover, this study explains the constraints and limitations of such methods. Subsequently, the study ends by distinguishing the current difficulties and challenges of ATS architectures, along with future research directions. The remainder of this paper is organized as follows: The literature review of existing ATS surveys shown in Section II, the motivations and applications of ATS are described in Section III, the basic structure is provided in Section IV, the most commonly used datasets in ATS are described in Section V, the widely used pre-processing techniques are addressed in Section VI, the strategies for extracting features are described in Section VII, main ATS approaches are discussed in VIII and algorithms are described in Section IX. The ATS approaches are reviewed in Section X and the ATS measuring performance methods are discussed in Section XI. The ATS challenges with potential research objectives are addressed in Section XII. Finally, Section XIII concludes the paper. Text summarization is an old topic, but this field continues to gain the interest of researchers. Nonetheless, the performance of text summarization is average in general, and the summaries created are not always ideal. As a result, researchers are attempting to improve existing text-summarizing methods. In addition, developing novel summarization approaches to produce higher-quality, human standards, and robust summaries is a priority. Therefore, ATS should be made more intelligent by combining it with other integrated systems to perform better. Automatic text summarization is an eminent domain of research that is extensively implemented and integrated into diverse applications to summarize and reduce text volume. In this paper, we present a systematic survey of the vast ATS domain in various phases: the fundamental theories with previous research backgrounds, dataset inspections, feature extraction architectures, influential text summarization algorithms, performance measurement matrices, and challenges of current architectures. This paper also presents the current limitations and challenges of ATS methods and algorithms, which would encourage researchers to try to solve these limitations and overcome new challenges in the ATS domain.

Machine learning is arguably the main driving force behind the rapid growth of modern Artificial Intelligence (AI) technology. Accordingly, the race for designing more accurate and versatile AI systems has led to an unprecedented rise in the complexity of machine learning tasks. In particular, both the models and the training datasets are growing in size by the day, especially when considering complex tasks such as natural language processing. To keep up with the growing computational demand of these machine learning tasks, it is now common to heavily rely upon distributed methodologies. This field is commonly referred to as distributed machine learning. Essentially, the training procedure is fragmented into several (simpler) sub-tasks that are distributed on different machines (or nodes). The nodes perform their respective sub-tasks in parallel and coordinate their local actions with each other, either by using a trusted authority (usually referred to as a server) or by interacting directly with each other. The former scheme is sometimes called server-based coordination, and it includes the celebrated federated learning mechanism. As for the latter, it is often called peer-to-peer coordination. Many prominent distributed machine learning algorithms consist of distributing among the nodes the expensive task of gradient computations in the renowned Stochastic Gradient Descent (SGD) method. Such a distributed implementation of SGD, collectively referred to as Distributed Stochastic Gradient Descent (D-SGD), divides the workload for each node by the total size of the system (i.e., the total number of nodes), thereby rendering the training of large complex models less cumbersome. Moreover, distributed machine learning algorithms do not require the nodes to share their respective data and therefore naturally provide the nodes some level of sovereignty over their training dataset. These appealing benefits of distributed machine learning have carried this field to the forefront of machine learning (and AI) research, in industry and academia alike. We have introduced a new perspective on the problem of Byzantine machine learning to standardize the evaluation of different solutions. Our approach presents a clear outlook on the merits and limitations of prominent existing solutions. Our introduced system of evaluation is generic and can be utilized for evaluating other methods that were not explicitly considered in the article. Finally, our systematization also allowed us to identify promising upcoming trends in Byzantine machine learning. Next, we present the primary takeaway from our survey. An ideal solution to Byzantine machine learning should attain optimality in all the three fronts: (i) tolerate up to half of the Byzantine nodes, (ii) ensure a training loss that matches the established lower bound, and (iii) have minimal overhead on gradient computations. However, existing schemes seem to only ensure optimality on at most two out of the three metrics. To put it succinctly, existing works exhibit tensions between breakdown point, robustness, and gradient complexity. Hence, an important question that remains to be addressed is the following: Does there exist a fundamental tradeoff that hinders the simultaneous optimization of breakdown point, robustness, and gradient complexity in Byzantine machine learning?

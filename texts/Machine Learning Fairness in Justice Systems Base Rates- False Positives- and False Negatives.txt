As machine learning best practice and ethical practice guidelines have proliferated, there has been a lack of consensus on what should be included in the rules. Each set of ethical guidelines is unique, but common themes have emerged. One recurrent axiom is that machine learning models should be fair and equitable. However, fairness is not always clearly defined, and there is no consensus on what constitutes fairness. Many papers have catalogued different definitions of fairness that might be applied to machine learning. For example, Verma and Rubin identify more than twenty different forms of equity proposed for machine learning. Fairness and equity in machine learning are particularly critical when applied to justice settings. Machine learning used to create decision support tools for determining who should enter the system, be diverted, receive specific dispositions, or be placed in secured facilities must be fair. Such decisions, with their significant consequences, must align with the principles of justice as set out in the U.S. Constitution. The Equal Protection Clause of the Fourteenth Amendment mandates that government entities, including justice systems, must treat individuals in similar situations equally. While this ethical standard is clear, it is not obvious how machine learning can uphold this Constitutional right. Most ethical machine learning guidelines provide a stance on fairness but lack detailed guidance on how to operate in settings that are not inherently fair or with data tainted by historical prejudice or structural inequality. For instance, IBM suggests that AI should be designed to minimize bias and promote inclusive representation. However, the path to minimizing bias is often unclear. Most justice systems do not explicitly address which types of errors are worse or quantify their impact. This lack of targeted consideration means that specific class weights for justice system decision support systems, risk assessments, and machine learning models are rarely established. Validation reports typically focus on overall accuracy, such as the area under the Receiver Operating Characteristic curve, or AUC, rather than examining false positives or false negatives. For example, the "Validation Study of the Youth Level of Service (YLS) Assessment in Hennepin County" does not address errors or mention false positives or negatives, while the "Long-Term Validation of the Youth Assessment and Screening Instrument (YASI) in New York State Juvenile Probation" reports AUC statistics without considering error rates. Achieving fairness in machine learning within the justice system cannot be accomplished through computation alone. Better algorithms, additional features, or rebalanced samples are insufficient for ensuring fairness. Sophisticated statistical methods, such as latent variable analysis, structural equation modeling, propensity score matching, or factor analysis, also fall short of addressing fairness concerns. The path to fairness in machine learning requires a deeper examination of the fairness of the underlying decision-making processes. This involves careful consideration of each decision point, incorporating input from leaders, workers, stakeholders, and impacted communities to clarify the systemâ€™s purpose, intervention goals, and objectives of punishment. Government agencies, supported by data and machine learning models, can advance fairness discussions by engaging in open dialogues with a broad range of voices about community desires and tolerances for achieving those goals.

During software evolution, fixing defects is often costly, particularly when discovered late in development or after release. Defect prediction plays a crucial role in software quality assurance by identifying defective software units early, which aids in improving testing and defect fixes and focuses quality assurance efforts. With the increasing size and complexity of code, manually inspecting defects across all software units becomes impractical, necessitating automated prediction to facilitate this process. Researchers have explored various techniques for predicting defects at different granularity levels, such as file, method, or system levels. However, there is limited research on package-level defect prediction. Package-level prediction offers a useful granularity that is finer than the system level but coarser than the file or method level. It allows for targeting logical groups of code aligned with features, facilitating the identification of issues in specific features. Packages, which encapsulate logical functionality and are often developed by specific teams, can provide valuable insights into defective designs or implementations within a system. This paper addresses this gap by developing package-level defect prediction models. We began by traversing a projectâ€™s source code to extract all its packages, calculating code metrics for each package, and labeling it as defective or not based on defectiveness derived from the project's bug data. To handle the challenge of noisy or redundant variables, we employed the CFS method to select an appropriate set of code metrics for each dataset. Using these metrics as input attributes, we applied seven machine learning algorithms to build defect prediction models at the package level. Evaluating twenty open-source projects with 6,536 identified packages, our results demonstrated that machine learning techniques, combined with an appropriate set of code metrics, can effectively predict defective packages. However, different projects may require different sets of code metrics for their prediction models. The paper is structured as follows: Section 2 presents background and related works, Section 3 describes the empirical study design, Section 4 covers evaluation methods, analysis, and results, Section 5 discusses threats to validity, and Section 6 concludes. We calculated bug rates by identifying bug-fixing commits through pattern matching on commit messages. Prior studies have noted limitations in this approach, such as the possibility of files changed in bug-fixing commits not being related to the bug, and missing bug fixes without ticket IDs. Thus, the extracted bug-fixing commits might be biased, affecting internal validity. We plan to combine our ticket-based approach with a keyword-based method in future work. Additionally, analyzing only twenty open-source projects poses a threat to external validity, though we selected projects of various sizes and domains. Extending experiments to more projects, particularly commercial ones, could provide further insights. Moreover, our findings might be limited in generalizability since we only analyzed projects using Git and JIRA. Future work will involve applying our approach to projects using other version control and issue-tracking systems, such as SVN, Bugzilla, or TFS. This work presents the development of defect prediction models at the package level using machine learning algorithms and code metrics. By selecting a set of code metrics based on correlation analysis for each project, we built prediction models with seven algorithms and assessed their performance through ten-fold cross-validation. Our experiments show that machine learning algorithms, with an appropriate set of metrics, can achieve accurate package-level predictions for each project. However, no single set of metrics is universally effective across all projects. In summary, our work demonstrates that machine learning models can provide reliable package-level defect predictions, guiding testing and quality improvement efforts, and highlights the need for customized predictive attribute subsets based on careful attribute selection.

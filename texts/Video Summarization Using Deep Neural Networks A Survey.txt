In July 2015, YouTube revealed that it receives over 400 hours of video content every single minute, which translates to 65.7 years’ worth of content uploaded every day. Since then, we are experiencing an even stronger engagement of consumers with both online video platforms and devices (e.g., smartphones and wearables) that carry powerful video recording sensors and allow instant uploading of the captured video on the Web. According to newer estimates, YouTube now receives 500 hours of video per minute; YouTube is just one of the many video hosting platforms (e.g., DailyMotion and Vimeo), social networks (e.g., Facebook, Twitter, and Instagram), and online repositories of media and news organizations that host large volumes of video content. Thus, how is it possible for someone to efficiently navigate through endless collections of videos and find the video content that she/he is looking for? The answer to this question comes not only from video retrieval technologies but also from technologies for automatic video summarization. The latter allows generating a concise synopsis that conveys the important parts of the full-length video. Given the plethora of video content on the Web, effective video summarization facilitates viewers’ browsing of and navigation in large video collections, thus increasing viewers’ engagement and content consumption. The application domain of automatic video summarization is wide and includes (but is not limited to) the use of such technologies by media organizations (after integrating such techniques into their content management systems), to allow effective indexing, browsing, retrieval, and promotion of their media assets, and video sharing platforms, to improve the viewing experience, enhance viewers’ engagement, and increase content consumption. In addition, video summarization that is tailored to the requirements of particular content presentation scenarios can be used for, e.g., generating trailers or teasers of movies and episodes of a TV series; presenting the highlights of an event (e.g., a sports game, a music band performance, or a public debate); and creating a video synopsis with the main activities that took place over, e.g., the last 24 hours of recordings of a surveillance camera, for time-efficient progress monitoring or security purposes. A number of surveys on video summarization have already appeared in the literature. In one of the first works, Barbieri et al. classify the relevant bibliography according to several aspects of the summarization process, namely the targeted scenario, the type of visual content, and the characteristics of the summarization approach. In another early study, Li et al. divide the existing summarization approaches into utility-based methods that use attention models to identify the salient objects and scenes, and structure-based methods that build on the video shots and scenes. Truong and Venkatesh discuss a variety of attributes that affect the outcome of a summarization process, such as the video domain, the granularity of the employed video fragments, the utilized summarization methodology, and the targeted type of summary. Money and Agius divide the bibliography into methods that rely on the analysis of the video stream, methods that process contextual video metadata, and hybrid approaches that rely on both types of the aforementioned data. Jiang et al. discuss a few characteristic video summarization approaches that include the extraction of low-level visual features for assessing frame similarity or performing clustering-based key-frame selection; the detection of the main events of the video using motion descriptors; and the identification of the video structure using eigenfeatures. Hu et al. classify the summarization methods into those that target minimum visual redundancy, those that rely on an object or event detection, and others that are based on multimodal integration. Ajmal et al. similarly classify the relevant literature in clustering-based methods, approaches that rely on detecting the main events of the story, and so on. Nevertheless, all the aforementioned works (published between 2003 and 2012) report on early approaches to video summarization; they do not present how the summarization landscape has evolved over the last years and especially after the introduction of deep learning algorithms. The more recent study of del Molino et al. focuses on egocentric video summarization and discusses the specifications and the challenges of this task. In another recent work, Basavarajaiah and Sharma provide a classification of various summarization approaches, including some recently proposed deep-learning-based methods; however, their work mainly focuses on summarization algorithms that are directly applicable to the compressed domain. Finally, the survey of Vivekraj et al. presents the relevant bibliography based on a two-way categorization that relates to the utilized data modalities during the analysis and the incorporation of human aspects. With respect to the latter, it further splits the relevant literature into methods that create summaries by modeling the human understanding and preferences (e.g., using attention models, the semantics of the visual content, or ground-truth annotations, and machine-learning algorithms) and conventional approaches that rely on the statistical processing of low-level features of the video. Nevertheless, none of the above surveys presents, in a comprehensive manner, the current developments toward generic video summarization, which are tightly related to the growing use of advanced deep neural network architectures for learning the summarization task. As a matter of fact, the relevant research area is a very active one as several new approaches are being presented every year in highly ranked peer-reviewed journals and international conferences. In this survey, we study in detail more than 40 different deep-learning-based video summarization algorithms among the relevant works that have been proposed over the last five years. In addition, a comparison of the summarization performance reported in the most recent deep-learning-based methods against the performance reported in other more conventional approaches shows that, in most cases, the deep-learning-based methods significantly outperform more traditional approaches that rely on weighted fusion, sparse subset selection, or data clustering algorithms and represent the current state of the art in automatic video summarization. Motivated by these observations, we aim to fill this gap in the literature by presenting the relevant bibliography on deep-learning-based video summarization and also discussing other aspects that are associated with it, such as the protocols used for evaluating video summarization. This article begins in Section II by defining the problem of automatic video summarization and presenting the most prominent types of video summary. Then, it provides a high-level description of the analysis pipeline of deep-learning-based video summarization algorithms and introduces a taxonomy of the relevant literature according to the utilized data modalities, the adopted training strategy, and the implemented learning approaches. Finally, it discusses aspects that relate to the generated summary, such as the desired properties of a static (frame-based) video summary and the length of a dynamic (fragment-based) video summary. Section III builds on the introduced taxonomy to systematically review the relevant bibliography. A primary categorization is made according to the use or not of human-generated ground-truth data for learning, and a secondary categorization is made based on the adopted learning objective or the utilized data modalities by each different class of methods. For each one of the defined classes, we illustrate the main processing pipeline and report on the specifications of the associated summarization algorithms. After presenting the relevant bibliography, we provide some general remarks that reflect how the field has evolved, especially over the last five years, highlighting the pros and cons of each class of methods. Section IV continues with an in-depth discussion on the utilized datasets and the different evaluation protocols of the literature. Following, Section V discusses the findings of extensive performance comparisons that are based on the results reported in the relevant papers, indicates the most competitive methods in the fields of (weakly) supervised and unsupervised video summarization, and examines whether there is a performance gap between these two main types of approaches. Based on the surveyed bibliography, in Section VI, we propose potential future directions to further advance the current state of the art in video summarization. Finally, Section VII concludes this work by briefly outlining the core findings of our study. In this work, we provided a systematic review of the deep-learning-based video summarization landscape. This review allowed us to discuss how the summarization technology has evolved over the last years and what is the potential for the future, as well as to raise awareness to the relevant community with respect to promising future directions and open issues. The main conclusions of this study are outlined in the following Concerning the summarization performance, the best-performing supervised methods thus far learn frames’ importance by modeling the variable-range temporal dependence among video frames/fragments with the help of recurrent neural networks and tailored attention mechanisms. The extension of the memorization capacity of LSTMs by using memory networks has shown promising results and should be further investigated. In the direction of unsupervised video summarization, the use of GANs for learning how to build a representative video summary seems to be the most promising approach. Such networks have been integrated into summarization architectures and used in combination with attention mechanisms or actor–critic models, showing a summarization performance that is comparable to the performance of state-of-the-art supervised approaches. Given the objective difficulty to create large-scale datasets with human annotations for training summarization models in a supervised way, further research effort should be put on the development of fully unsupervised or semisupervised/weakly supervised video summarization methods that eliminate or reduce to a large extent the need for such data and facilitate adaptation to the summarization requirements of different domains and application scenarios. Regarding the evaluation of video summarization algorithms, there is some diversity among the used evaluation protocols in the bibliography, which is associated with the way that the used data are being divided for training and testing purposes, the number of the conducted experiments using different randomly created splits of the data, and the used data splits; concerning the latter, a recent work showed that different randomly created data splits of the SumMe and TVSum datasets are characterized by considerably different levels of difficulty. All the above raise concerns regarding the accuracy of performance comparisons that rely on the results reported in the different papers. Moreover, there is lack of information in the reportings of several summarization works, with respect to the applied process for terminating the training process and selecting the trained model. Hence, the relevant community should be aware of these issues and take the necessary actions to increase the reproducibility of the results reported for each newly proposed method. Last but not least, this work indicated several research directions toward further advancing the performance of video summarization algorithms. Besides these proposals for future scientific work, we believe that further efforts should be put toward the practical use of summarization algorithms, by integrating such technologies into tools that support the needs of modern media organizations for time-efficient video content adaptation and reuse.

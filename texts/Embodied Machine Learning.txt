Machine learning (ML) continues to gain prevalence across various domains, including finance, policing, and advertising. Particularly in healthcare, ML has been embraced by researchers, who are actively working on a range of issues from interpreting high-dimensional medical data to predicting cancer. However, despite this widespread adoption in ML research, it has not been uniformly accepted by domain experts in the field, with the integration of artificial intelligence (AI) models in medical workflows often eliciting mixed emotions. While some experts embrace the potential of leveraging AI, others express fear and distrust in AI systems. Therefore, there’s a serious challenge of improving trust in medical AI models that must be addressed before we can assume adoption of said models. Researchers have made efforts to tackle this distrust by developing Explainable AI (XAI) systems, aiming to install rational trust through explanations of AI models, such as saliency maps and model visualizations. However, most of this work primarily considers engendering rational trust with regards to an existing AI model after its development, with little research exploring means for engendering trust during the model development process through affective and normative trust between those creating the model. To address this research gap, we introduce the Embodied Machine Learning (EML) system, which aims to engender affective and normative trust in AI through frequent interaction and collaboration between data scientists and domain experts during the medical AI model development process. We achieve this by shifting the process of labeling and model fine-tuning from the desktop to a large multi-touch interface, capitalizing on the benefits of embodied interactions and large form factor displays to promote collaboration and sensemaking. The system aims to achieve the following goals: increasing affective trust through increased collaboration between domain experts and data scientists during the training process, and increasing normative trust through increased interactions between domain experts and the machine learning models they train. To evaluate these goals, we present a user study, and its results highlighted increases in drivers of both normative and affective trust, increased trust in the resulting model, as well as more pointed model goals. We’ve presented EML, a system for embodied machine learning training around a multi-touch tabletop display, with the goal of increasing affective and normative trust in medical ML research. Based on the related work, we’ve highlighted deficiencies in engendering trust in current medical ML research workflows, as well as methods for addressing them through embodied interactions at large form factor displays. Building on this related work, we then presented the design of the EML system and provided both an expert evaluation and a user study to collect feedback on our design. From these evaluations, we have highlighted the efficacy of the EML system in engendering trust and collaboration, while also noting areas for future work.

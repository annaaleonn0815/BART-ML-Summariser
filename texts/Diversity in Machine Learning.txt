Traditionally, machine learning methods can learn model's parameters automatically with the training samples and thus it can provide models with good performances which can satisfy the special requirements of various applications. Actually, it has achieved great success in tackling many real-world artificial intelligence and data mining problems, such as object detection, natural image processing, autonomous car driving, urban scene understanding, machine translation, and web search/information retrieval, and others. A successful machine learning system often requires plentiful training data which can provide enough information to train the model, a good model learning process which can better model the data, and an accurate inference to discriminate different objects. However, in real-world applications, a limited number of labeled training data is available. Besides, there exist large amounts of parameters in the machine learning model. These would make the "overfitting" phenomenon in the machine learning process. Therefore, obtaining an accurate inference from the machine learning model tends to be a difficult task. Many factors can help to improve the performance of the machine learning process, among which the diversity in machine learning plays an important role. Diversity shows different concepts depending on context and application. Generally, a diversified system contains more information and can better fit various environments. It has already become an important property in many social fields, such as biological systems, culture, products, and so on. Particularly, the diversity property also has significant effects on the learning process of the machine learning system. Therefore, we wrote this survey mainly for two reasons. First, while the topic of diversity in machine learning methods has received attention for many years, there is no framework of diversity technology on general machine learning models. Although Kulesza et al. discussed the determinantal point processes (DPP) in machine learning, which is only one of the measurements for diversity. Reference mainly summarized the diversity-promoting methods for obtaining multiple diversified search results in the inference phase. Besides, analyzed several methods on classifier ensembles, which represent only a specific form of ensemble learning. All these works do not provide a full survey of the topic, nor do they focus on machine learning with general forms. Our main aim is to provide such a survey, hoping to induce diversity in the general machine learning process. As a second motivation, this survey is also useful to researchers working on designing effective learning processes. Here, the diversity in machine learning works mainly on decreasing the redundancy between the data or the model and providing informative data or representative models in the machine learning process. This work will discuss the diversity property from different components of the machine learning process, including the training data, the learned model, and the inference. The diversity in machine learning tries to decrease the redundancy in the training data, the learned model as well as the inference and provide more information for the machine learning process. It can improve the performance of the model and has played an important role in the machine learning process. In this work, we summarize the diversification of machine learning into three categories: the diversity in training data (data diversification), the diversity of the model/models (model diversification), and the diversity of the inference (inference diversification). Data diversification can provide samples with enough information to train the machine learning model. The diversity in training data aims to maximize the information contained in the data. Therefore, the model can learn more information from the data via the learning process and the learned model can be better fit for the data. Many prior works have imposed diversity on the construction of each training batch for the machine learning process to train the model more effectively. In addition, diversity in active learning can also make the labeled training data contain the most information, and thus the learned model can achieve good performance with limited training samples. Moreover, in special unsupervised learning methods, diversity of the pseudo classes can encourage the classes to repulse from each other, and thus the learned model can provide more discriminative features from the objects. Model diversification comes from the diversity in the human visual system. References have shown that the human visual system represents decorrelation and sparseness, namely diversity. This makes different neurons in the human learning respond to different stimuli and generates little redundancy in the learning process which ensures the high effectiveness of human learning. However, general machine learning methods usually perform redundancy in the learned model where different factors model similar features. Therefore, diversity between the parameters of the model (D-model) could significantly improve the performance of the machine learning systems. The D-model tries to encourage different parameters in each model to be diversified and each parameter can model unique information. As a result, the performance of each model can be significantly improved. However, general machine learning models usually provide a local optimal representation of the data with limited training data. Therefore, ensemble learning, which can learn multiple models simultaneously, becomes another hot machine learning method to provide multiple choices and has been widely applied in many real-world applications, such as speech recognition and image segmentation. However, general ensemble learning usually makes the learned multiple base models converge to the same or similar local optima. Thus, diversity among multiple base models by ensemble learning (D-models), which tries to repulse different base models and encourages each base model to provide choice reflecting multi-modal belief, can provide multiple diversified choices and significantly improve performance. Instead of learning multiple models with D-models, one can also obtain multiple choices in the inference phase, which is generally called multiple choice learning (MCL). However, the obtained choices from usual machine learning systems present similarity between each other where the next choice will be one-pixel shifted versions of others. Therefore, to overcome this problem, diversity-promoting priors can be imposed over the obtained multiple choices from the inference. Under the inference diversification, the model can provide choices/representations with more complement information. This could further improve the performance of the machine learning process and provide multiple discriminative choices of the objects. This work systematically covers the literature on diversity-promoting methods over data diversification, model diversification, and inference diversification in machine learning tasks. In particular, three main questions from the analysis of diversity technology in machine learning have arisen. How to measure the diversity of the training data, the learned model/models, and the inference and enhance this diversity in the machine learning system, respectively? How do these methods work on the diversification of the machine learning system? Is there any difference between the diversification of the model and models? Furthermore, is there any similarity between the diversity in the training data, the learned model/models, and the inference? Which real-world applications can diversity be applied to improve the performance of the machine learning models? How do the diversification methods work on these applications? Although all of the three problems are important, none of them has been thoroughly answered. Diversity in machine learning can balance the training data, encourage the learned parameters to be diversified, and diversify the multiple choices from the inference. Through enforcing diversity in the machine learning system, the machine learning model can present a better performance. Following the framework, the three questions above have been answered with both the theoretical analysis and the real-world applications. The remainder of this paper is organized as Fig. 1 shows. Section II discusses the general forms of the supervised learning and the active learning as well as a special form of unsupervised learning in machine learning model. Besides, as Fig. 1 shows, Sections III, IV and V introduce the diversity methods in machine learning models. Section III outlines some of the prior works on diversification in training data. Section IV reviews the strategies for model diversification, including the D-model, and the D-models. The prior works for inference diversification are summarized in Section V. Finally, section VI introduces some applications of the diversity-promoting methods in prior works, and then we do some discussions, conclude the paper and point out some future directions. This article surveyed the available works on diversity technology in general machine learning model, by systematically categorizing the diversity in training samples, D-model, D-models, and inference diversity in the model. We first summarize the main results and identify the challenges encountered throughout the article. Recently, due to the excellent performance of the machine learning model for feature extraction, machine learning methods have been widely applied in real-world applications. However, the limited number and imbalance of training samples in real-world applications usually make the learned machine learning models be sub-optimal, sometimes even lead to the "overfitting" in the training process. This would limit the performance of the machine learning models. Therefore, this work summarizes the diversity technology in prior works which can work on the machine learning model as one of the methods to improve the model's representational ability. We want to emphasize that the diversity technology is not decisive. The diversity can only be considered as an additional technology to improve the performance of the machine learning process. Through the introduction of the diversity in machine learning, the three questions proposed in the introduction can be easily answered. The detailed descriptions of data diversification, model diversification, and inference diversification are introduced in sections III, IV, and V. With these methods, the machine learning model can be diversified and the performance can be improved. Besides, the diversification of the model (D-model) tries to improve the representational ability of the machine learning model directly (see Fig. 5 for details) while the diversification of the models (D-models) aims to obtain multiple diversified choices under the diversification of the ensemble learning (see Fig. 6 for details). It should also be noted that the diversification measurements for data diversification, model diversification, and the inference diversification show some similarity. As introduced, the diversity aims to decrease the redundancy between the data or the factors. The key problem for diversification is the way to measure the similarity between the data or the factors. However, in the machine learning process, the data and the factors are processed as vectors. Therefore, there exist overlaps between the data diversification, model diversification as well as the inference diversification, such as the DPP measurement. More importantly, we should also note that the diversification in different steps of machine learning models presents its special characteristics. The details for the applications of diversity methods are shown in section VI. This work only lists the most common applications in real-world. The readers should consider whether the diversity methods are necessary according to the specific task they face with. Advice for Implementation: We expect this article is useful to researchers who want to improve the representational ability of machine learning models for computer vision tasks. For a given computer vision task, the proper machine learning model should be chosen first. Then, we advise to consider adding diversity-promoting priors to improve the performance of the model and further what type of diversity measurement is desired. When one desires to obtain multiple models or multiple choices, then one can consider diversifying multiple models or the obtained multiple choices and section IV-B and V would be relevant and helpful. We advise the reader to first consider whether the multiple models or multiple choices can be helpful for the performance. The training of machine learning models requires large amounts of labelled samples. However, the limited training samples constrain the performance of machine learning models. Therefore, effective diversity technology, which can encourage the model to be diversified and improve the representational ability of the model, is expected to be an active area of research in machine learning tasks. This paper summarizes the diversity technology for machine learning in previous works. We introduce diversity technology in data pre-processing, model training, inference, respectively. Other researchers can judge whether diversity technology is needed and choose the proper diversity method for the special requirements according to the introductions in former sections.

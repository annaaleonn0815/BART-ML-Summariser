In current era, information is considered as an inevitable asset. Nowadays, plenty of data is available on the internet, which is being raised day by day, rapidly. According to International Data Corporation, measured data circulating the globe during 2013 was approximately 4.4 zettabytes, which will be elevated to 180 zettabytes in 2025. Therefore, there will be need to shorten text in order to reduce overheads. Additionally, short text reduces searching time, reading time, and augments total quantity of details that can apt within the same area. In this scenario, it is very vital to have a mechanism, which eliminates repetitive as well as less significant data from the pile of information. The resultant data thus obtained may still contain plenty of information, which can be reduced further into just glance of the information, called summary; revealing the view of writer. This task of summarization becomes very challenging, since it includes natural processing of data. To perform such task in machinelike manner, machine has to capture all available raw data and generate semantically correct sentence pattern. In this era of technology, huge amount of data is available in each and every field of the world. Data mining plays crucial role to deal with large data. When it comes to analysis of reviews of a product, there are plethora of reviews available on the internet. It is tedious and time consuming task to go through each and every product review manually. To ease this process, automatic text summarization becomes useful. In this advance field of text summarization, day by day, new technologies are developed to ease the process of summarization. Still, there are some flaws in already developed technologies, which can be handled by developing alternate options to make it efficient. Abstractive text summarization generated by LSTM encoder-decoder model using beam search strategy with linear normalization at inference decoder and attention mechanism can improve text summarization results compared to greedy strategy in same.

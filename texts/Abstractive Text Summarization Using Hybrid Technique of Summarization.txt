Over time, text summarization has played an increasingly important role in managing the vast amounts of web-based data. Due to the overwhelming volume of information, there is a growing demand for automated abstractive summarizers. Users often search the internet for specific information, but finding it after browsing multiple web pages can be time-consuming and tedious. Automatic text summarization (ATS) is a core component of natural language processing (NLP) and is utilized in various domains such as newspaper headline generation and student information summaries. ATS has been a research area since the 1950s, with two main categories: extractive and abstractive summarization. Extractive summarization summarizes the document by selecting important sentences or phrases, whereas abstractive summarization generates new sentences that capture the document's semantic meaning. This research aims to improve the readability and correctness of the generated summary by using a hybrid technique that combines various models, including BERT2BERT, ParsBERT, and Seq-to-Seq. This research focused on generating single document-based abstractive summarizations using a hybrid technique. The study compared the performance of the proposed deep learning model with two commonly used machine learning algorithms: logistic regression (LR) and support vector machine (SVM). The findings reveal that the proposed deep learning model outperforms the other two models in terms of overall performance. The F1-measure approach was employed to achieve these results. Future work will expand on the current approach by training larger models on additional datasets to further enhance the performance of abstractive summarization techniques

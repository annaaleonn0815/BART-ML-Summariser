A software engineering model is considered imperfect if it does not meet the expectations of its users, for example, if it does not operate as specified in its requirement documentation. This situation may arise from incorrect assumptions in the methodology, lack of certain features, outdated tools, advanced frameworks, or bugs. These deficiencies can be recognized as defects. The primary concerns are the degree of software inadequacy, as not all defects have the same priority based on their impact during the maintenance phase, and the number of defects in a software system is also crucial in determining how defective the software is. A software system with a single non-critical bug is far more desirable than one with numerous bugs. Metrics of a software system must be collected before applying any defect prediction strategy. Since software is inherently difficult to measure, gathering its metrics is not a simple task. Often, organizations do not invest time in measurement collection and analysis due to market pressures. Although analyzing software metrics saves time and provides more effective project management, it is generally very challenging to convince managers of its value. Furthermore, even if managers are convinced, metrics are often not shared because the data is considered confidential and sharing is undesirable. As a result, collecting measurement data from commercial projects is typically difficult, and even if data is collected, it is often impractical to reuse it. However, some public datasets, such as those shared by NASA and REPOSITORIES dataset models, are increasing in number. These public datasets allow for comparative analysis and the evaluation of various defect prediction techniques. This research paper utilizes these datasets for our proposed analysis methods. Defect prediction and correction are among the most costly activities in embedded software development. Given the size, complexity, time, and cost pressures, tracking and predicting quality is a significant challenge in software development projects. To meet the demands for high quality and reliability, considerable effort is dedicated to software verification and validation. Testing software is a crucial part of software engineering, where verification and validation are employed to ensure the correctness and reliability of software systems. Machine learning techniques can be used to analyze data from different perspectives and help engineers extract valuable insights. Machine learning has been successfully applied to make predictions across various datasets. Given the vast number of bug datasets available today, predicting the presence of bugs can also be accomplished using different machine learning methods. Machine learning techniques can be used to identify bugs in software datasets through classification and clustering techniques. Classification is a data mining and machine learning approach useful in software defect-prone models. It involves categorizing software modules into defective or non-defective based on a set of software complexity metrics using a classification model derived from previous development project data. The field of machine learning has been rapidly advancing, producing a variety of algorithms for different software applications. The ultimate value of these algorithms is generally determined by their success in addressing real-world problems. Therefore, algorithm enhancement and application to new tasks are crucial to the field's development. Although various machine learning researchers are currently publishing advancements in software defect prediction model development, machine learning techniques can learn from past data and be applied in various complex situations. In our research, we use machine learning techniques to provide a predictor for classifying source dataset models according to a pre-defined classification scheme. We consider two classification schemes: one for classifying dataset models as defective or non-defective and another for fault level classification, where datasets are categorized as low fault, medium fault, or high fault. We assess the effectiveness, accuracy, and efficiency of software defect-prone models based on classification techniques using LibSVM and LibLinear classifiers. A comparative analysis between LibSVM and LibLinear classifications on software defect-prone datasets reveals that LibSVM is faster for training datasets. However, while some evaluation metrics show improvement with LibLinear, others improve with SVM. Overall, LibSVM is more effective for training datasets and demonstrates better accuracy and efficiency for defect-prone models. Nevertheless, with the use of percentage splits, both LibLinear and SVM show good performance in certain evaluation measures, enhancing accuracy and efficiency.

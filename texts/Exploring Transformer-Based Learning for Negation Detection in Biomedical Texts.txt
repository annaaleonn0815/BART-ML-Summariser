Natural language processing (NLP) methods provide computers with capabilities to perform speech and language understanding tasks. The biomedical domain is one important field that has benefited from NLP in a wide range of applications, including searching biomedical articles and analyzing patient information presented in clinical texts. Clinical texts (e.g., clinical cases and medical records), as an example of biomedical data, typically contain doctors' notes about a patient's health, including previous medical history such as treatments, procedures, diseases, lab exams, and any other relevant information. Patient health data can grow immensely, as every visit to the hospital leads to the addition of more data to the patient record, making manual analysis of patients' health data by physicians and extraction of relevant information difficult and time-consuming. Therefore, applying NLP tools to these tasks is expected to be beneficial, as it can reduce their complexity and help to perform these tasks on time. NLP and text analytics tools are generally utilized by health care providers, such as physicians and biomedical researchers, to automate the analysis of biomedical textual documents for extracting and deriving new knowledge from these documents. For instance, clinicians may use these tools to detect the presence of key clinical concepts (i.e., clinical findings such as health conditions) regarding patient health in a clinical case describing the health status of a given patient, which can assist them in conducting more effective diagnostic procedures for patients and providing better health care. Likewise, biomedical researchers can utilize these tools to extract relations among biological entities (such as proteins, genes, small molecules, and diseases), which can assist in deriving new scientific facts and understanding certain biological processes (e.g., understanding the interactions among certain proteins). However, the presence of negated assertions in biomedical texts may present a challenge when analyzing such textual data. For instance, a sentence like "there was no evidence of polyps" can be found in a clinical text to indicate that a patient does not have a polyps condition, making it harder to analyze these texts properly relying only on conventional text-analysis methods. Negation generally is considered a complex linguistic phenomenon, and its presence in affirmative natural language statements modifies their meaning by reversing and turning them into negative statements (e.g., she is performing this task/she is not performing this task). Therefore, due to the importance of such a phenomenon in natural languages, negation has been well studied in previous literature from linguistic, philosophical, and psychological perspectives. More specifically, in the biomedical domain, negated contexts in biomedical documents (biomedical articles, medical records, radiology reports, etc.) can be as important as affirmed contexts to indicate the uncertainty of some idea or the nonexistence of a specific biomedical concept. Handling negation in biomedical texts is an important text-analytics task and can be beneficial for several applications. For example, in the automatic processing and analysis of medical records, it allows the detection of negative assertions that indicate the absence of certain symptoms or conditions from a patient, helping to rule them out from a diagnostic procedure later conducted by a physician. For search and retrieval of biomedical literature, detecting uncertain and negative contexts while indexing biomedical articles is also important, as it can assist in improving the quality of search by reducing the false-positive results that were matched to a query entered by a user (i.e., by filtering out articles containing statements denying the presence of terminology that is searched by the user). However, identifying negated contexts and distinguishing them from positive contexts is not a trivial NLP task. It requires performing two subtasks: first, identifying if a given sentence contains negation, and second, recognizing the scope of negation (i.e., at which word the negation starts and at which it ends). For instance, in the search and retrieval application discussed above, the first negation-detection subtask can be helpful while preprocessing an article for indexing to find all negated sentences in that article, whereas the second subtask can be applied to the identified sentences to locate the negation scopes for each, eliminating them from indexing so their texts are not searchable by users. We will refer to the two subtasks as negation sentence identification and negation scope recognition. Several approaches have been proposed in prior work to address this challenge by exploring various techniques, including rule-based systems, conventional machine learning (ML), convolutional neural networks (CNNs), and bidirectional long short-term memories (Bi-LSTMs). Recent successes in the adaptation of transformer-based learning using bidirectional encoder representation from transformers (BERT) for a wide range of language understanding tasks have motivated us to consider such learner models for our negation detection task. In this work, we propose using a pre-trained BERT model to perform two negation subtasks. For negation identification, we aim to develop a classification tool that receives a sentence and identifies whether the sentence contains a negated or affirmed context. For the second subtask, negation scope recognition, our goal is to develop a tool that can recognize the scope of negation within a negated sentence. After developing several approaches that rely on BERT to accomplish these tasks, we conducted an evaluation of these approaches using the BioScope dataset and reported the performance using several metrics, including accuracy, precision, recall, F1, and the percentage of correct scopes (PCS). Moreover, we extended our approaches to be implemented on other BERT-like models, such as ALBERT, XLNet, and ELECTRA. The results of our evaluation suggest that transformer-based learning has the potential to be adopted for negation detection subtasks, as it achieved state-of-the-art performance for both tasks, reaching an accuracy of 99% for negation identification and a PCS of 95.14% for the task of negation scope recognition. Generally, the main contributions of this work can be summarized as follows. First, we explored applying BERT for addressing the two negation subtasks, negation identification and negation scope recognition, while examining different ways of representing the negation scopes, and we showed that BERT can be effective for addressing the negation detection problem. Second, we explored extending our problem representation to other transformer models and showed that our framework is generalizable to other BERT-like models. The remainder of the paper is organized as follows. Section II contains background information on medical text analysis, and we define transformer-based learning as used in this work. Then, in Section III, we review some prior work on approaches for negation detection in biomedical texts. In Section IV, we describe our approaches for addressing the two negation detection subtasks, and later, in Section V, we conduct an empirical evaluation and analysis of these approaches. Finally, in Section VI, we conclude this work and provide summarizing remarks. Health care practitioners and biomedical researchers rely on text analysis tools to perform various tasks, such as searching biomedical articles and extracting patient health information, aiming to make better clinical decisions for patients and derive knowledge from biomedical data. As discussed in this paper, negation identification and scope recognition are two information extraction tasks that can be provided by tools to detect negated contexts in texts to be utilized by clinicians and biomedical researchers during the analysis of biomedical data. We contribute by showing a case study for the successful application of BERT and other transformer learning models to address two negation detection tasks, leading to high performance and reducing their complexity. Although transformer learners are shown to deliver state-of-the-art performance for many applications, some limitations are observed. The learners have high dimensional parameters, and fine-tuning these parameters is a time-consuming job, making the learning of a model for a target task a slow process (for instance, we spent several hours fine-tuning BERT for our tasks). In addition, they have a slow inferencing capability that makes them less practical for highly demanding inference tasks. Therefore, future work should focus on tackling these limitations by exploring the reduction of the parameters' space and producing lighter-weight transformer models while attempting to keep the level of performance effectiveness produced in this study.

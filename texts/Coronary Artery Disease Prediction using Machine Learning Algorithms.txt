Cardiovascular Disease (CVD) is considered to be one of the deadliest diseases we have come across as it leads to cardiac arrests. Due to the rare occurrence and symptoms that can often be related to other diseases, many people do not take the risk of heart failure seriously. There are different ways by which the heart might fail, such as due to damage to the heart arteries and a sudden sensation of a squeeze in the heart can be felt. Anmad and G.Wang used CVD as a collective term for any form of heart disease. Any condition that affects the heart and its vessels can be related to coronary vascular diseases. CVD is more frequently found amongst men than women. Considering the contemporary rate at which cardiovascular disease is increasing, it imposes a huge financial burden. There are many ways to identify heart diseases. The traditional way to do this is by identifying them through ECG or Electrocardiogram tests and can also be identified by using Angiogram. The deformities in the human heart can be easily identified using ECG by medical experts Acharya U et al. Artificial Intelligence plays a vital role in giving a diagnosis and smart decision making. These diagnoses can be categorized using various classifiers. The health industry generates copious amounts of medical data which contains a lot of beneficiary information. Javad Hassannataj Joloudari and Raghupathi analyzed and experimented on many classifier models and achieved predictive diagnosis avoiding unnecessary cost for the medical sector. Due to its unaffordability, it is not easily accessible for economically weaker sections of society. To overcome such a circumstance and to make expensive healthcare widely available and affordable to all people, a quintessential model has to be developed. We have used these popular classifiers and they are, Logistic Regression, K-Nearest Neighbor, Support Vector, Naive Bayes, Decision Tree, Random Forest, MLP Classifier, XG Boost, and Gradient Boost. Given below are a few technical explanations of the various classifiers: Logistic Regression: Probability is the main concept being implemented here. It provides value in binary format, i.e. 1 and 0. K Nearest Neighbor: KNN is a non-parametric algorithm that makes no assumptions about the input data. It collects the data which are available and classifies new data points based on how similar they are. Support Vector Machine: The way SVM works is that it maps data into a multidimensional feature space, allowing data points to be categorized even when the data cannot be separated linearly. Naive Bayes: It is based on the Bayes Theorem and conditional probability. This classifier is based on the assumption that it is independent between attributes of data points. Decision Tree: This classifier is used for its capability to capture descriptive decision-making knowledge from the supplied data. In layman terms, the model asks a question and based on the answer which will be either ‘Yes’ or ‘No’, it further splits the tree into subtrees. XGBoost: It uses the same machine learning techniques as Gradient Boosting; this classifier also has a parallel tree boosting that is used for enhancing accuracy. Random Forest: This classifier consists of a sizable number of individual decision trees. Each tree is being diagnosed, and the set of trees with the most votes is elected as the model’s prediction. Multi-Layer Perceptron: MLP is a feedforward AN model which maps an input data set to a corresponding output dataset. It consists of multiple layers, each fully connected to the next one. Gradient Boost: In this boosting classifier, each predictor tries to improve from its previous results by reducing the errors. Concluding the results from Table 1, we can observe that XGBoost, Decision Tree, and Random Forest give 100% accuracy without getting introduced to any Bagging and Boosting algorithms. Following that both Gradient Boosting and K-Nearest Neighbors give 97% and 92% accuracies. In Table 2, we have used the new dataset with 6 attributes that contribute mostly to the accuracy; we can observe that the results are comparatively minimal when compared with Table 2. Both Decision Tree and Random Forest give 100% accuracy, but XGBoost reduces by 1%. Even though the accuracies of some classifiers are reduced by 1% or 2%, classifiers such as Gradient Boosting and Multi-Layer Perceptron show better results with an increase in accuracy by 3% and 2% respectively. We can conclude from the above results; we obtain the best accuracy from both the Decision Tree model and Random Forest model compared to the other models. This is because it uses the most important attributes as the major factor to give accurate results. Random Forest gives the same result since it is a derivative of Decision Tree. In this proposed work, we have been able to achieve higher levels of accuracy using classifiers such as Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Naive Bayes. The fundamental basis of this research are: (i) To compare the performance of different classifiers using the heart disease dataset collected from four different databases: Cleveland, Hungary, Switzerland, and Long Beach, and (ii) To reduce the cost and make it affordable and minimize the number of tests/scans the patient has to undergo. In future implementations, heart disease can be classified and prioritized according to their risk levels and can be experimented with different parameters.

There are only two core requirements for automatic summarization: 1. The length of the summary should be short and 2. The summary should have the central content of the text given. Different from the way of extractive abstract, the abstractive method usually needs to go through (1) understanding the original text, (2) information fusion, and (3) summarizing the sequence and steps of the summary text. The switch to study abstractive summarization is due to the substantial progress that has been made since people start to use deep learning neural networks in the field of generative text summarization. Abstractive summaries are more challenging to generate than extractive summaries, but arguably much similar to human summaries because they may contain expressions that are not present in the original text. The abstractive summarization method based on deep learning has many distinctive designs mainly through the encoder-decoder framework, which can deal with the abstractive summarization of short texts, but there is still the possibility of missing some information in the case of the abstractive summarization on long texts. And through Attention, Distraction, Coverage, Pointer networks, Copy mechanism, and other methods to improve the encoder-decoder process to deal with long text has been able to make the accuracy of summarization results to be improved. The improvement for Chinese text summarization mainly focuses on the preprocessing process of the encoder. Processing the text through named entity recognition and Chinese word segmentation enables the subsequent process to achieve the summarization effect as good as the English text summarization. The attention mechanism can make the summary closer to the human-made summary with the distraction method, and the pointer network also makes the encoder-decoder framework more sensitive to the infrequent words and the words that play a key role but did not appears a lot in the paragraph so in that way the summary actually did the same as the human summary habits. In addition, in the evaluation process, although the current evaluation method we use is effective, still it is not that accurate, because, unlike extractive summarization, in the process of abstractive summarization decoding, many different words or phrases can be used to achieve the same semantic vector, so there may be deviations from the words used in manual summaries, resulting in lower scores. It is not advisable to directly compare scores for abstractive summaries to scores for extractive summaries. The current deep learning-based generative text summarization method has been improved, but the overall summary test score is still low, and there are a lot of problems that need to be solved. We still cannot pinpoint the significance of the word that is unusual to appear. There is still no better solution to various problems such as polysemy in Chinese and specific sorting after named entity recognition. More research and new encoder and decoder designs are needed to address these issues.

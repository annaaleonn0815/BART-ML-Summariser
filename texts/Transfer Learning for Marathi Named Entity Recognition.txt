Named Entity Recognition (NER) is mainly usable and suitable for data extraction. NER problem is described as an identification of Proper Noun and classifying the Proper Noun as Organization, Person, Geographical Entity, Event, and Location. NER is a crucial task in all the Natural Language processing functions such as Extraction of Information, Question Answering, Translation, Automatic Summarizing, etc. NER structures have performed well in various domains, such as recognizing named entities in prison archives, social media content, and news articles. Extracting the essential entities from a text helps to sort unstructured statistics and become aware of the necessary information. It concerns identifying the key elements in the text and organizing them into predefined categories such as Organization, Person, Geographical Entity, Event, and Location. Accurate and advanced NER systems are now available for European languages, mainly English and Asian. Indian languages are distinctly under-represented in the current NLP research. In this paper, a Marathi labeled dataset is created using Marathi news articles. The data has been scraped from different news sources on the internet. The dataset comprises news articles from Entertainment, Sports, Politics, and Environment. There are 100 news articles scraped from different sources of news articles available on the internet. There are 900 sentences and 7000 tokens in the NewsCorpus. POS tagging is an essential task in NLP. We analyzed traditional approaches for POS tagging, such as statistical approaches, rule-based methods, etc. In the statistical approach, the paper proposed a unigram tagger for Marathi NewsCorpus. There is a lack of sufficient labeled data available for different Natural Language processing tasks in the Marathi language. Thus, the paper proposed a transfer learning approach for Marathi NER. In the transfer learning-based approach, knowledge from one pretrained model is used for another model for a given task. Transfer learning improves the modelâ€™s performance and does not require much data for model building. Developed Models and Pre-trained Models are common transfer learning approaches for predictive problems. The pre-trained approach selects a pretrained source model from available models. Many research institutions and organizations release models trained on large and challenging datasets. These include multilingual-BERT, XLM-Roberta, and IndicBERT. Sequential transfer learning is a form that has led to significant improvements to date. The common practice is to pre-train representations on extensive unlabeled data using different models and then use the learning of these models in a supervised target task using labeled data. This paper compares the performance of the traditional algorithms such as support vector machine, conditional random field, and bidirectional LSTM (Long Short Term Memory) with transfer learning-based algorithms (multilingual-BERT, XLM-R, IndicBERT) trained on the same dataset. Both Traditional and Transfer learning models are trained on the same Marathi NewsCorpus data, which contains 900 sentences and 7000 words. Transfer Learning Algorithms perform well than traditional Machine Learning Algorithms. Different classification algorithms showed excellent performance for our news corpus dataset. These classifier algorithms are evaluated using accuracy, precision, recall, and f-score. Traditional algorithms such as Random Forest and Conditional Random Field performed similarly on the news corpus dataset. Bidirectional LSTM gives better performance for the Marathi NewsCorpus dataset. The transfer learning model performed well. From different transfer learning models, IndicBERT acquired the highest accuracy of 88.60%. IndicBERT performs better than other models for Marathi entity recognition on our corpus. mBERT and XLM-RoBERT give 81.8% and 80.3% accuracy on the Marathi corpus. In this research, different Marathi Named Entity Recognition techniques were applied on our NewsCorpus created on Marathi news documents. Machine Learning and transfer learning models are evaluated using accuracy, precision, recall, and F1-score evaluation metrics. Pretrained mBERT, IndicBERT, and XLM-RoBERT models were implemented for Marathi Named Entity Recognition. This research paper gives a comparative analysis of the performance of Transfer learning and Traditional algorithms on the NewsCorpus dataset. In the future, we aim to perform experiments on a sizeable Marathi dataset and more named entities. Also, we will try to use real-time news data for entity recognition.

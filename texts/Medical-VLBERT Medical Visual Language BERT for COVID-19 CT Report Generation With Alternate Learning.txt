Medical imaging from computed tomography (CT) and Chest X-Ray (CXR) is crucial for detecting pulmonary lesions, such as ground-glass opacities and infiltrations, which are key indicators of COVID-19. Despite their significance, these imaging systems often produce large volumes of scans, placing a significant burden on radiologists and impeding rapid COVID-19 diagnosis. Therefore, developing an intelligent system capable of analyzing lesions and generating medical reports automatically is highly valuable. This system’s core component is medical report generation, which involves linking lesion areas in images with corresponding pathological analysis. Previous methods, which often focus on normal image patches due to imbalanced training data, tend to produce reports with incorrect terminologies due to insufficient attention to lung injuries. Hence, integrating external medical-specific knowledge is essential for accurate anomaly detection and report generation. Traditional methods using vanilla recurrent neural networks (RNNs) and their variants fall short because they do not leverage both visual and linguistic information effectively. The introduction of BERT and its variant VL-BERT addresses this issue by enabling the integration of visual and linguistic knowledge, thus improving the quality of medical reports. However, deep neural networks like BERT and ResNet require large amounts of training data, which is a challenge given the recent emergence of COVID-19. To overcome the data shortage, recent approaches have introduced additional datasets, such as viral pneumonia and community-acquired pneumonia images. However, these additional datasets may introduce noise, which can negatively impact model performance. To address this, we propose the Medical Visual Language BERT (Medical-VLBERT) for automatic medical report generation. This model employs an alternate learning approach consisting of knowledge pretraining and knowledge transferring. The pretraining phase involves learning from medical textbooks, while the transferring phase utilizes this knowledge to generate medical reports. Specifically, the model includes a terminology encoder to process multimodal features (medical images and reports) and a shared language decoder for sentence generation based on the encoder’s information. In this study, we constructed a dataset comprising 368 medical findings in Chinese for 96 patients and 1,104 corresponding chest CT scans, guided by the Diagnosis and Treatment Protocol for Novel Coronavirus Pneumonia. We used a transfer learning strategy where the model is first trained on a large-scale CX-CHR dataset of 45,598 X-ray images and then fine-tuned on the COVID-19 dataset. This approach helps mitigate the data shortage issue while leveraging the similarities in medical report formats. Our contributions are fourfold: we introduce Medical-VLBERT, the first model designed for generating medical reports from COVID-19 CT scans; we utilize transfer learning to address the limited COVID-19 data, achieving state-of-the-art results in terminology prediction and report generation; we develop an alternate training strategy to enhance the accuracy of reports by minimizing discrepancies between scans and diagnosis texts; and we provide a COVID-19 CT dataset with 1,104 scans and 368 standardized Chinese medical reports, which is publicly available. In conclusion, Medical-VLBERT effectively aids radiologists by automatically generating medical reports based on CT scans of COVID-19 patients. By leveraging transfer learning and advanced neural network techniques, our model achieves high performance in terminology prediction and report generation, easing the diagnostic burden on radiologists and improving the efficiency of COVID-19 diagnosis. Future work will focus on incorporating diagnostic phase information and generating more comprehensive reports.

In the United States civil litigation process, a party has the right to withhold certain documents from courtâ€™s production request on the basis that they contain privileged content involving communications between attorneys and clients with the purpose of obtaining legal advice. Protection of privileged information from inadvertent disclosure is a critical component of the US legal system. To safeguard this information, parties are often engaged in expensive processes to review documents for production. Review costs continue to rise as the number of documents that need to be reviewed in cases only gets larger and larger based on what we see in projects in the industry over the years. The high stakes involved in privilege review makes it the most expensive part of document review. The legal market has resorted to leveraging supervised machine learning approaches, or called predictive coding in legal community, to automate/semi-automate the document review process. Existing research shows that predictive coding has the potential to substantially reduce review costs. The classic supervised machine learning approach requires significant amount of training examples to perform well in tasks. To address this bottleneck, transfer learning is a new learning paradigm that aims to leverage data from existing domains to train models that can generalize to the task at hand. This is especially appealing to the privilege review task in that not only would it reduce the review costs and facilitate the document review workflow, but also that the concept of privilege may be generalizable across cases in a sense, as the definition of privileged circumstances are well defined in law. In this paper, we empirically study three kinds of machine learning models in privilege prediction and investigate the transferability of models trained on different datasets using different machine learning methods. Main contributions of this paper include experimenting transferred models in predictive coding and conducting experiments with datasets with true labels instead on simulated labels. The paper is organized as follows: Section II we review existing research related to machine learning for legal document review and transfer learning. Section III presents the research questions. Section IV describes the datasets used, the machine learning methods and experimental protocol, and evaluation measures. Section V discusses the results and Second VI concludes the papers with remarks on future work. This paper empirically studies the transferability of both traditional machine learning models and deep learning models on predicting privileged documents in legal document review. We have found that that BERT-based models with fine-tuning outperform logistic regression with TFIDF document representations according to F1 score. Zero shot transferred models though underperform natively trained models, can still achieve satisfactory performance if the target dataset is from a close domain. The above conclusions are based on the utilized portion of the three datasets. In future, we plan to test with more data points and other available data sets. For the BERT-based models, we only add a single linear layer on top of BERT. It would also be worthwhile to explore with more sophisticated architecture, such as CNN or LSTM, etc. Also due to BERT restricting the input to only 512 Word Pieces, we currently have truncated long documents to only use the first 512 Word Pieces. More principled ways to deal with long documents in Transformers models will be explored in the next steps.

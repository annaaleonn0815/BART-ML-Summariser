Lately, text summarization has gained growing significance, driven by the sheer volume of data available on the internet. The resulting information overload has amplified the need for more robust and adaptable text summarization techniques. This importance stems from its wide-ranging applications, including the summarization of newspaper articles, books, magazines, narratives on similar subjects, events, scholarly publications, and other lengthy texts that require concise summaries. With the ever-expanding volume of information and data available on the World Wide Web, the challenge of swiftly accessing and assimilating the necessary information has emerged as a continual focal point of research. Collecting all the pertinent information and then presenting it in a concise form is a laborious task. The internet serves as a platform for sourcing information from databases, yet the sheer magnitude of this data remains unwieldy. This is why text summarization has gained significance; it condenses lengthy documents while preserving their meaning and content. Summaries prove to be invaluable, saving both time and effort when dealing with vast amounts of document data. In the past, this process relied on manual labor, but nowadays, automation has introduced numerous advantages. Supporting this condition, there are several implementations of machine learning and deep learning research in text summarization. Lin et al. use a transformer model for their research on text summarization in Bangla documents, while Islam et al. leverage tokenization techniques. Text summarization techniques can generally be categorized into two primary groups: extractive and abstractive summarization. Extractive summarization involves the extraction of crucial sentences or phrases from the source document, grouping them together to create a summary without any alterations to the original text. These sentences are typically arranged in the same order as they appear in the original document. In contrast, abstractive summarization aims to condense the content while understanding the original text through linguistic methods for text comprehension and analysis. The goal of abstractive summarization is to generate a concise and generalized summary that effectively conveys information, often necessitating advanced language generation and compression techniques. Abstractive summarization represents an efficient method of summarization when compared to the extractive approach, which entails retrieving information from multiple documents to generate a cohesive and accurate summary. The popularity of abstractive summarization has grown due to its capacity to craft new sentences, narrate stories, and distill crucial information from textual documents. According to recent research, there is growing interest in leveraging machine learning techniques for text summarization in languages such as Indonesian. However, implementing abstractive summarization with a state-of-the-art model like BERT demands substantial computational resources. BERT is a heavyweight model, known for its impressive natural language understanding capabilities, but it can be resource-intensive. For researchers, developers, and organizations seeking an efficient solution for abstractive text summarization without the heavy computational demands of BERT, ALBERT offers a compelling alternative. ALBERT, short for "A Lite BERT," provides a more resource-friendly implementation of BERT in text summarization. This model maintains the strengths of BERT while significantly reducing the computational resources required for training and deployment. In the following sections, we will delve into the capabilities of abstractive text summarization in Indonesian using BERT and explain how ALBERT provides an excellent, resource-friendly alternative. We will explore the benefits of ALBERT in the context of text summarization in Indonesian, making it an appealing choice for a wide range of applications. Our fine-tuned ALBERT model, equipped with optimized hyperparameters, achieves an impressive ROUGE-1 score of 45.28, a ROUGE-2 score of 40.77, and a ROUGE-L score of 44.39. Despite the brief fine-tuning duration, our model produces comparable ROUGE scores to BERT, demonstrating ALBERT's resource efficiency and its ability to deliver results on par with BERT. We carried out a qualitative analysis by contrasting the summaries created by ALBERT FT with reference summaries and the original paragraph to further evaluate the quality of the generated summaries. According to this investigation, ALBERT FT regularly generated summaries that were more accurate, informative, and fluid, successfully summarizing the main ideas of the original text. Although it seems a little shorter than the referenced summary and could miss other main ideas, one significant benefit of our suggested approach is its computational effectiveness. The ALBERT FT model can be fine-tuned in about 50 minutes, which is far faster than other methods like BERT or BART, which usually take around 4 hours to train but yield marginally better results. Because of its efficiency, ALBERT FT is a more sensible and economical choice for real-world applications. Despite the brief fine-tuning duration, our model produces comparable ROUGE scores to BERT, demonstrating ALBERT's resource efficiency and its ability to deliver results on par with BERT.

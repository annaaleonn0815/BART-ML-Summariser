[
    {
        "input_text": "Rate of Return shows a profitable investment over a certain time, and Net Present Value is an essential factor. In equation 1, cash flow is calculated for a house in each period K, and re is a satisfactory rate of return. One of the essential factors in measuring the Net Present Value of investment is cash flow. Rent, house mortgage, annual house tax, annual house expenses can affect Cash flow. House rent positively affects cash flow, but the higher vacancy rate can create a negative cash flow. Several machine learning techniques are used for rent prediction. Greenland investigates eager methods like bagging REP trees to predict rent price for residents near the university campus, and bagging REP trees provides the best prediction accuracy. However, the coverage area of the training set is limited zip codes near the university campus. So, their solution can generate a biased model since all data has the same Geo location characteristics. The previous predictive models for rent price did not consider the house type and zip code at the same time. Rent variances can be very high for properties, even in the same city. For instance, the zip codes 20143 and 20105 are in the same neighborhood, but the average rent prices are not similar. The rent price is also based on area space, bedrooms, bathrooms, community factors, etc. The average rent price for a zipcode depends on internal and external factors such as walking score, transit score, and crime rate of a zip code. The transit score indicates the connectivity or access to essential services. Crime score shows different violent incidents based on specific zip code. Lazy learning models find the local optimal solutions for each test instance by storing the training instances and postpone the generalization until a new instance arrives. This research uses multiple online resources for real estate knowledge extraction by using different natural language processing approaches. Deep learning architectures can be powerful tools in text processing of online reviews. People share their ideas about different business topics in social media platforms. Data from public and personal blogs can be valuable for improving online data collection phase, and this data can be categorized based on different topics in real estate. Homeowners and homebuyers express their ideas in both positive and negative ways on online platforms. So for each house, the sentiment score can be calculated by the sentiment classification of home owners. Also, if the online review has a limited number of words, by using advanced sentiment classification techniques, we can assign a sentiment score to each online review. The Polarization of ideas in the real estate market to promote buyers to buy real estate properties in specific cities can be recognized by polarization detection methods in social media platforms. It is also essential to detect Discrimination or social relations in online comments text in rent price in different areas in the cities. Real estate companies can use homebuyers emotions to make specific real estate deals, so it is vital to do emotion classification in the home owners comments to feed the classifiers with more reliable data. Real estate websites show the houses in each zip code based on the price map; pictures with similar price tags can be recognized by image caption generation techniques. Also, the comparison between the house images by expert image processing techniques can detect pictures of the homes which have more number of views by home buyers. These images can show the customer interest in the real estate market about specific house types. Chatbots can be used to analyze more verbal and textual data shared in social media platforms about the real estate market. In this work, to compensate for the computational time of NLP algorithms, we optimize hardware and power usage by using memory-efficient methods. We optimized memory usage by using for intensive data workload. The advanced design of hierarchical convolutional neural networks is used in this research to accelerate processing time. Convolutional neural network models are examined in different scientific domains such as Cyber security, health, business applications. The business deal recommender system in this research is based on convolutional neural network models. In this research, the classification of real estate properties into profitable rental properties and not profitable provides investors with a model to make a safe business investment. This work contribution can be summarized in 3 main points: first, Using the BERT model as a bidirectional transformer for sentiment classification of online reviews based on different online resources to detect profitable rental properties. Second, using semantic CNN model to extract the deep context of online housing reviews. The new CNN model improves prediction accuracy very significantly. Third, a new public data set of more than five million houses with its sentiment score and semantic information. The new data set will be available for machine learning research, especially in the real estate market research",
        "target_summary": "This study explores a novel approach to enhancing the security of real estate investments by utilizing natural language processing techniques. Focusing on rental properties, the study aims to help low-income individuals make informed decisions in the real estate market, especially during economic downturns when many homeowners face the risk of foreclosure. By analyzing online textual data from platforms like Airbnb and Zillow, the research employs a transfer learning approach with Bidirectional Encoder Representations from Transformers (BERT) to extract key features. These features are then integrated into a semantic convolutional neural network (CNN) model to predict the rental potential of properties. The study also introduces a new public dataset with over five million U.S. properties, providing a rich resource for further research. The paper evaluates the performance of this advanced model against traditional machine learning methods, demonstrating significant improvements in prediction accuracy and offering a robust tool for safer real estate investment decisions."
    },
    {
        "input_text": "Various COVID-19 vaccines have been developed and tested worldwide since March 2020, when the pandemic was declared. It has been shown that public health measures can somewhat limit COVID-19 spread, but establishing and maintaining protective immunity through vaccinations is essential for ending the pandemic. To achieve some level of herd immunity, at least 70% of the population would need to be vaccinated. The public must support vaccination to achieve this. It is therefore vital to understand public attitudes towards vaccination and what the public is willing to accept. Public health perspectives have been investigated through traditional surveys; however, social media is progressively being used to discuss and share experiences on outbreaks of infectious diseases and health topics. Several pandemic-related topics, including vaccines, have been discussed using social media as a forum during the current COVID-19 pandemic. It has been observed that social media users may express negative sentiments, leading to vaccine uncertainty or rejection. The WHO has stated that globally, vaccine hesitancy is a major health threat. Simple evocative analyses of vaccine-related tweets have been employed in prior studies to evaluate public sentiment on immunization. Sentiment analysis using a lexicon involves calculating document orientation based on the semantic relations or polarity of phrases and words in the document. This lexicon-based strategy does not require storing enormous data sets as machine learning does. Instead, it uses lexicons or vocabularies to determine document orientation. Semantic Orientation (SO) measures the polarity and intensity of phrases and words in a text, which helps establish the documents overall attitude. The dictionary strategy begins with a limited collection of known words and then explores well-known corpora. New words found are added to the existing list until none are found. This technique lacks topic and context-dependent opinion words, aiming to find related words with respect to the context. To trace other related terms in a large corpus, it uses syntactic patterns and a sentiment words seed list. This strategy has been employed using a limited seed list of opinion adjectives to identify more adjective opinion terms and their directions. However, it is challenging to construct a large corpus of all English terms, making the corpus-based technique less successful. For supervised learning approaches, a well-labelled corpus is required, with several algorithms available. The primary issue with supervised learning is the need for distinct labelled data for training and testing. Unsupervised methods, which may involve machine learning or vocabulary-based approaches, do not always require a tagged corpus, and the model does not need to be labelled while using this method. Semi-supervised learning is a hybrid of supervised and unsupervised learning, aiming to categorize some data samples using labelled data. The Hybrid technique combines lexicon-based and machine learning rules, with classifiers in this approach working in a cascade, so when one fails, the next one succeeds. The training dataset in a deep learning model consists of samples used to train the network. Since the answers to the input data are already recognized, the network can learn from these instances to produce the expected results. To extract sentiments from clean tweets, the TextBlob library is used. Analysis shows that there are 78,771 positive tweets, 23,993 negative tweets, and almost 125,443 neutral tweets. The most common positive, negative, and neutral words in tweets are identified. The precision, recall, F1 score, and accuracy of BMB, SVM, and LR models are evaluated, with the SVM model achieving the highest accuracy of 97%, while the accuracy of the BMB model and LR model are 81% and 95% respectively",
        "target_summary": "This study delves into the application of sentiment analysis to Twitter data related to COVID-19 vaccines, utilizing a combination of natural language processing (NLP) and machine learning classifiers. By analyzing public sentiment on social media platforms, the study seeks to understand public attitudes toward vaccination, which is critical for achieving widespread immunity and ultimately ending the pandemic. The research employs lexicon-based methods alongside machine learning models, such as Support Vector Classification (SVC) and logistic regression, to classify sentiments expressed in tweets. Through data preprocessing, vectorization, and classification, the study evaluates the effectiveness of various models, with findings indicating that SVC outperforms other methods in accurately predicting sentiment. This work underscores the importance of sentiment analysis in public health and offers insights into how social media data can be leveraged to gauge public opinion on critical issues like vaccination."
    },
    {
        "input_text": "Artificial Intelligence is a contiguous learning method by the computer approaching the real-life scenario as an input and give relevant human-like response. Machine Learning is a sub-domain of AI, that is more concerned to understand the underlying structure of the data and fit that dataset into particular learning model and gradually the model is being trained and give the best response by itself. There may be certain example by which we can easily understand this domain easily. Like “Tesla” is a company which invented the autonomous car, that can drive through the road driverless. On the other hand, the Google is using the voice assistant or in our mobile phone there is fingerprint verification which is based upon AIML technique only. In our project we are also taking some dataset of Amazon customer review. And what we want to do is that we train our model in such a way that that further if any comment is coming in real time then our model can easily predict this as a positive comment or as a negative comment. So, we have done a project on the sentiment analysis of customer review and the specialty that we did was that we additionally add Transfer learning model with the conventional ML and DL model. So not only the accuracy but the precision, recall F1 score are also the judging parameter to decide which algorithm is the best suit. But we have tried different algorithms to enhance the supportive comparative study. Transfer learning is basically transfer of knowledge from one machine to another machine but for that we need encoder-decoder model which is the most recent powerful tool for any ML problems. As usual transfer learning model beat any other model in this particular problem statement. So we can say that this model works more or less perfectly. And this model can be more developed by increasing the layers or using more deep learning knowledge",
        "target_summary": "This study processes the dataset to extract and clean features using an NLP pipeline, applying various models including Logistic Regression, Random Forest, and deep learning architectures like RNN and LSTM. Notably, the research integrates a transfer learning model, BERT, built on top of a deep learning framework to enhance predictive accuracy. The results underscore BERTs superior performance, achieving the highest accuracy and demonstrating the effectiveness of transfer learning in sentiment analysis. This approach offers valuable insights for businesses looking to analyze customer feedback efficiently, ultimately aiding in better decision-making processes."
    },
    {
        "input_text": "Making judgments based on customer satisfaction towards a service or product is crucial for businesses to improve the quality of their products, adjust business strategies and marketing, enhance customer services, manage crises, and monitor performance. Sentiment analysis, also known as opinion mining, is implemented using Natural Language Processing (NLP) techniques to extract the emotions hidden behind text reviews or comments. These reviews or comments serve as metrics in business, providing insights into customer satisfaction. Organizations use this approach to categorize opinions about products, services, or ideas, leveraging NLP in combination with Machine Learning (ML), Artificial Intelligence (AI), and Data Analysis. These technologies help organizations analyze various forms of text, whether organized or unstructured, from different sources, to gain valuable insights. The manual processing of data has been replaced by algorithms that implement various new or hybrid methods. Hybrid sentiment analysis combines two methodologies, recognizing that emotions are significant not only in personal life but also in business. Understanding how an audience feels about products or a brand provides crucial context for evaluating marketing and communication strategies. Sentiment analysis helps businesses understand the insights customers share online, revealing what features they like or dislike and what they need from a product. Analyzing these comments allows businesses to draw conclusions, adjust marketing strategies, and ultimately improve their products. The final model in this context is a fully tuned model capable of analyzing the sentiment of movie reviews, whether they are part of the dataset or not. By exposing only a portion of the dataset to the model during the data preprocessing phase, the goal is to achieve better results under general conditions and obtain an unbiased, optimized result for both included and new reviews. In the project, the Support Vector Machine (SVM) algorithm outperformed the others, with an accuracy score of 0.84, which is significantly better than the Decision Tree algorithm, which scored only 0.64. The Logistic Regression algorithm achieved an accuracy of 0.83, which, while better than the Decision Tree, still underperformed compared to SVM. SVM demonstrated the best accuracy among all the algorithms implemented in the project. Even after achieving these results, the model was further tuned using GridSearchCV to enhance its performance. The final accuracy scores were as follows: Support Vector Machine: 0.84, Logistic Regression: 0.83, and Decision Tree: 0.64",
        "target_summary": "This study explores the application of supervised machine learning techniques for sentiment analysis using the IMDB dataset, which contains 50,000 movie reviews. The studys objective is to develop a model that can accurately classify these reviews as either positive or negative. The research implements text representation using TF-IDF, followed by the application of classification algorithms like Support Vector Machines (SVM) and Decision Trees. After thorough model evaluation, SVM emerged as the most accurate classifier, outperforming other models in terms of predictive accuracy. The findings highlight the effectiveness of SVM in sentiment analysis tasks, particularly when applied to text data from movie reviews."
    },
    {
        "input_text": "Since agriculture provides a livelihood for a sizable portion of Indias population, it is essential to the nations economic development. Nonetheless, the agricultural industry confronts complex difficulties, as farmers struggle to choose which crops are best to grow in a particular situation. Complications stem from a variety of soil properties, fluctuating weather patterns, common plant illnesses, and the continuous requirement for crop observation. This research suggests a thorough and cutting-edge strategy to assist agricultural decision-making in order to address these issues. Our suggested method makes use of a wide variety of sophisticated ML models, such as the XGBoost, Decision Tree approaches, Random Forest model, and Support Vector Classification (SVC) based on accuracy values. Our goal is to find the best model for accurate crop detection through thorough analysis and comparison. The main objective is to increase the precision and effectiveness of selecting the best crops to grow, providing farmers with tools for data-driven decision-making. We leverage the strength of the deep learning model ResNet, a deep learning architecture renowned for its speed and accuracy, in the field of plant disease detection. Our goal in putting this model into practice is to quickly and accurately diagnose plant diseases. By enabling prompt disease detection, this technology gives farmers the ability to prevent and manage disease outbreaks, ultimately protecting their crops. In the future, we see Internet of Things (IoT) technology being integrated, using Bolt IoT in conjunction with an LDR sensor to determine the plants light intensity. This real-time light condition monitoring helps to improve the accuracy of crop recommendations. In addition, we use the ESP32 platform in conjunction with an NPK sensor to ascertain the soils values for nitrogen (N), phosphorus (P), and potassium (K). This novel method enables accurate evaluation of the soils health and customized fertilizer recommendations based on the soils unique nutrient needs. In conclusion, this all-encompassing strategy leverages deep learning capabilities, integrates IoT technology, and combines advanced machine learning techniques to transform agricultural practices. Our mission is to advance Indias agricultural sector, give farmers priceless insights, and promote efficient and sustainable farming practices through creative and technologically advanced solutions. IoT and machine learning are used in intelligent crop selection and decision support systems to provide farmers accurate information they need to make wise decisions. These algorithms accurately detect plant illnesses and forecast the best crop choices by analyzing data from several sources, such as satellite photography and Internet of Things sensors. Their performance is improved by user input and real-world testing, which guarantees broad acceptance. In conclusion, crop disease detection and recommendation systems play critical roles in providing farmers with accurate decision-making tools. The addition of a fertilizer suggestion system improves this capability by providing information on soil nutrient levels, which aids in crop selection. Furthermore, the integration of IoT sensors, including NPK sensors, provides continuous real-time data monitoring, hence decreasing the need for costly laboratory assessments. Furthermore, the integration of an LDR sensor enables specialized crop care solutions, allowing farmers to effectively meet individual growth requirements. Together, these components comprise a comprehensive and efficient agriculture management system, allowing farmers to make intelligent choices and maximize crop production in a sustainable manner",
        "target_summary": "This study presents an innovative approach to enhancing agricultural practices through the integration of machine learning and Internet of Things (IoT) technologies. The study focuses on developing a comprehensive system that utilizes advanced machine learning models like XGBoost, Random Forest, and Support Vector Classification to monitor crop health, predict suitable crops, and recommend fertilizers. By incorporating IoT sensors to gather real-time data on soil nutrients and environmental factors, the system provides a data-driven decision support system (DSS) tailored for farmers. This smart agriculture solution aims to optimize crop yields and improve productivity, offering a user-friendly interface for farmers regardless of their technological proficiency. The studys findings underscore the potential of combining AI and IoT to revolutionize farming practices, making agriculture more sustainable and efficient."
    },
    {
        "input_text":  "A software engineering model is appreciated imperfect in the event that it doesnt meet the desire for its clients for example it doesnt carry on as it is determined in its prerequisite inquiry record. This situation might be because of an erroneous conclusion in a method, absence of certain feature highlights, developed tools, advanced structure, or a fault or error. These lacks can be acknowledged as deformities. The most two significant focuses are with respect to the degree of software inadequacy that all deformities have not a similar need considering their impact in the upkeep stage and other point the quantity of imperfections in a software framework is likewise essential to decide how a lot of inadequate a software is. A software framework with one nonbasic bug is significantly more desirable over a framework with an excessive number of bugs. The measurements of a software framework must be prepared before applying any imperfection forecast strategy. Since the software is extremely difficult to gauge, gathering its measurements isnt a simple assignment. More often than not, organizations dont invest energy for measurement assortment and investigation, because of the well-known truth for example scramble for advertise. Despite the fact that investigating software measurements spares time and gives progressively productive venture the executives, it is generally exceptionally hard to cause supervisors to have faith in this. Moreover, regardless of whether the chiefs are persuaded, more often than not measurements are not shared in light of the fact that measurements information is respected private and sharing isnt desired. As a result, gathering measurements information from business ventures is extremely difficult as a rule, and regardless of whether the information is gathered it is preposterous to expect to reuse them. Then again, there are some open informational indexes shared by NASA and these are REPOSITORIES dataset models. The quantity of informational collections in such repositories is expanding step by step. It is conceivable to make near investigations utilizing these open informational indexes. A few deformity expectation techniques could be thought about utilizing these open informational collections and we are additionally utilizing these datasets in this examination paper for our proposed investigation strategies. Verdict and adhesive fault are by and large the costliest action in embedded software improvement. Given the size, multifaceted nature, time, and cost pressures - following and foreseeing quality is a significant test in software improvement ventures. To fulfill the needs of high caliber and unwavering quality - critical exertion is devoted to software Verification and Validation. Testing the software is a significant piece of software where confirmation and approval are utilized for guaranteeing the right usefulness and dependability of software frameworks. Machine learning systems can be utilized to dissect data from alternate points of view and empower engineers to recover valuable data. Machine learning has been effectively applied to make forecasts in different datasets. Given the tremendous number of bug datasets accessible today, foreseeing the nearness of bugs also should be possible utilizing different machine learning procedures. The machine learning systems that can be utilized to identify bugs in software datasets by use of classification and clustering techniques. Classification is a data mining and machine learning approach, helpful in software Defect-Prone model. It includes the order of software modules into flawed or non-damaged that is signified by a lot of software intricacy measurements by using a classification model that is gotten from before improvement ventures data. The field of machine learning has been developing quickly, delivering an assortment of learning algorithms for various software applications. A definitive estimation of those algorithms is, all things considered, made a decision by their accomplishment in taking care of genuine issues. In this way, calculation multiplication and application to new assignments are urgent to the advancement of the field. Although, different machine learning analysts presently distribute for software issue expectation model advancement. Machine learning procedures can gain from past information and these methods can be utilized in an assortment of multifaceted circumstances. In our examination, we utilize machine learning procedures so as to give a forecaster to order source datasets model appropriately to a pre-characterized classification design. In our research, we think about two classification plans. The primary diagram permits to characterize datasets model as being faulty or not. The subsequent pattern is called fault level classification where datasets model is named low fault, medium fault, or exceptionally fault individually. Presently, we classify effectiveness accuracy and efficiency software defect-prone model dependent on classification technique where we have utilized LibSVM and LibLinear classification. A Comparative analysis has performed between LibSVM & LibLinear classification in software defect-prone datasets models. To know the highest accuracy and efficiency for software-defect prone datasets models. In the results, we observed that the comparative analysis between all these classification techniques we can say that LibSVM is very faster in the use of training datasets. Other comparative is that few evaluation measures are increased with LibLinear and few increased with SVM. The overall % rate is that LibSVM is very useful to use training datasets and also have increased very good positive accuracy and efficiency of defect-prone datasets models. But with the use of % split, LibLinear and SVM are also good in few evaluation measures for their enhancement accuracy and efficiency",
        "target_summary": "This study emphasizes the importance of accurate defect prediction in software engineering, as software imperfections can lead to significant operational issues and customer dissatisfaction. By leveraging datasets from NASA repositories, the research focuses on using LibSVM and LibLinear to enhance the classification accuracy of software modules that are prone to defects. The methodology involves using attribute selection techniques and Grid Search for optimizing the classification models. The study evaluates the performance of these models using various metrics such as True Positive Rate (TP Rate), F Measure, Precision, Recall, and Area Under the Curve (AUC). The results demonstrate that LibSVM significantly outperforms LibLinear in terms of accuracy and efficiency, making it a more effective tool for defect-prone classification. This research provides valuable insights into the use of machine learning for improving software quality assurance processes, highlighting the potential of advanced classification algorithms in identifying and mitigating software defects. The findings suggest that integrating such models into software development workflows can lead to more reliable and robust software systems."
    },
    {
        "input_text": "As we all know, Machine learning algorithms use a range of developmental, mathematical, and probable development techniques to learn from data generated from past events, and use them in higher cognitive processes. The enhancement of AI has enabled computer systems to acknowledge, think, and use intelligence as human beings. ML algorithms are considered to be utilized in a wide range of fields including network login recognition, customer acquisition behavior detection, production process improvement, credit card fraud detection, and disease predictions. Many of these applications are built using the ML supervised method. In this way, records with labels have been delivered to hypothesis models that predict non-labeled models. This implies the idea that medical doctors can use supervised learning as a robust tool for diagnosing diseases effectively. According to our study, general mathematical techniques, work experience, and the understanding of medical professionals led to unpopular selections and errors when detecting disease-related risks. With the rapid proliferation of electronic health data, medical doctors are facing the challenge of diagnosing diseases accurately in advance. For this reason, advanced calculation methods such as supervised ML algorithms are introduced to detect logical patterns and hidden information from data, which can be used to make critical decisions. As a result, the burden on medical staff will be decreased, and the death rate of patients will be reduced, and the survival rate will be enhanced. The classical process could be a procedure within which the patient should visit a doctor, receive various treatments, and at the end gets the results. This procedure takes a lot of time. To save time and money needed for the diagnostic process, this program suggests an automated diagnostic system based on user input. The program takes symptoms as input from the user and provides predicted diseases as output within the text box. The system will predict the disease as an infection or anything adverse happening to you which makes you uncomfortable. The disease will be predicted using the Naive Bayesian algorithm. According to the literature study, this algorithm results in enormous datasets. The GUI contains symptoms of all possible diseases as labels; symptoms are chosen accordingly, and then prediction will be done. 70% of the data is used for training and 30% will be used for testing, and the testing will be done on the GUI and the results obtained are available. Diseases are predicted using the Naïve Bayesian algorithm, which works mainly with Multinomial NB as many symptoms will be there. According to the literature, this algorithm leads to the complete accuracy of an outsized dataset. The GUI contains diseases like [list of diseases] for each disease. The project is built so far that the system takes symptoms from the user as input and generates output, i.e., disease prediction. The user can select at least one to five symptoms. Less accuracy will be obtained if only one input is selected. The greater the number of inputs, the greater the accuracy. In this paper, we have proposed a learning model for a compact novel machine algorithm of Naïve Bayes. We also tried to reduce the number of features from the dataset. In this process, we were able to obtain sufficient accuracy for all data sets using our machine learning model. We found the best accuracy of the most disease approximately (78.6%). In upcoming work, the development of complex ML algorithms is extremely necessary for the enhancement of disease prediction. Additionally, datasets should be expanded to different demographics to avoid overcrowding and to extend the accuracy of the models used. Finally, well try to put all the medical reports into this system, especially the last 10 years, so that everything should be smooth and our system will lighten the burden on medical staff in many aspects",
        "target_summary": "This study discusses the development of a Special Disease Prediction System using supervised machine learning techniques, with a specific focus on the Naive Bayes Classifier. The primary objective is to create an automated system that can predict potential diseases based on symptoms provided by users, offering a preliminary diagnostic tool that can assist medical professionals and patients alike. The system leverages the vast amounts of electronic health data available, addressing the challenges faced by healthcare providers in diagnosing diseases early and accurately. The proposed system uses the Naive Bayes algorithm, which has been tested and validated for its accuracy in classifying diseases based on symptoms. Implemented in Python, the system includes a user-friendly graphical interface developed with Tkinter, allowing users to easily input symptoms and receive predictions. The results from this system show promising accuracy, especially when dealing with larger datasets, making it a valuable tool in the healthcare industry. Overall, the study demonstrates the potential of machine learning in enhancing disease prediction and diagnosis, suggesting that future developments could include more complex algorithms and larger, more diverse datasets to improve the systems accuracy and applicability."
    },
    {
        "input_text": "Improving the level of refined wind speed forecasting is a demand for refined weather forecasting services in many industries. Machine learning has many applications in the field of meteorology, including the observation and evaluation of live conditions and improving performance in weather data pre-processing, but its main application is the continuous tuning and optimization of numerical forecasts using machine learning-related algorithms. In recent years, the development of machine learning has introduced new methods for error correction of meteorological data. Sun Q D et al. pointed out that LASSO regression, random forest, and deep learning methods had better correction results than traditional MOS methods. Fu X D et al. used the random forest algorithm to construct a 1-6h wind field forecast model, and the accuracy of the model forecasts was generally high. A multi-model ensemble forecast analysis of four numerical models for day-by-day surface 2m temperature forecasts using machine learning was performed by Men X L et al., showing that the machine learning-based revised model could better capture non-linear changes in meteorological elements and revise weather forecast data more effectively than traditional numerical models. There are many machine learning algorithms, generally classified into supervised learning, unsupervised learning, and reinforcement learning algorithms based on the learning method. Real-life problems often lack sufficient a priori knowledge, making it difficult or costly to manually label categories. Solving problems in pattern recognition based on training samples with unknown categories is known as unsupervised learning. Unsupervised learning algorithms, such as k-Means and autoencoder (AE), are often applied to learning association rules and clustering and are now used in various fields, including medicine. In this paper, unsupervised machine learning models were used to model the European Centre for Medium-Range Weather Forecasts (ECMWF), the China Meteorological Administration Global Forecast System (CMA-GFS), and observation data from meteorological stations. Machine learning corrections were made to the numerical forecasts and weather conditions, and the K-fold cross-validation method was used to compare the effects of different built-in models to select the optimal model. The selected model was then used to obtain objective forecasts of average and extreme winds at 9 weather stations in the Zhangjiakou Zone of the Beijing 2022 Winter Olympic Games. We used 5 forecasts from 2 numerical forecasting models, ECMWF and CMA-GFS, and weather station data, processed through bilinear and linear interpolation. A post-processing model for wind corrections was developed using an unsupervised learning approach and applied to forecasts for average and extreme winds at 9 stations in the Zhangjiakou zone during the Beijing 2022 Winter Olympic Games. A comparative evaluation of the forecast effectiveness of the single-model forecast and the machine learning method was conducted during the Beijing Winter Olympic Games Test Games from February 15 to February 26, 2021",
        "target_summary": "This research focuses on improving wind speed predictions for the Beijing Winter Olympic Games using unsupervised machine learning. The study highlights the complexity of forecasting wind in the Dama Mountains, where traditional numerical models often underperform. By utilizing historical data and applying unsupervised learning techniques, the authors developed models that significantly enhance the accuracy of wind forecasts, particularly for extreme wind conditions. The findings demonstrate a marked improvement in prediction accuracy, reducing errors such as mean absolute error and mean squared error when compared to existing numerical methods."
    },
    {
        "input_text": "Users still retrieve documents that contain content related to their query through traditional search engines. This could be time-consuming, especially as the amount of data posted on the web is huge and increasing rapidly. In 2021, there were over 1 billion sites on the world wide web. Taking advantage of this enormous quantity of available data is still a challenge. A traditional search engine based on information retrieval (IR) does not retrieve short answers to a query. The need for a question answering system that retrieves short answers has thus increased. In artificial intelligence and natural language processing (NLP), question answering (QA) remains a significant problem and one of the most researched fields. Manning and Schutze defined QA systems as systems which try to answer a user query that is formulated in the form of a question by returning an appropriate noun phrase such as a location a person or a date. A QA system automatically answers human natural language questions. Community question answering (cQA) refers to questions asked and answered by users about various topics in an online forum. The digital footprint of human dialogues in those forums provides a great source for teaching question-answer models. Particularly, cQA forums such as Stackoverflow and Quora have an abundance of question-and-answer pairs. The rapid increase of question-answer pairs in such platforms results in the need for a way to automatically find relevant historic questions to newly asked questions, through question similarity, to answer new questions. cQA handles QA via different types of tasks, such as semantic question similarity matching, answer selection, and ranking question-answer pairs. Semantic textual similarity (STS) is an important component of many NLP tasks, including QA, document summarization, and IR. Moreover, the semantic question similarity task is an application of the STS task. STS is concerned with measuring the semantic equivalence of two text segments. The question similarity task is also concerned with detecting the semantic similarity between two questions. Two questions are defined as semantically similar if they can be correctly answered by the same answer. This task is a key step to automate the answering of new questions by reusing answers to semantically equivalent questions. The question similarity task is also known as question relevance, duplicate question detection, and recognizing question entailment. Some of the main challenges related to cQA NLP tasks are that the cQA forums consist of an open domain and non-factoid question-answer pairs, leading to extreme variance in the quality of question-answer pairs. Furthermore, cQA tasks require the neural network to comprehend the semantic component of texts because it must predict the semantic relation between two texts. Moreover, the questions in cQA forums contain long sentences ranging from a dozen words to hundreds of words. This paper contributed to the field of Arabic question similarity in two ways. First, it proposed the TAQS system with four question similarity models. They are four deep learning models for a real-world question similarity task, including two novel models. All the proposed models achieved significant performance gains. The models we presented to solve the task of semantic question similarity are BERT-BiLSTM, HT-BERT-BiLSTM, fine-tuned AraBERTv2, and fine-tuned AraBERTv0.2. The difference between the BERT-BiLSTM and HT-BERT-BiLSTM models is the feature extraction process: the former extracts the features from the fine-tuned AraBERT, and the latter extracts the features from the pre-trained AraBERT. We provided a thorough comparison of three approaches using the pre-trained language representation AraBERT. These were the fine-tuning approach through AraBERTv2 and AraBERTv0.2, the feature-based approach through BERT-BiLSTM, and a combined fine-tuning and feature-based approach through HT-BERT-BiLSTM. The proposed models were tested against BiLSTM and surpassed the performance of BiLSTM with SkipGram by a gain of 43.19%. In particular, HT-BERT-BiLSTM with the features of Layer 12 reached an accuracy of 94.45%, whereas AraBERTv2 and AraBERTv0.2 achieved 93.10% and 93.90% accuracy, respectively. This contributed to the proposed hybrid transfer learning, which positively affected the learning process. This finding demonstrates that combining fine-tuning and feature-based approaches through HT-BERT-BiLSTM enhances the performance of the semantic question similarity task. Second, we proposed, curated, and annotated an Arabic QA dataset, Tawasul. The Tawasul dataset contains 44,404 pairs of data, which are split into 36,016 training pairs and 8,388 testing pairs. Among the pre-processes that were applied to Tawasul for curation was splitting multiple similar questions in a single cell into separate cells. This increased the dataset by almost 1,000 entries and increased the number of candidate similar question examples from a maximum of 10 to a maximum of 14. Moreover, we applied the proposed rule-based approach to automatically annotate the Tawasul dataset by searching for suitable irrelevant examples and appending them. This method increased the dataset by 21,000, or 50% of the entries. The Tawasul dataset has features that have not yet been used, such as keywords and categories. As a future research direction, these features could be used to train machine learning models on different tasks, such as question generation or question classification. Furthermore, it would be interesting to evaluate and compare the effects of using other language models, such as generative pre-trained transformer (GPT), to extract the contextual features",
        "target_summary": "This study introduces TAQS, an advanced system designed to determine the similarity between Arabic questions, which is crucial for various natural language processing (NLP) applications, such as question-answering systems, chatbots, and information retrieval systems. The system leverages the power of transfer learning by employing the Bidirectional Encoder Representations from Transformers (BERT) model in combination with a Bidirectional Long Short-Term Memory (BiLSTM) network. This approach aims to achieve high accuracy in detecting similar questions within the Arabic language, a task that is particularly challenging due to the languages rich morphology and syntax. BERT is used to generate dense vector representations (embeddings) for Arabic questions, which are then processed by the BiLSTM model. The BiLSTM network captures the sequential dependencies of words in a sentence by processing the data in both forward and backward directions. This combination allows the system to better understand the context and relationships within the text, enabling more accurate similarity detection. The study also highlights the use of transfer learning by fine-tuning the pre-trained BERT model specifically on Arabic data, thus adapting the model to the specific nuances of the language while leveraging its general language understanding capabilities. The systems performance was evaluated using a dataset of Arabic questions, and the results demonstrated that the combination of BERT and BiLSTM outperforms traditional methods in terms of accuracy and effectiveness. The study shows that the system is capable of handling the complexities of the Arabic language better than traditional models, making it a valuable tool for various NLP applications. The study concludes that TAQS represents a significant advancement in Arabic NLP, offering a robust and efficient solution for question similarity detection, and has the potential to be applied in a wide range of real-world scenarios where understanding and processing Arabic text is essential."
    },
    {
        "input_text": "Named Entity Recognition (NER) is mainly usable and suitable for data extraction. NER problem is described as an identification of Proper Noun and classifying the Proper Noun as Organization, Person, Geographical Entity, Event, and Location. NER is a crucial task in all the Natural Language processing functions such as Extraction of Information, Question Answering, Translation, Automatic Summarizing, etc. NER structures have performed well in various domains, such as recognizing named entities in prison archives, social media content, and news articles. Extracting the essential entities from a text helps to sort unstructured statistics and become aware of the necessary information. It concerns identifying the key elements in the text and organizing them into predefined categories such as Organization, Person, Geographical Entity, Event, and Location. Accurate and advanced NER systems are now available for European languages, mainly English and Asian. Indian languages are distinctly under-represented in the current NLP research. In this paper, Marathi labeled dataset is created using Marathi news articles. The data has been scraped from different news sources on the internet. Dataset comprises news articles from Entertainment, sports, Politics, and Environment. There are 100 news articles scraped from different sources of news articles available on the internet. There are 900 sentences and 7000 tokens in the NewsCorpus. POS tagging is an essential task in NLP. We analyzed traditional approaches for POS tagging, such as statistical approaches, rule-based methods, etc. In the statistical approach, the paper proposed a unigram tagger for Marathi NewsCorpus. There is a lack of sufficient labeled data available for different Natural Language processing tasks in the Marathi language. Thus, the paper proposed a transfer learning approach for Marathi NER. In the transfer learning-based approach, knowledge from one pretrained model is used for another model for a given task. Transfer learning improves the model’s performance and does not require much data for model building. Develop Models and Pre-trained Models are common transfer learning approaches for predictive problems. The pre-trained approach selects a pretrained source model from available models. Many research institutions and organizations release models trained on large and challenging datasets. These include multilingual-BERT, XLM-Roberta, and IndicBERT. Sequential transfer learning is a form that has led to significant improvements to date. The common practice is to pre-train representations on extensive unlabeled data using different models and then use the learning of these models in a supervised target task using labeled data. This paper compares the performance of the traditional algorithms such as support vector machine, conditional random field, and bidirectional LSTM (Long Short Term Memory) with transfer learning-based algorithms (multilingual-BERT, XLM-R, IndicBERT) trained on the same dataset. In this Research, Different Marathi named Entity Recognition techniques were applied on our NewsCorpus created on Marathi news documents. Machine Learning and transfer learning models are evaluated using accuracy, precision, recall, and F1-score evaluation metrics. Pretrained mBERT, IndicBERT, and XLM-Robert models were implemented for Marathi Named Entity Recognition. This research paper gives a comparative analysis of the performance of Transfer learning and Traditional algorithms on the NewsCorpus dataset. In the future, we aim to perform experiments on a sizeable Marathi dataset and more named entities. Also, we will try to use real-time news data for entity recognition.",
        "target_summary": "This study presents the creation of the NewsCorpus Marathi monolingual dataset, which contains 900 sentences and 7,000 tokens. The dataset was scraped from various Marathi news sources available on the internet. The study analyzed traditional NER and POS (Part of Speech) tagging approaches for the Marathi language, which is one of the regional languages in India. A Pre-trained Model Approach of transfer learning for Marathi NER is proposed in this paper. IndicBERT, mBERT, and XLM-Roberta, which are BERT-based pretrained models, are shown to produce state-of-the-art results on downstream tasks such as Named Entity Recognition. The BERT model is specifically trained on Wikipedia and Google’s BooksCorpus. This study compares the transfer learning approach with traditional machine learning algorithms, demonstrating that transfer learning-based models outperform the conventional approach. Among the BERT-based models, IndicBERT performs better on this dataset."
    },
    {
        "input_text": "Abstractive text summarization deals with creating a smaller, concise synopsis of a larger textual data which still contains all the important points present in the original text. Unlike extractive text summarization, the final summary does not contain the same sentences that are present in the original text. Here, we take the tasks of summarizing opinions and reviews of products, movies, and businesses from Amazon, Yelp, and Rotten Tomatoes to get a smaller, more convenient review that has all that a new user needs to look through to get a gist. Autoencoders are now becoming a common model used for summarization tasks. An autoencoder model consists of an encoder that converts a high-dimensional input to a low-dimensional latent vector and a decoder that converts the latent vector back to a high-dimensional output of a different size from the input size. For unsupervised learning, a categorical vector is also produced as an intermediate result which is used to find similar latent vectors. Variational autoencoders and adversarial autoencoders are improvements to the traditional autoencoder model since they regularize each input as a distribution in the given latent space. They generate a probabilistic latent vector which helps in generative tasks. The K-Means clustering algorithm is then used to cluster the normally distributed latent vector which is verified by the one-hot encoded vector produced by the encoder. Finally, a pre-trained T5 model is used to generate nearly semantically correct sentences. The key contributions of this work are: using Adversarial Autoencoders for abstractive text summarization, using language modeling techniques to get final semantically correct summaries, and using a Hindi dataset for the same task. The structure of the paper is organized as follows. We first discuss the existing research studies as part of our literature survey including their methodology and results obtained. Section 3 deals with the methodology of our study which includes the dataset, preprocessing technique, and model architecture. Finally, the evaluation, results, and analysis are explained in Section 4 with the concluding remarks in Section 5. The experiment was conducted on the four datasets in English and one in Hindi. The performance metric used for this study is Recall-Oriented Understudy for Gisting Evaluation or ROGUE scores. ROGUE-N matches N-Grams of the given text with respect to a reference text when an N-Gram refers to a group of N consecutive words. Here, N used is one and two. The recall is measured as the ratio of the number of common N-Grams found in both summary and reference to the number of N-Grams present in the reference. This is different from the BLEU score which finds the precision or the ratio of common N-Grams to the number of N-Grams present in the generated summary. ROGUE-L is used to measure the longest common subsequence present in the given text and the reference. While ROGUE-N provides us a measure of how informative the generated text is, the ROGUE-L score tells us how fluent it is. The ROGUE-1 Score obtained is 22.19, ROGUE-2 is 4.56 and ROGUE-L is 19.88. This paper deals with the training of an adversarial autoencoder to generate an abstractive summary of an input text. Amazon, Yelp, and Rotten Tomatoes reviews for English and Hindi Text Short Summarization Corpus for Hindi language were used for training the model. The model is trained using the help of two discriminators which help in making sure the latent vector follows normal distribution and categorical vectors are predicted correctly. The ROGUE-1, ROGUE-2, and ROGUE-L scores are comparable for Amazon, Yelp, Rotten Tomatoes, and CNNMail Dataset. The ROGUE scores for the Hindi dataset turned out to be a little low but a major reason for that is that the headlines for the articles were used for reference instead of human-generated summaries which is not a good reference to calculate the ROGUE scores. For future work, we would like to use a better dataset for summarization tasks in Indian languages with human-generated summaries to evaluate the model better",
        "target_summary": "This study focuses on Named Entity Recognition (NER), which is primarily used for data extraction. NER involves identifying Proper Nouns and classifying them into categories such as Organization, Person, Geographical Entity, Event, and Location. It is a crucial task in various Natural Language Processing (NLP) functions, including Information Extraction, Question Answering, Translation, and Automatic Summarization. NER systems have shown effectiveness in domains such as recognizing named entities in prison archives, social media content, and news articles. Extracting key entities from text helps to organize unstructured data and identify important information by categorizing elements into predefined groups. While accurate and advanced NER systems are available for European and some Asian languages, Indian languages, particularly Marathi, are underrepresented in current NLP research. This study created a labeled dataset in Marathi using news articles scraped from various sources on the internet, covering topics such as Entertainment, Sports, Politics, and Environment. The dataset includes 100 news articles, 900 sentences, and 7,000 tokens. Traditional approaches for POS tagging, including statistical methods and rule-based techniques, were analyzed. A unigram tagger was proposed for the Marathi NewsCorpus due to the lack of sufficient labeled data for NLP tasks in Marathi. To address this limitation, a transfer learning approach for Marathi NER was proposed, leveraging knowledge from one pretrained model to improve another models performance without requiring extensive data for training. Transfer learning approaches, including the development of pre-trained models, have become common in predictive problems. This study utilized pre-trained models such as multilingual-BERT, XLM-Roberta, and IndicBERT, which are released by research institutions and organizations after being trained on large datasets. Sequential transfer learning, which involves pre-training representations on extensive unlabeled data followed by supervised learning on labeled data, has shown significant improvements. This study compares the performance of traditional algorithms like support vector machines, conditional random fields, and bidirectional LSTM (Long Short Term Memory) with transfer learning-based algorithms (multilingual-BERT, XLM-R, IndicBERT) on the same dataset. Different Marathi NER techniques were applied to the NewsCorpus, and the Machine Learning and transfer learning models were evaluated using accuracy, precision, recall, and F1-score metrics. Pretrained mBERT, IndicBERT, and XLM-Roberta models were implemented for Marathi Named Entity Recognition. This study provides a comparative analysis of the performance of transfer learning and traditional algorithms on the NewsCorpus dataset and aims to expand future research to include larger Marathi datasets and additional named entities, as well as exploring the use of real-time news data for entity recognition."
    },
    {
        "input_text": "In July 2015, YouTube revealed that it receives over 400 hours of video content every single minute, translating to 65.7 years worth of content uploaded every day. Since then, consumer engagement with both online video platforms and devices such as smartphones and wearables has increased significantly, enabling instant uploading of captured videos on the Web. According to newer estimates, YouTube now receives 500 hours of video per minute, and YouTube is just one of many video hosting platforms, including DailyMotion and Vimeo, as well as social networks like Facebook, Twitter, and Instagram, and online repositories of media and news organizations that host large volumes of video content. With this overwhelming amount of video content available, efficient navigation through these vast collections to find specific content has become increasingly challenging. The solution lies not only in video retrieval technologies but also in automatic video summarization technologies, which generate concise synopses that convey the important parts of full-length videos. Effective video summarization is crucial for facilitating viewers browsing and navigation through large video collections, thereby increasing viewer engagement and content consumption. The application domain of automatic video summarization is extensive and includes its use by media organizations, after integrating such techniques into their content management systems, to allow effective indexing, browsing, retrieval, and promotion of their media assets, and by video sharing platforms to enhance the viewing experience and increase content consumption. Additionally, video summarization tailored to specific content presentation scenarios can be used for generating trailers or teasers of movies and TV episodes, presenting highlights of events such as sports games or music performances, and creating video synopses of activities recorded by surveillance cameras for time-efficient monitoring or security purposes. Several surveys on video summarization have already appeared in the literature, classifying summarization approaches according to various aspects such as the targeted scenario, type of visual content, and summarization methodology. Early studies categorized methods into utility-based and structure-based approaches, discussed attributes affecting summarization outcomes, and explored the use of video stream analysis, contextual metadata processing, and hybrid methods. More recent studies have focused on specific aspects like egocentric video summarization and deep-learning-based methods for video summarization, with some works highlighting the advancements brought by deep neural networks. However, none of the previous surveys comprehensively covered the current developments in generic video summarization, particularly the growing use of advanced deep neural network architectures for learning the summarization task. This survey examines more than 40 different deep-learning-based video summarization algorithms proposed over the last five years, comparing their performance against more conventional approaches. The findings indicate that deep-learning-based methods often outperform traditional approaches, representing the current state of the art in automatic video summarization. Motivated by these observations, this survey fills the gap in the literature by presenting the relevant bibliography on deep-learning-based video summarization and discussing associated aspects such as evaluation protocols. The article begins by defining the problem of automatic video summarization, presenting the most prominent types of video summary, and providing a high-level description of the analysis pipeline of deep-learning-based algorithms. It introduces a taxonomy of the literature based on data modalities, training strategies, and learning approaches, followed by a systematic review of the bibliography. The review categorizes methods according to the use of human-generated ground-truth data for learning and the learning objectives or data modalities utilized. General remarks are provided on how the field has evolved, especially in the last five years, highlighting the strengths and weaknesses of different methods. The article also discusses the datasets and evaluation protocols used in the literature, findings from performance comparisons, and the most competitive methods in supervised and unsupervised video summarization. Potential future directions are proposed to further advance the current state of the art in video summarization. The article concludes by outlining the core findings, including the strengths of supervised methods that model temporal dependencies using recurrent neural networks and attention mechanisms, the promise of GANs in unsupervised video summarization, and the need for more research in fully unsupervised or weakly supervised methods. The article also addresses concerns about evaluation protocols and calls for increased reproducibility of results. Finally, the article suggests that further efforts should be directed toward the practical application of summarization algorithms in tools that support modern media organizations needs for time-efficient video content adaptation and reuse.",
        "target_summary": "This study focuses on video summarization technologies, which aim to create concise and comprehensive synopses by selecting the most informative segments of video content. Over the past few decades, various approaches have been developed, with the current state of the art being dominated by methods that utilize modern deep neural network architectures. The work provides a thorough survey of the latest advancements in deep-learning-based methods for generic video summarization. It begins by discussing the motivation behind the development of video summarization technologies, followed by a formulation of the video summarization task and an exploration of the main characteristics of a typical deep-learning-based analysis pipeline. A taxonomy of existing algorithms is proposed, along with a systematic review of relevant literature that traces the evolution of these technologies, culminating in suggestions for future developments. Additionally, the study presents protocols for the objective evaluation of video summarization algorithms and compares the performance of several deep-learning-based approaches. Drawing from these comparisons and considerations regarding the availability of annotated data and the appropriateness of evaluation protocols, the study highlights potential future research directions."
    },
    {
        "input_text": "There are different types of viruses present all over the world. The paper discusses a machine learning method to detect the presence of a virus named Human T-lymphotropic Virus (HTLV), which causes a type of cancer named T-cell leukemia/lymphoma and is transferred from body to body through body fluids. It is mainly of three types, HTLV-1, HTLV-2, and HTLV-3, with HTLV-1 being the most prominent and primarily associated with cancer. The prediction of Human papillomavirus (HPV), a common virus that can cause cancer and other genital problems in humans, is done in another paper. HPV enters the body through skin-to-skin contact, and there are over 100 types of HPV viruses, not all of which are harmful, but some are dangerous. Another article proposes a model for the prediction of Hepatitis C virus, a viral infection that can cause serious liver diseases and spreads through contaminated blood. Many people are infected by Hepatitis C by sharing needles used to inject drugs, leading to either short-term illness or chronic conditions. The article presents a method for predicting influenza virus pathogenicity, which causes flu and other respiratory disorders in humans and spreads easily via air. Influenza A, B, C, and D are the most common kinds, and the pathogenicity of each type is determined by the proposed system. Additionally, another article provides a model for Influenza A virus prediction using an ANN-based approach for efficient prediction. A model for HIV viral prediction is proposed in another publication, based on monitoring changes in a suspected person’s DNA sequences, using clustering algorithms and an HMM model. HIV viruses attack the body’s immune system, lowering immunity, making even minor illnesses serious, and while HIV cannot be cured, it can be managed with proper medical care. Another paper focuses on HIV, using evolving neural networks to predict the use of R5, X4, and R5X4 coreceptors, which are different types of HIV. Drug resistance prediction, an important aspect of medical care, is also covered in a paper that uses machine learning algorithms for predicting drug resistance in HIV patients through multiple regression, which is crucial for determining the best course of therapy. A model for predicting interactions between the COVID-19 virus and the host is proposed, with SARS-CoV-2 identified as the strain responsible for the pandemic declared in March 2020. The primary cause of SARS-CoV-2 virus infection in humans is protein-protein interactions (PPI), making the prediction of these interactions critical during the pandemic. Multiple machine learning models are applied for PPI prediction between the virus and human proteins, validated through biological tests. Another study provides a model for classifying distinct viral genomes based on the biological structure of each DNA sequence, using a novel platform called CASTOR for viral classification. All the ten papers discussed here propose models for predicting virus-related characteristics using different machine learning methods, resulting in varying accuracies. One paper predicts the types of HTLV viruses with an accuracy of 97%, while another achieves 94.73% accuracy for predicting HPV risk types. The highest accuracy for predicting Hepatitis C is 89.17% using SVM, and the sensitivity for predicting influenza virus pathogenicity is measured at 99.3%. The model for predicting Influenza A infections has an accuracy of 97%, while the HIV prediction model achieves 88% accuracy. Another model predicts HIV coreceptors with 75.5% accuracy, and the drug resistance model shows accuracy greater than 95%. The COVID-19 interaction prediction model achieves an accuracy above 72% for the ensemble method, and the viral genome classification model reaches 91.74% accuracy. From these results, it is concluded that all models predict results with at least 70% accuracy. While machine learning technologies cannot detect all viruses, some can be detected using various techniques. This study discusses several models for virus prediction, each using different machine learning concepts to improve accuracy. Common techniques like SVM, Naive Bayes, neural networks, regression, Adaboost, and bagging are employed, with each model presenting its own advantages and disadvantages, such as time consumption. Future research should focus on developing a new model that overcomes these disadvantages.",
        "target_summary": "This study examines viruses, which are microorganisms that are small in size and capable of replicating only within a host organism, affecting a range of living beings such as animals and plants. Viruses contain genetic material in the form of DNA or RNA, but they cannot replicate independently, requiring a host organism like a human, animal, or plant to do so. This replication process can have various effects on the host, often resulting in diseases. Due to the diversity in the biological structure of viruses and their ability to target specific regions within an animal's body, a single detection method is insufficient for all types of viruses. Consequently, different detection techniques are needed for different viruses. This study reviews various viruses, the diseases they cause, and the machine learning algorithms used to detect these diseases. Ten different articles were selected for the review, each employing a variety of machine learning algorithms and feature selection methods to create models for disease diagnosis. These approaches include Support Vector Machine (SVM), Linear Model (LM), Linear Regression (LR), K-Nearest Neighbors (KNN), Artificial Neural Network (ANN), and K-means, among others. The use of different feature selection methods and machine learning techniques across these articles results in varying model accuracy. This research aims to enhance the understanding of viruses and diseases or to inform the development of hybrid machine learning algorithms."
    },
    {
        "input_text": "Much has been written about the specifications and effects of machine learning (ML) systems, variously assessing, critiquing, and hyping their technical and social significance. Less attention, however, has been dedicated to ML technologies as objects of design—implements with technical features that could be otherwise, fabricated through subjective and creative choices. This omission is notable, as ML technologies are integral across major institutions, implicated in consequential decisions, and interwoven with the paces and practices of everyday life. How these systems are designed, matters. Considered attention from the design studies tradition is thus a generative intervention in the production and understanding of ML systems, and the task to which this paper is set. In particular, I apply a central construct from design studies—affordances—to the ML domain, mobilizing an operational framework from which to evaluate, build, and reimagine ML applications. This move scaffolds a bridge between ideals and execution, alleviating the perennial principles-to-practice gap that has long plagued AI and ML fields. The design of technological systems is the design of social systems, deriving from and shaping both culture and practice. As a discipline, design studies is premised on the notion that human behaviors and experiences are affected by the contours and levers of designed objects. This base postulate has long circulated through myriad professional sectors such as user experience (UX) research, law hardware and software development, human-computer-interaction (HCI), robotics, engineering, architecture, and education via a common conceptual tool: technological affordances. In its simplest sense, affordances are the ways technical features enable and constrain for socially situated subjects. Assessing and in-building particular affordances through combined feature sets has been a central practice across spheres of technological design. Here, I extend affordances to ML through the mechanisms and conditions framework, an operationalization attending to dynamism in both subjects and structure. I begin by motivating the argument with two fundamental assumptions, before delving into a concise overview of affordances in technological design, including a summative description of the mechanisms and conditions framework. I then explicate how the framework applies to the domain of ML and highlight the relevance of an ML-affordance pairing. With this foundation set, three case examples exhibit the utility of affordances for ML and demonstrate how practitioners can apply the mechanisms and conditions framework for targeted ends of analysis and (re)design. Based in design studies, theories of affordance have long been central to understanding and intervening in the development and analysis of technological systems, yet ML has remained outside of the design studies purview. This is perhaps a function of ML as data driven, and thus less obviously conceived as an object of design. As I have demonstrated, however, ML systems and the models on which they run are subject to myriad design decisions which both reflect and shape the social worlds in which these systems operate. The present work thus extends affordances to ML, anchored by the M&C framework. Through three case examples across work, policing, and housing justice, I show how the M&C framework can illuminate the ways both technological and social systems afford across subjects and circumstances. Not only does this bring affordances (and design studies more broadly) to the field of ML, but also shows by example how to mobilize the M&C framework as a critical tool of both revelation and (re)making, scaffolding a path between principles and practice. Beyond this core contribution, I conclude with four key takeaways to guide affordance studies of ML systems going forward. The first point is that analysis and design are intertwined. Though presented distinctly across the case examples herein, analysis and design are inextricable and reciprocally connected. Analyses should be done with an aim towards remaking, while objects remain always in process and under analytic scrutiny, subject to adaptation and dismantle. This reciprocal relation is especially relevant to objects that develop through machine learning as they are never complete and always responsive to the data of a dynamic social world that has, and continues to, reflect, perpetuate, and intensify patterns of social order. Point two builds on yet also complicates the first: though analysis and design are intertwined and ongoing, front-end planning should take precedence. Once a system is built, it runs on its own inertia. Adjustments and retrofits overlaid upon a faulty core may alleviate some harms, but remain tied to a basic set of parameters from which deviations are necessarily limited and alternative pathways preemptively foreclosed. Moreover, as Ehsan and colleagues point out, ML applications can leave imprints upon the people and institutions through which these systems are implemented, such that effects endure even after an ML system discontinues. Point three goes back to the socio-structural character of technological systems, and the ways these systems are always and inevitably embedded within political economies of interests and power. Those working at the intersection of affordances, technology, and the law are especially instructive here, reckoning with the ways policies and regulation materialize with and through hardware, software, and code. This is a vital point when considering the case examples presented above—each of which contends with institutional actors backed by corporate and/or state authority. Put plainly, one cannot expect such institutions to redesign if they do not perceive it in their interests to do so. However, ML standards can be compelled through interrelated efforts of collective action, organizational policies, and legislative interventions which together, focus social, regulatory, and legal attention upon sociotechnical configurations. Such efforts can be aided by, developed through, and implemented with the M&C framework. Finally, identifying affordances—as they are in analysis and how they ought to be, in design—is contingent on the people in the proverbial room. Everyone has a standpoint that renders some things more observable and others, less. But, if we take seriously the canonical feminist point that those on the margins offer a uniquely valuable perspective for their recognition of realities otherwise clouded by privilege, then the White masculinity of computational professions becomes acutely salient. As D’Ignazio and Klein point out, data and computational sciences (broadly conceived) suffer from a ‘privilege hazard’, by which those who make and evaluate technological systems cannot access, predict, or understand the harms that will ensue for people unlike themselves. An affordance approach and its manifestation through the M&C framework requires sharp social attunement, best deployed through many and diverse hands. This means efforts at design and audit that give access to the outside, that place affected communities at the center, and that approach with humility, honesty, and a readiness to respond when the machines that decide, assist, govern, and predict inevitably learn to reproduce (and regress) the social systems of which they are a part. The logics and tools of design studies, including affordances, do not solve the troubles wrought by AI, automation, machine learning, or any other technological system, nor do they promise the realization of social good. These logics and tools do, however, draw explicit links between technical choices and their social effects, making these connections observable and thus actionable. Such a linkage is necessary, if not sufficient, for deliberate and considered approaches that hold technological systems and those who make and deploy them to account, and for building technologies that reflect and contribute to renditions of society that we hope to achieve. The M&C framework, in particular, attends to how, for whom, and under what circumstances technologies operate, by which a simple vocabulary exposes sociotechnical complexities underneath. As applied to the ML domain, the framework has been shown here as an instrument of analysis, (re)design, dismantle, and critical reflection. It also joins together design studies and machine learning, laying a foundation for this generative pairing",
        "target_summary": "This study highlights the significant role of design in machine learning (ML) systems, emphasizing that ML technologies are not just technical tools but also design objects. It argues that how ML systems are designed has profound implications across various sectors due to their integration into major institutions and impact on daily practices. By applying design studies' concept of affordances to ML, the paper presents a framework to assess, construct, and re-envision ML applications. This approach helps bridge the gap between theoretical principles and practical application, ensuring that ML systems are not only technically robust but also socially responsible. The study employs a mechanisms and conditions framework to explore how ML can be more effectively managed throughout its lifecycle, using design principles to address the dynamic complexities of real-world applications. Through this lens, the study offers a structured methodology for understanding and enhancing the interaction between technological functionalities and social outcomes in ML systems."
    },
    {
        "input_text": "Much has been written about the specifications and effects of machine learning (ML) systems, variously assessing, critiquing, and/or hyping their technical and social significance. Less attention, however, has been dedicated to ML technologies as objects of design—implements with technical features that could be otherwise, fabricated through subjective and creative choices. This omission is notable, as ML technologies are integral across major institutions, implicated in consequential decisions, and interwoven with the paces and practices of everyday life. How these systems are designed matters. Considered attention from the design studies tradition is thus a generative intervention in the production and understanding of ML systems, and the task to which this paper is set. In particular, I apply a central construct from design studies—affordances—to the ML domain, mobilizing an operational framework from which to evaluate, build, and reimagine ML applications. This move scaffolds a bridge between ideals and execution, alleviating the perennial principles-to-practice gap that has long plagued AI and ML fields. The design of technological systems is the design of social systems, deriving from and shaping both culture and practice. As a discipline, design studies is premised on the notion that human behaviors and experiences are affected by the contours and levers of designed objects. This base postulate has long circulated through myriad professional sectors such as user experience (UX) research, law, hardware and software development, human-computer interaction (HCI), robotics, engineering, architecture, and education via a common conceptual tool: technological affordances. In its simplest sense, affordances are the ways technical features enable and constrain for socially situated subjects. Assessing and in-building particular affordances through combined feature sets has been a central practice across spheres of technological design. Here, I extend affordances to ML through the mechanisms and conditions framework, an operationalization attending to dynamism in both subjects and structure. I begin by motivating the argument with two fundamental assumptions, before delving into a concise overview of affordances in technological design, including a summative description of the mechanisms and conditions framework. I then explicate how the framework applies to the domain of ML and highlight the relevance of an ML-affordance pairing. With this foundation set, three case examples exhibit the utility of affordances for ML and demonstrate how practitioners can apply the mechanisms and conditions framework for targeted ends of analysis and (re)design. Based in design studies, theories of affordance have long been central to understanding and intervening in the development and analysis of technological systems, yet ML has remained outside of the design studies purview.",
        "target_summary": "This study emphasizes the importance of considering machine learning (ML) systems as designed objects, highlighting the significant role of design in their development and impact. It introduces the concept of affordances from design studies to the ML domain, advocating for a structured approach that bridges the gap between theoretical ideals and practical execution. By applying a mechanisms and conditions framework, the study reimagines the design and functionality of ML systems, ensuring they are both technologically robust and socially responsible. The integration of design principles helps address the challenges in ML development, making the systems more adaptable and aligned with human needs and societal values."
    },
    {
        "input_text": "Climate change poses one of the greatest challenges of our time, affecting ecosystems and the well-being of communities worldwide. Estimating and reducing carbon emissions is crucial in mitigating its effects. Recent estimates suggest that the Information and Communications Technology (ICT) sector contributes approximately 2% to global CO2 emissions. As traditional software gets integrated with large Machine Learning (ML) components to create ML-Enabled Systems (MLES), ICT’s CO2 emissions are expected to grow even further. The development of data through complex algorithms often requires multiple iterations to optimize model parameters for accuracy. This computational demand typically necessitates the use of powerful hardware, such as GPUs or TPUs, which consume substantial amounts of electricity. Additionally, training large-scale ML models, such as those used in natural language processing or image recognition, can take days, weeks, or even months to complete, further exacerbating energy consumption. Therefore, there has been a growing call within the artificial intelligence (AI) community to emphasize the mitigation of the environmental footprint linked to the development, deployment, and maintenance of AI models, especially in industrial contexts. Since 2008, the software industry has witnessed the rise of DevOps, promoting collaborative synergy between development and operations teams of traditional software. This methodology optimized workflows, nurtured efficiency and effectiveness in software development and operations. Nevertheless, the conventional DevOps approach is not viable for ML-Enabled Systems (MLES) due to the unique challenges associated with such systems (e.g., data dependencies, boundary erosion, continuous learning) that require specialized deployment and maintenance processes. Therefore, as application development progresses, the need for an updated methodology arises, driving a necessary evolution in DevOps. This evolution, crucial in the ML domain, has led to the emergence of MLOps, essentially DevOps customized for ML. MLOps plays a vital role in operationalizing ML in production by automating the entire ML lifecycle, speeding up model development and deployment, and facilitating innovation delivery. This shift has not only enhanced software accuracy but also streamlined development processes. In this work, we analyze how the energy footprint of MLES is assessed through the lens of MLOps pipeline. We also review the approaches and the lessons put forward to minimize it at each of the MLOps phases. The primary contributions of this study are: Reviewing existing research on the environmental impact of MLES, Conducting a Systematic Mapping Study (SMS) that identifies and examines proposed software engineering practices to evaluate and reduce the environmental cost of MLES within the MLOps methodology. Our study offers a comprehensive overview of community efforts to understand/reduce the environmental impact of MLOps in MLES and contributes towards sustainable engineering of such systems. The subsequent sections of the paper are organized as follows. Section 2 summarizes the background and work related to our study. Section 3 outlines the research methodology. Section 4 presents the results and engages in discussions. Finally, Section 5 concludes this work. As traditional software gets integrated with large ML components to form MLESes, ICT’s CO2 emissions are expected to grow even further. While there has been an increased recognition of the environmental cost of MLES, a unified approach towards ensuring sustainability remains elusive. This mapping study presented a thorough examination of methodologies, tools, and metrics employed to assess and mitigate MLES environmental cost and their mapping onto the MLOps pipeline. By analyzing 52 primary studies from 2019 to 2023, our study offered insights into the growing landscape of this field, laying the groundwork for future investigations into eco-friendly strategies for MLES MLOps. The main findings reveal a critical gap in the current practices and strategies for managing the environmental impact of MLES within the MLOps pipeline. Despite the availability of strategies and metrics aimed at assessing the environmental impact of these systems, the study shows that there is no optimal practices for integrating these assessments into the development and operation of MLES. Moreover, our analysis indicates that the majority of existing research is focused on controlled, in vitro environments. Hence, there is a pressing need for more in vivo studies, i.e., field experiments and studies in industrial settings. As future work, we plan to develop foundational concepts and techniques for improving, monitoring, and optimizing the efficiency and sustainability of MLES MLOps pipelines over time.",
        "target_summary": "This study examines the escalating environmental impact of Machine Learning-Enabled Systems (MLES) within the Information and Communications Technology (ICT) sector, which is a growing contributor to global CO2 emissions. It highlights the transition from traditional DevOps to MLOps, which not only automates the ML lifecycle but also emphasizes sustainability in software development. The research reviews and analyzes methods to assess and minimize the carbon footprint of MLES through the lens of the MLOps pipeline, suggesting that current practices in MLES development need to incorporate more comprehensive, real-world applicable eco-friendly strategies. The findings advocate for the urgent integration of sustainable practices into the ML development process to mitigate the environmental costs of these advanced technologies."
    },
    {
        "input_text": "Diabetes is a chronic metabolic disorder that affects how the body converts food into energy, potentially impacting the entire body system. In 2019, of the 37.3 million adults with diabetes in the United States, 8.5 million were undiagnosed. It is crucial to address this condition because if diabetes remains undiagnosed and is not properly treated to control blood sugar levels, serious health problems can arise. Undiagnosed cases can lead to life-threatening health conditions, including heart disease, kidney disease, gum disease, vision loss, nerve, or blood vessel damage, and more. However, with early detection of the disease, healthcare workers can provide patients with the proper treatment to avoid many of these health complications. Diabetes can be detected and predicted based on various patient information features, such as age, blood pressure levels, smoking/drinking history, physical activity levels, number of pregnancies, etc. Many hospitals and healthcare companies collect vast amounts of this patient data. However, drawing conclusions about the data is challenging and time-consuming for humans alone. The use of artificial intelligence in the healthcare industry is actively increasing. Machine learning (ML) techniques are playing an emerging role in healthcare systems to analyze medical data for disease diagnosis. Machine learning models can process large volumes of patient data for extremely fast analysis. Our study focuses on the utilization of machine learning models for predicting diabetes diagnoses in medical patients. With the use of artificial intelligence, models are trained on past data to learn the underlying patterns within the features (patient qualities) that indicate whether an individual has diabetes or not. These models can then apply the knowledge they have gained from previous data to new data for predicting whether someone will have diabetes based on their individual values of the independent patient information features. In our work, we propose a method for utilizing machine learning algorithms to predict diabetes diagnoses in the manner. We will achieve this using various machine learning models for classification. In machine learning, classification refers to any predictive modeling problem in which a class label is predicted for a given input data example. In our study, we will employ six machine learning models for the classification of the outcome variable ’Diabetes,’ which indicates whether an individual patient has diabetes, for both the ’Pima Indians Diabetes’ and ’Diabetes 2019’ datasets. We explore the effectiveness of six models for the classification of patient data on two different data sets. Our first data set is the ’Pima Indian Diabetes’ data set, collected from the public data set repository Kaggle. This data set includes eight features of patient qualities and the outcome variable, which indicates whether the patient has diabetes (note that this data set only includes information on female patients of Pima Indian heritage over the age of 21). Our second data set, the ’Diabetes 2019’ data set, was also collected from Kaggle. It includes 17 features of patient qualities as well as the outcome variable, diabetes, for patients with no one specific set of demographics. We utilize the Gaussian Naïve Bayes, Decision Tree Classifier, Random Forest, K-Nearest Neighbor, Linear Support Vector Clustering (SVC), and Ridge Classifier ML models to find the algorithm with the highest accuracy for each of our data sets. With our first data set, we found that the algorithm that produced the highest accuracy for diabetes prediction was the Ridge Classifier with an 83.12% accuracy. With our second data set, both the Decision Tree and Random Forest classifiers achieved the same value for the highest accuracy of 95%. Our goal is to identify the most robust algorithm for medical use in healthcare settings. We aim to enhance the utilization and dependability of ML models for early diabetes prediction and detection, thereby assisting healthcare workers in expediting treatment initiation. This, in turn, offers patients a better chance of leading healthy lives while avoiding serious health complications that often result from undiagnosed cases. One of the limitations of our study is the difficulty that arises when trying to compare the results of our two data sets. As shown in Table 1 and Table 2, the evaluation metrics of all of the models except for Linear SVC were much higher with the ’Diabetes 2019’ data set than they were in the ’Pima Indians Diabetes’ data set. In a sense, it is difficult to compare the two data sets because the ’Pima Indians Diabetes’ data set includes information from patients of only a very specific set of demographics (i.e. only females of Pima Indian heritage of at least 21 years of age), while the ’Diabetes 2019’ data set has information from a more generalized population of no restricted demographic(s). However, the differences in model evaluations of the two data sets might indicate that the presented ML models are likely to perform better on more generalized data with a greater number of features. This conclusion can be drawn because of the data set population samples described above as well as the fact that the ’Pima Indians Diabetes’ data set has only 8 features while the ’Diabetes 2019’ data set has seventeen. Our overall goal was to test the reliability of various machine learning models to determine the best algorithms to suggest for use in real healthcare systems. In our experiments, the best models for diabetes prediction were Ridge Classification, Decision Tree, and Random Forest. We achieved the highest accuracy scores of 83.12% (Ridge Classifier) for the ’Pima Indians Diabetes’ data set and 95% (both Random Forest and Decision Tree Classifiers) for the ’Diabetes 2019’ data set. These accuracy scores indicate that these models performed very well on the data sets, since the general rule of thumb is that any accuracy score greater than 80% indicates great model performance. Additionally, our top algorithmsRidge Classifier, Random Forest, and Decision Tree– also each have high precision scores, indicating that they perform well at correctly identifying the patients as having diabetes that do have diabetes out of the total number of patients it predicts to have diabetes (including those that did not actually turn out to have diabetes). With that, each of these three ML models also have high recall scores, suggesting that they have high True Positive Rates, meaning that they perform well at predicting how many patients have diabetes out of the total number of patients who do have diabetes. Therefore, we suggest that these three ML models would be wellsuited for diabetes prediction in a real healthcare setting because they achieved high values of the evaluation metrics, indicating that they would be able to accurately predict diabetes diagnoses from patient data. The use of these algorithms in artificially intelligent healthcare systems can potentially help healthcare providers get patients the proper treatment at a sooner date, avoiding the increased risk of health complications that results from undiagnosed diabetes cases",
        "target_summary": "This study addresses the significant challenge of undiagnosed diabetes by utilizing machine learning (ML) models to enhance early detection and treatment. It highlights the integration of traditional software with large ML components, contributing to increased carbon emissions in the Information and Communications Technology (ICT) sector. Through the application of various ML models, such as Ridge Classifier, Decision Tree, and Random Forest, the study demonstrates a high accuracy in predicting diabetes using two distinct datasets: the 'Pima Indians Diabetes' and 'Diabetes 2019.' These models are shown to effectively identify diabetes from patient data, with accuracies reaching up to 95%. The research advocates for the implementation of MLOps to streamline the development and operationalization of ML models in healthcare, aiming to mitigate the environmental impact while improving medical diagnostics. This approach not only advances the technical capabilities of ML systems but also addresses the crucial need for sustainable practices in healthcare technology."
    }


    ]
